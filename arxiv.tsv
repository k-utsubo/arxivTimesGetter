1	Sim-to-Real Robot Learning from Pixels with Progressive Nets	https://arxiv.org/abs/1610.04286	Andrei A. Rusu, Matej Vecerik, Thomas Rothrl, Nicolas Heess,Razvan Pascanu, Raia Hadsell	強化学習(DQN)の転移学習についての論文。読む限りは、アンサンブル学習のように(シミュレーター等で)学習させたモデル(これがColumnになる)を並列に統合している感じ(これをProgressive Networksと呼んでいる)。
2	Using Fast Weights to Attend to the Recent Past	https://arxiv.org/abs/1610.06258v2	Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Z. Leibo, Catalin Ionescu	RNNでは通常前回の隠れ層しか考慮されないが、「前回」しか考慮しないのに意味はないので、前回、前々回・・・(S回)の隠れ層を再帰的に適用しようという話。もちろん、直近の隠れ層の方を重く見る(decay rateで調整)。これで既存のLSTMより優秀な性能を記録できたとのこと
3	Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling	https://arxiv.org/abs/1610.07584	Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, Joshua B. Tenenbaum	3DのGANだけでなく、GANで使ったdiscriminator(真偽判定を行うネットワーク)の隠れ層使ってSVMしたら分類性能がすごかったり、写真から3Dモデルを構築したり、3Dモデルの足し算引き算をやったりとやりたい放題である。
4	A Learned Representation For Artistic Style	https://arxiv.org/pdf/1610.07629v1.pdf	Vincent Dumoulin, Jonathon Shlens, Manjunath Kudlur	いわゆる画風変換についての論文。色々な変換ができるけど遅いvs速く変換できるけどスタイルごとに学習させる必要があるというトレードオフを解決。色々なスタイルとはいえ共通するところはあるはず、という仮定の下試してみたら正規化とパラメーター調整だけで複数スタイルが表現できたという話。
5	Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards	https://arxiv.org/abs/1610.08120	Suchet Bargoti, James Underwood	リンゴの果樹園で、CNNつかってリンゴ数えまくった話
6	Predicting First Impressions with Deep Learning	https://arxiv.org/abs/1610.08119	Mel McCurrie, Fernando Beletti, Lucas Parzianello, Allen Westendorp, Samuel Anthony, Walter Scheirer	CNNで、人の顔から第一印象(賢そうだなとか、年いってるなとか)を予測した研究。データセットはクラウドソーシングで作成。TestMyBrain.orgという心理テスト？のサイトを使って作成したらしい。
7	Learning Scalable Deep Kernels with Recurrent Structure	https://arxiv.org/abs/1610.08936	Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu, Eric P. Xing/Carnegie Mellon University	LSTMの出側にGaussian processを加え、データにおいて量的にも質的にもrobustにした研究。カーネル自体を学習する。推定値の信頼性も出力されるのもありがたみの一つ。計算量削減の工夫もされている。
8	LEARNING TO PROTECT COMMUNICATIONS WITH ADVERSARIAL NEURAL CRYPTOGRAPHY	https://arxiv.org/abs/1610.06918	Mart´n Abadi, David G. Andersen/Google Brain	情報を守るための鍵の使い方を3つのDNN（3つのエージェント）を使って学習させた研究。
9	Review Networks for Caption Generation	https://arxiv.org/pdf/1605.07912v4.pdf	Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, William W. Cohen/Carnegie Mellon University	イメージキャプション生成におけるEncoder-Decoder+attentionモデルにreview steps(thought vectors)を加えて既存アルゴリズムの欠点を緩和した。
10	End-to-end learning of deep visual representations for image retrieval	https://arxiv.org/abs/1610.07940	Albert Gordo, Jon Almazan, Jerome Revaud, Diane Larlus	i) training dataを自動的にクリーニング、ii)ネットワークを最適化、iii)訓練を最適化して画像検索精度をSoTaまで上げたという話。
11	Feature-Augmented Neural Networks for Patient Note De-identification	https://arxiv.org/pdf/1610.09704v1.pdf	Ji Young Lee, Franck Dernoncourt/MIT	DLを使って医療データの匿名化すべき箇所を特定するDernoncourtet al., 2016の手法の特徴量作成部分を改良。SoTA。
12	Reasoning With Neural Tensor Networks for Knowledge Base Completion	http://nlp.stanford.edu/pubs/SocherChenManningNg_NIPS2013.pdf	Richard Socher, Danqi Chen, Christopher D. Manning, Andrew Y. Ng	エンティティ間の関係性(猫/持つ/しっぽ、などといったトリプル)を表現するモデル(Neural Tensor Network)の提案と、エンティティの表現に単一の分散表現でなく所属する分散表現の平均を用いると良かったという話(学習済みならなお良し)。かなりの高精度。
13	Universal adversarial perturbations	https://arxiv.org/abs/1610.08401	Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard	画像の分類機を「だます」ことについての研究で、異なる種類の分類機でも同様の手法で「間違わせる」ことが可能という結果。だます側には正常な分類結果から「押し出す」最小の変更量を学習させる。しかも変更を加えた画像は元のものとほとんど見分けがつかない(Figure3と11参照)。
14	CRF-CNN: Modeling Structured Information in Human Pose Estimation	https://arxiv.org/abs/1611.00468	Xiao Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang/The Chinese University of Hong Kong	ポーズ推定（関節のポジション推定）にCNNで構築したCRFアルゴリズムを付加し、関節間の関係性を考慮した推定を行えるようにした。近似にはmessage passingを使用。それぞれの関節を独立に扱う事によって効率的に近似が行える。
15	SEMI-SUPERVISED KNOWLEDGE TRANSFER FOR DEEP LEARNING FROM PRIVATE TRAINING DATA	https://arxiv.org/abs/1610.05755	Nicolas Papernot/Pennsylvania State University	医療情報などの（個人情報が含まれるなどの理由で）扱いに制約があるデータで、他のデータと結合することが出来ない時に効果的にモデルの精度を上げる手法の研究。半教師あり学習を活用。
16	Operator Variational Inference	https://arxiv.org/abs/1610.09033	Rajesh Ranganath, Jaan Altosaar/ Princeton University	ベイズ最適化などでは事前分布として正規分布が仮定されており、KLダイバージェンスが収束判定に使用されているが、実データが仮定と異なる場合は推定に悪影響を及ぼす。
17	Active Learning from Imperfect Labelers	https://arxiv.org/abs/1610.09730	Songbai Yan, Kamalika Chaudhuri, Tara Javidi/ University of California, San Diego	ラベル付が間違っているもの、ラベルが付けられていないもの含むデータに対しての学習を頑健にするための能動学習アルゴリズムを提案。（下限）性能保証付き。
18	COMPRESSED LEARNING: A DEEP NEURAL NETWORK APPROACH	https://arxiv.org/abs/1610.09615	Amir Adler, Michael Elad and Michael Zibulevsky	信号処理で使われているcompressed sensing手法を機械学習に転用したCompressed LearningをDLに適用。1層目でsensing matrixを学習（入力と同じunit数）。2層目のFC層で入力の1%までunit数を落とした場合（sensing rate=1%）でもerror rateは6.46%。実験データはMNIST。SoTA。
19	SoundNet: Learning Sound Representations from Unlabeled Video	https://arxiv.org/pdf/1610.09001.pdf	Yusuf Aytar, Carl Vondrick, Antonio Torralba	自然音のデータを利用して特徴認識(クラス分類)を行おうという話。学習に際しては、ラベルを学習済みの物体/シーン認識のモデルから取得し、それを音声の教師ラベルにするという手法。これで教師有りより高い性能を出せた。
20	Neural Machine Translation in Linear Time	https://arxiv.org/abs/1610.10099	Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, Koray Kavukcuoglu	イメージ的には、Encoder/DecoderをCNNで行うといった形。RNNはシーケンスを順に入れていかないので並列計算が難しく、前後の単語の関係を記憶するための機構を入れるとさらに計算が重たくなる。そこで、計算を並列でできるようにしつつ、単語間の関係も考慮できるようにということで考案。これをBytenetと名付けている。
21	Semi-Supervised Classification with Graph Convolutional Networks	https://arxiv.org/abs/1609.02907	Thomas N. Kipf, Max Welling	与えられたグラフのノードを半教師ありクラスタリングする。既存の研究では隣接ノードは同じクラスであるという前提を正則化項として加えていたためモデルのキャパシティが制限されていたが（エッジのもつ情報はsimilarityとは限らない）、グラフ構造をニューラルネットとして表現することでその制約を取り払った。各Layerにおいて隣接行列が登場するので例えば3層ネットワークの場合は3段飛びの関係が考慮されることになり、それが正則化項の代わりをする(と思われる)。
22	Deep Reinforcement Learning for Mention-Ranking Coreference Models	http://nlp.stanford.edu/pubs/clark2016deep.pdf	Kevin Clark, Christopher D. Manning	文中で同一のエンティティを言及している言葉(鈴木さん=彼など)を探索するタスク(Coreference Resolution)についての論文。手法としてAはBのことを指している(mention)、とするのを「行動」とみなし、強化学習の手法を用いて最適化している。GitHub実装有
23	Neural Architecture Search with Reinforcement Learning	http://openreview.net/forum?id=r1Ue8Hcxg	Barret Zoph, Quoc Le	CNNの各レイヤのパラメーター(幅や高さ)をRNNで決め、そのRNNがCNNの精度が最大になるパラメーターを出力するように、強化学習するという手法。なお普通にやっていると日が暮れるので、並列で実行する工夫も行われている。結果として、最高精度同等、またRNNではより良い結果が出た
24	Safe and efficient off-policy reinforcement learning	https://arxiv.org/abs/1606.02647	Rmi Munos (1), Tom Stepleton (1), Anna Harutyunyan (2), Marc G. Bellemare (1)	returnベースの方策オフ強化学習における安全で効率的なアルゴリズムの提案。安全とは、方策の"オフ具合"に対して性能がロバストであること。効率的とは、学習効率が良いこと。収束性の保証と実験を与えた。NIPS 2016に通っていて、真面目に解析を読むのはつらそう。
25	LipNet: Sentence-level Lipreading	https://arxiv.org/abs/1611.01599	Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de Freitas	唇の動きを読んでテキストにするという、DNNによる読唇術ともいえるもの。既存の語ベースではなく、文単位での認識を可能にした。時空間の畳み込み(STCNN)から、最終的には特徴ベクトルから直接テキストを生成するCTCを用いて文を生成。既存79.6%の精度を93.4と圧倒。
26	TUNING RECURRENT NEURAL NETWORKS WITH REINFORCEMENT LEARNING	https://arxiv.org/pdf/1611.02796.pdf	Natasha Jaques^{1,2}, Shixiang Gu^{1,3,4}, Richard E. Turner^{3}	RNNによる楽譜生成をよりよくするため、学習済のRNNから与えられる音符の生成確率と音楽理論により要請されるよい音符の条件を組合せたものを報酬とし、音符の生成をactionとする強化学習の問題を設定した。単にRNNより与えられる音符の連鎖を保持しつつ音楽理論によって与えられる条件をみたすよりよい楽譜を生成することができた
27	GENERATING INTERPRETABLE IMAGES WITH CONTROLLABLE STRUCTURE	http://openreview.net/pdf?id=Hyvw0L9el	S. Reed, A. van den Oord, N. Kalchbrenner, V. Bapst, M. Botvinick, N. de Freitas	テキスト以外にオブジェクトの範囲やキーポイントなどを渡して画像を生成する研究。よくあるGANではなくPixcelCNNを拡張した自己回帰モデルを利用しており(周辺ピクセルから妥当なピクセルを予測する)、より高速に学習できる利点がある。
28	A COMPOSITIONAL OBJECT-BASED APPROACH TO LEARNING PHYSICAL DYNAMICS	http://openreview.net/pdf?id=Bkab5dqxe	Michael Chang, Tomer Ullman, Antonio Torralba, Joshua B. Tenenbaum	NNで物理法則を表現しようという話。物理法則による変化を状態遷移モデルとして捉えて、状態を「着目物体」と「その周辺の物体(=コンテキスト)」で表現し、それらの状態を入力にしたLSTMで遷移を表現している。Matter.jsを使用した実験結果があり予測結果をgifで確認できる。
30	Improving Stochastic Gradient Descent with Feedback	https://arxiv.org/abs/1611.01505	Jayanth Koushik & Hiroaki Hayashi	Adamの変形で、目的関数からのフィードバック、具体的には目的関数の出力増減率の移動平均を加味してやることで性能が向上したというもの。論理コードがあるため、簡単に実装できそう。
31	Joint Multimodal Learning with Deep Generative Models	http://www.creativeai.net/posts/qD952SHHHAzCDeNGE/joint-multimodal-learning-with-deep-generative-models	Masahiro Suzuki, Kotaro Nakayama, Yutaka Matsuo	画像の特徴と、モダリティ(男性・女性など)の画像的特徴をそれぞれ学習させ、その結果を結合させる=元の画像に近く、かつ指定したモダリティの画像的特徴にも近い画像を生成する、という研究。画像のVAEにモダリティのVAEをJointさせると言うことで、JMVAEと命名。
33	Progressive Neural Networks	https://arxiv.org/abs/1606.04671	Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell	Progressive netというfine tuningのためのネットワークの提案，pre trained networkの各層の出力をfine tuneしたいnetworkの各層に重みづけして足し合わせる．fine tuneの際pre trained networkの重みについては変更せずfine tuneに入力するときの重みを学習することで，pre trainした結果を忘れることもなく、また複数のpre trained networkを重ねあわせることもできる。この手法自体の適用先は強化学習に限らないが強化学習タスクで評価。Fisher情報行列を評価してpre trainded networkのどの素性が寄与しているかを解析もしている。
34	PRIORITIZED EXPERIENCE REPLAY	https://arxiv.org/abs/1511.05952	Tom Schaul, John Quan, Ioannis Antonoglou and David Silver	Experience replayにおいて効率的にサンプルを学習するため優先度によってサンプリングすることを提案。優先度としてTD-errorを使用、importace samplingになるので重みを調節し、学習終了付近でのバイアスの影響をなくすため一様サンンプリング近づくようなannealingをしている。DQN,Double-DQNで評価しともにbaselineより良好。人工的なimbalanced dataを使い通常の識別学習への適用も評価している。
35	SKIP-GRAPH: LEARNING GRAPH EMBEDDINGS WITH AN ENCODER-DECODER MODEL	http://openreview.net/forum?id=BkSqjHqxg	John Boaz Lee & Xiangnan Kong	与えられたグラフの埋め込み表現を抽出する手法を提案。生成したグラフ上のランダムウォークのシーケンスに対しseq2vecと同様にベクタ表現を得る。分子構造データベースを使い識別タスクで他のグラフ埋め込み手法と比較。SoTa
37	GRAM: GRAPH-BASED ATTENTION MODEL FOR HEALTHCARE REPRESENTATION LEARNING	http://openreview.net/forum?id=SkgewU5ll	Edward Choi^1, Mohammad Taha Bahadori^1, Le Song^1, Walter F. Stewart2 & Jimeng Sun^1	診断記録のsequenceから病気をあてるタスク。各診断記録にのっている専門用語の集合を1つの入力とするRNNを構築する。各専門用語のベクタをそのまま入力に使わず、オントロジーを使い上位概念のベクタも考慮しattentionで上位概念を考慮した専門用語のベクタ作り入力とする手法GRAMを提案。単純にRNNを使った場合と比較しaccuracyで10%,AUCで3%の性能向上。
38	word2vec Parameter Learning Explained	https://arxiv.org/abs/1411.2738	Xin Rong	word2vecのパラメータ更新式の詳細な導出と説明を行っている論文。対象のモデルはCBOWとSkip-Gram。パラメータ学習の過程を詳細に解説した資料がなかったから書いたそうだ。
39	A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task	http://arxiv.org/abs/1506.03340	このタスクにおいて、必要な言語理解の程度、また精度上の上限を明らかにするのが本追試の目的。	DeepMindの教師なしによるQA回答(下)の追試。これは、文章中の固有表現をentityタグに置換して学習し、QAをentityの穴埋め問題として解くという手法。
41	word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method	https://arxiv.org/abs/1402.3722	Yoav Goldberg	ネガティブサンプリングの式の導出を詳しく解説している論文
42	Machine Solver for Physics Word Problems	https://openreview.net/forum?id=HyFkG45gl	Megan Leszczynski & Jose Moreira	"椅子が51mphで28度の角度で打ち上げられた。惑星ワトソンでは重力加速度を98m^2/sとせよ。最大の高さに達するにはかかる時間を求めよ。"のような力学の問題が与えられたとき回答を返すシステムを構築。具体的には2次元空間中で重力しか力のかからない自由粒子に関する自然言語で与えられる問題を想定。入力文にラベル付与するLSTMとラベル付与済の文からタスク抽出するLSTMとODEソルバーからなるシステム。テストケースで99.8%の正解率。
43	TUNING RECURRENT NEURAL NETWORKS WITH REINFORCEMENT LEARNING	https://arxiv.org/pdf/1611.02796v2.pdf	ブログ	音楽を生成するRNNを、強化学習で学習させるという方法。actionは音符を選ぶことで、Rewardは実際の曲的に出現しうるか＋音楽理論に沿っているか(いくつかの特徴量で設定)で与える。これにより、これまでより格段に音楽的に好ましくない性質は低減し、好ましい性質は高くすることができた。
44	Hierarchical Object Detection with Deep Reinforcement Learning	https://imatge-upc.github.io/detection-2016-nipsws/	Miriam Bellver, Xavier Giro-i-Nieto, Ferran Marques, Jordi Torres	物体認識を行う際、写真のどこに注目するかを決めて(右上・左上、など)切り取り、拡大する。そこからさらにどこに注目するか決め・・・と再帰的に繰り返すことで認識精度を上げるという研究。この挙動を強化学習で学習させる。重複には強くなったが、切り取りによる領域縮小で精度が下がったとのこと
45	Categorical Reparameterization with Gumbel-Softmax	http://blog.evjang.com/2016/11/tutorial-categorical-variational.html	Eric Jang, Shixiang Gu, Ben Poole	離散値の予測(=クラス分類など)を行う際の出力について。クラスの特定に使うargmaxなどは微分可能でないためうまく誤差伝搬できない。なので、連続/離散分布の特性を調整する変数を導入した方法を提案。その名はガンベル・ソフトMAX
46	Dynamic Coattention Networks For Question Answering	https://arxiv.org/abs/1611.01604	ブログ	一度だけ文書を読んで質問に答えるより、質問がわかってから見返せたほうがいいよね？ということで文書のencodeだけでなく、質問も掛け合わせたものを利用する(Coattention)、また一度で回答するのでなく何回か見直すことで局所最適を避けるという手法の提案。これでSOTAを更新
47	A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks	https://arxiv.org/abs/1611.01587	Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher	言語を扱う複数のタスクは、相互に有用な知識を持つはずだから、組み合わせたほうがいい精度が出るのでは、という話。品詞づけ・文節判定・係り受け・文意関係(補強・反対・普通)・文関係の度合い、といった複数のタスクをこなす一つのネットワークを構築し、最高精度を達成。
48	How to scale distributed deep learning?	https://arxiv.org/abs/1611.04581	Peter H. Jin, Qiaochu Yuan, Forrest Iandola, Kurt Keutzer/University of California, Berkeley	DeepLearningを行う上でボトルネックとなる学習時間を短縮するために、分散処理アルゴリズムとしてgossiping SGDという手法を提案。従来の同期アプローチでは障害（1ノードの計算失敗など）に弱く、非同期アプローチではparameterサーバを使用しているためパラメタが競合状態となり、学習が遅くなる。
49	Learning to reinforcement learn	https://arxiv.org/abs/1611.05763	Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick/DeepMind	強化学習は、精度は高いが大量のデータを必要とする。人間ならもうちょい効率よくやるのに・・・ということで、様々なタスクのモデル(RNN)の学習を強化学習でやることで、各タスク間の共通構造などを習得させられないか(習得できれば、他の新しいタスクの時に上手くやれる)という試み。
50	Spectral Convolution Networks	https://arxiv.org/abs/1611.05378	Maria Francesca, Arthur Hughes, David Gregg/Trinity College, Dublin	ラプラス変換やフーリエ変換を応用した計算量削減手法を、さらに効率的に改良した手法を提案。
51	Improved Particle Filters for Vehicle Localisation	https://arxiv.org/abs/1611.04790v1	Kira Kempinska, John Shawe-Taylor	粒子フィルタは移動物体の位置推定問題で多用されている。
52	Recurrnt Neural Network Grammars	https://arxiv.org/pdf/1602.07776v4.pdf	Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A. Smith	RNNを発展させたRecurrnt Neural Network Grammars（RNND）を用いて
53	Zero-Shot Visual Question Answering	https://arxiv.org/abs/1611.05546	Damien Teney, Anton van den Hengel	画像を見て質問に答えるタスクでは、学習した画像についてだけ答えられる、良くある答え(「2つ」とか)を多めに繰り出して精度が上がっているなど明らかな過適合が見られた。そこで真実見たことない画像(Zero-Shot)に回答可能かをテストするためのデータとベースラインモデルの提案
54	Lip Reading Sentences in the Wild	https://arxiv.org/abs/1611.05358	Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zisserman	DNNによる読唇術の続編。画像に加え、音声を用い(MFCCをLSTMでencode)、これらにAttentionを張った文字ベースのLSTM(Attend and Spell)で挑戦。※音声は使わない版も検証。BBCニュースの映像で検証したところ、プロを上回ることに成功。
55	Equality of Opportunity in Supervised Learning	https://drive.google.com/file/d/0B-wQVEjH9yuhanpyQjUwQS1JOTQ/view	Visualization	機械学習による判断が、特定集団にとって差別的にならないようにするには、という話。単純に収益最大化を目的とするとすると差別的になってくるので、True Positive(この実験では、ローンを返せると見込んだ人が実際に返せた割合)を平等にする=機会の平等を担保するのが重要という話。
56	REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS	https://arxiv.org/abs/1611.05397	Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver & Koray Kavukcuoglu	教師なしの補助タスクを同時に行う強化学習の手法UNsupervised REinforcement and Auxiliary Learning (UNREAL)を提案。画像入力3D迷路で従来手法に対し10倍の学習速度、人間の87%のスコア、Atariで人間の9倍のスコア。
57	Efficient Estimation of Word Representations in Vector Space	https://arxiv.org/abs/1301.3781	Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean	Word2vecの本家の論文。モデルの詳細についてはほとんど記述されていないし目的ともしていない。それより、既存モデル(NNLM、RNNLM)と提案モデル(Skip-Gram、CBOW)について計算量や性能の比較を行っている。結果として、既存手法より性能が高く計算量は小さかった。また、ベクトルの足し引きで興味深い結果(ex: vector("biggest")-vector("big")+vector("small")=vector("smallest"))を得られることがわかった。
58	Distributed Representations of Words and Phrases and their Compositionality	https://arxiv.org/abs/1310.4546	Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean	目的はskip-gramの高速化とより良いword vectorを得ること。またフレーズに対する学習を行うことも目的としている。高速化のために階層的ソフトマックス、ネガティブサンプリング、サブサンプリングを用いている。結果として、高速化しつつ良いword vectorを得られている。skip-gramに初めてネガティブサンプリングを使った論文のようだが説明が簡素なので、ネガティブサンプリングについて理解するには
59	Recurrent neural network based language model	http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf	Tomas Mikolov, Martin Karafit, Luks Burget, Jan Cernock, Sanjeev Khudanpur	word2vecの基となった論文の一つ。音声認識にRecurrent Neural Network based Language Model(RNNLM)使ったら良い結果を得られたという論文。従来のNNLMの欠点として、固定長コンテキストしか扱えず、設定するパラメータ数が多いということが挙げられる。これらの問題をRNNを使うことで解決できた。
60	Deep Learning with Sets and Point Clouds	https://arxiv.org/abs/1611.04500	Siamak Ravanbakhsh, Jeff Schneider, Barnabas Poczos	データセットの中の各データを「点」と考えると、データセットは各点を関連付ける「構造」(点をつなぐ構造=グラフ構造)を持っていると考えることができる。この構造として何パターンかのシンプルな定義を行い、データセットへ適用してみることで「構造からの外れ値」の検出などを行っている。
61	Robust end-to-end deep audiovisual speech recognition	https://arxiv.org/abs/1611.06986	Ramon Sanabria, Florian Metze, Fernando De La Torre	LipNetに先を越された感はあるが、音声認識に画像特徴量を組み合わせる試み。ノイズありの環境では、あらかじめノイズありの音声で学習＋口のあたりの画像特徴量を併用するのが良い結果になるとの結果。ノイズなしで学習させた場合、画像を組み合わせても精度が出なくなるのは重要な示唆。
62	Image-to-Image Translation with Conditional Adversarial Nets	https://phillipi.github.io/pix2pix/	Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros	最近流行のconditional GAN(条件指定の画像生成)。ある画像(イラスト)から画像(写真)を作り出す、画像翻訳ともいえるもの。特徴量比較でなく、GANの仕組みで「ペアかどうか」を真偽判定させる形で学習。少ないデータ(数百件・数時間 on GPU)でも学習可能なことを確認
63	Finding Alternate Features in Lasso	https://arxiv.org/abs/1611.05940	Satoshi Hara/National Institute of Informatics	Lasso最適解では無視されてしまう、モデルの結果の性能に影響を与える重要な変数を抽出する手法の提案。
65	Generalized Dropout	https://arxiv.org/abs/1611.06791	Suraj Srinivas, R. Venkatesh Babu	ニューラルネットの重みをベイズ推定するBNNは、すべての重みが互いに独立なのは過程として無理がある。そこで、重みの相関を表現するゲートを設けるという話。ノードの接続を調整するという面でDropOutの拡張、Dropout++と命名。
66	Grad-CAM: Why did you say that?	https://arxiv.org/abs/1611.07450	Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra/	CNNモデルの出力結果に対する可視化手法であるGrad-CAMの提案。入力画像に対する注目領域の可視化を行う。image captioningやvisual question answering(VQA)モデルにも適用可能。
67	Aggregated Residual Transformations for Deep Neural Networks	https://arxiv.org/abs/1611.05431	Saining Xie, Ross Girshick, Piotr Dollr, Zhuowen Tu, Kaiming He/	ResNetの拡張版のResNeXtを提案。block内にinceptionモデルような構造を持たせた、“cardinality”という構造に変更。単純に層を深く、広くするよりも効果的にaccuracyの向上が行える。パラメタ数や時間計算量も通常のResNetと同程度。
68	Residual Networks Behave Like Ensembles of Relatively Shallow Networks	https://vision.cornell.edu/se3/wp-content/uploads/2016/10/nips_camera_ready_draft.pdf	Andreas Veit, Michael Wilber, Serge Belongie/Cornell University	ResNetの学習の振る舞いを調査した研究。勾配消失が何故防げているのかを解き明かしている。
69	Cost-Sensitive Learning of Deep Feature Representations from Imbalanced Data	https://arxiv.org/abs/1508.03422	Salman H. Khan, Mohammed Bennamoun, Ferdous Sohel, Roberto Togneri/	不均衡データに対して頑健に学習（特徴量抽出）を行うために、Cost-Sensitive Learning手法をnetworkの中に組み込み、自動でclass依存性のcost matrixを学習しようという提案。cost matrix用のlossは混合分布をベースとて算出、epoch毎に1度更新。
70	Generalizing Skills with Semi-Supervised Reinforcement Learning	https://openreview.net/forum?id=ryHlUtqge	Chelsea Finn^1, Tianhe Yu^1, Justin Fu^1, Pieter Abbeel^{1,2}, Sergey Levine^1	報酬ありMDPと報酬なしMDP、両方使い強化学習するsemi-supervised reinforcement learning(半教師あり強化学習)を考え、その学習アルゴリズムsemi-supervised skill generalization(S3G)を提案。報酬ありなしの情報を両方使い汎化できていることが確かめられた。
71	Linguistic Regularities in Continuous Space Word Representations	http://www.aclweb.org/anthology/N13-1090	Tomas Mikolov, Wen-tau Yih, Geoffrey Zweig	word2vecの基となった論文の一つ。
72	GloVe: Global Vectors for Word Representation	http://nlp.stanford.edu/pubs/glove.pdf	Jeffrey Pennington, Richard Socher, Christopher D. Manning	単語ベクトルを学習するためのモデルであるGloVeの論文。グローバルな行列分解のモデルとlocal context windowのモデルを組み合わせてよい単語ベクトルを学習した。評価はアナロジータスクとNERで行っている。
73	Self-Supervised Video Representation Learning With Odd-One-Out	https://arxiv.org/pdf/1611.06646.pdf	Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould	時間順序をひっくり返したビデオクリップをひっくり返していないものと区別させるタスクで学習させるのでアノーテーション不要。UCF101データセットでの（アノーテーションありの）行動分類タスクにこのself-supervisedなtrainingを併用した場合、およそ10%のperformace向上をするらしい。
74	Object Recognition with and without Objects	https://arxiv.org/pdf/1611.06596.pdf	Zhuotun Zhu, Lingxi Xie, lan L. Yuille	object recognitionでは人間がAlexnetに勝つ。この論文は、objectをマスクして背景だけにしてobjectを推測させると、人間よりAlexnetの方が上ということを確かめた。またobjectだけにすると人間が勝つことも。deep CNNは認識において背景情報をやはりかなり利用していることが分かる。
75	Evaluation methods for unsupervised word embeddings	http://www.aclweb.org/anthology/D15-1036	Tobias Schnabel, Igor Labutov, David Mimno, Thorsten Joachims	word embeddingの評価手法に関するサーベイ論文。既存の評価手法を解説しつつ、クラウドソーシングを用いた新しい評価手法を提案し、比較を行っている。結果として、既存の評価手法と新しい評価手法の結果が類似していることがわかった。
76	Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition	https://arxiv.org/abs/1610.09975	Hagen Soltau, Hank Liao, Hasim Sak	音声認識システムにおいて語彙数が多いときは単語より小さい音声単位を出力せざるを得ないと考えられていた。しかしこの論文では１０万語彙で単語を直接出力するシステムが可能であることを示した。そのためモデルは従来よりも単純化できて、双方向LSTMとCTS lossを使いend-to-end trainingされている。12万5千時間のsemi-supervisedなデータセットが単語単位モデルにおけるデータ不足の問題を解消した。
77	Improving Word Representations via Global Context and Multiple Word Prototypes	http://www.aclweb.org/anthology/P12-1092	Eric H. Huang, Richard Socher, Christopher D. Manning, Andrew Y. Ng	GloVeの基となった論文(多分)。従来の単語表現にはlocal contextしか使っていない、一つの単語につき一つの表現という問題があった。提案モデルでは、local contextとglobal contextを組み合わせて使用し、また一単語につき複数のembeddingsを学習することで性能向上を図った。
78	HOW MUCH DATA IS NEEDED TO TRAIN A MEDICAL IMAGE DEEP LEARNING SYSTEM TO ACHIEVE NECES - SARY HIGH ACCURACY ?	https://pdfs.semanticscholar.org/a21d/e9f6408b333d917f7a8b2585230ed8b6c57a.pdf	Junghwan Cho, Kyewook Lee, Ellie Shin, Garry Choy, and Synho Do	CT画像の異常状態の分類器において何枚の画像が必要か、を実験した論文。結果としては100枚以上からは精度の向上はあまり見られなかった。使用したNetworkはGoogLeNet。
79	Learning Python Code Suggestion with a Sparse Pointer Network	https://arxiv.org/abs/1611.08307	Avishkar Bhoopchand, Tim Rocktschel, Earl Barr, Sebastian Riedel	Pythonのコード補完をRNN言語モデルで行う話。Attentionをclassやfunctionの宣言にはることで、予測精度を上げている。なるべく品質の高いPythonコードを使用するため、GitHubでStar100以上のリポジトリ、かつfolkが多いものを学習データとして使用。
80	Bidirectional Attention Flow for Machine Comprehension	https://arxiv.org/abs/1611.01603	Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi	SQuADで最高精度を更新した論文。文字・単語・フレーズ、その上に文書/クエリの関連(Attention)、さらにそれらの関連、出力、という階層型のモデル。Attentionとencodeの役割分担が肝とのこと。
81	Awesome Typography: Statistics-Based Text Effects Transfer	https://arxiv.org/abs/1611.09026	Shuai Yang, Jiaying Liu, Zhouhui Lian, Zongming Guo	StyleTransferをテキストに応用し、かっこいいフォントを作るというもの。フォントにかかるエフェクトは細かく、単純にStyleTransferをかけても上手くいかない。そこで、既存のフォントの特性を数学的に定義していって、それを適用していくという手法を取っている。
82	A Neural Conversational Model	https://arxiv.org/pdf/1506.05869.pdf	Oriol Vinyals, Quoc V. Le	sequence2sequenceモデルを対話に使ったシンプルなモデルで対話を実現した。評価についてはある質問に対する答え方を既存の対話システム(CleverBot)と比較することで行った。提案モデルでは200の質問の内97が好ましく、CleverBotは200の内60が好ましいという結果となった。
83	Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation	https://arxiv.org/abs/1606.00776	Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula, Bowen Zhou, Yoshua Bengio, Aaron Courville	RNNはLSTMとかの構造をいじるのが主流になっているけれど、入力を工夫した方がいいんじゃない？ということで、入力列を通常の文字列とそこから「粗い(Coarse)」情報を抽出したものとを並列で入力するモデルを提案している(=multi resolution)。
84	Dialogue Learning With Human-In-The-Loop	https://arxiv.org/abs/1611.09823	Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc'Aurelio Ranzato, Jason Weston	対話中の学習を可能にするため、Memory Networkと強化学習を組み合わせる手法の提案。正しい回答「だけ」を模倣するよう学習するモデル(RBI)と、返答から報酬を推定するモデル(FP)を検証。双方有効なことを確認。
85	High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis	https://arxiv.org/abs/1611.09969	Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, Hao Li	画像の補完を、高解像度で行う手法の提案。単に画像全体だけでなく、周辺のテクスチャのパターンとの差異も見ることで従来手法より良い(loss的には微減という感じだが)補完を実現。
86	User Personalized Satisfaction Prediction via Multiple Instance Deep Learning	https://arxiv.org/abs/1611.08096	Zheqian Chen, Ben Gao, Huimin Zhang, Zhou Zhao, Deng Cai	コミュニティ内での質問に対して、質問者が満足する適切な回答を素早く返す事モデルを作れないか、というのを研究した論文。深層学習とmultiple instance learning(弱教師あり学習)の枠組を手法として用いている。従来手法では特徴量をハンドメイキングしていたが、本手法では特徴量抽出は自動で行われるため、従来手法に比べ、性能だけでなく説明力も向上。
87	Fully Convolutional Crowd Counting On Highly Congested Scenes	https://arxiv.org/abs/1612.00220	Mark Marsden, Kevin McGuinness, Suzanne Little and Noel E. O’Connor	FCNベースの人混みの中の人物数カウントモデルの提案。実際に提案しているのはaugmentation手法。入力画像のcropを行う際に4象限っぽく、4つにcropした画像を入力とし、入力データとして重なりをなくした。これによりoverfitのリスクを減らしている。また、test時の入力画像の大きさを50%downsamplingしても精度が大きく変わらず、計算速度は4倍早くなる事を指摘。SoTA。
88	Captioning Images with Diverse Objects	https://arxiv.org/abs/1606.07770	Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach	Novel Object Captioningタスクで、(1) 画像とキャプションが対となっていないデータの活用/学習、(2) 複数ソースからのデータの活用、(3)pre-training済みのembedding空間の活用を行えるようにした。これにより、zero-shotなデータに対してもcaption生成が可能。手法としては、visual recognition network(画像のみのencode)、caption model(caption generation)、language model(sentence generation)を別々のデータソースで学習。そのパラメタを共有するというもの。従来手法と比較してzero-shotデータに対してMS COCOで10%、ImageNetで20%のF1を改善。
89	Playing Doom with SLAM-Augmented Deep Reinforcement Learning	https://arxiv.org/abs/1612.00380	Shehroze Bhatti, Alban Desmaison, Ondrej Miksik, Nantas Nardelli, N. Siddharth, Philip H. S. Torr	DQNを3次元に適用すると、次元が増えた分探索が疎になり学習が非常に難しい。そこで、SLAMによる自己位置推定と、Faster-RCNNによる物体検出で俯瞰図(=神の視点)を作って補助情報として入れてやる手法の提案。単純なDQNより学習の初速が速く、総報酬も2倍超となった。
90	ATTEND, ADAPT AND TRANSFER: ATTENTIVE DEEP ARCHITECTURE FOR ADAPTIVE TRANSFER FROM MULTIPLE SOURCES IN THE SAME DOMAIN	https://openreview.net/pdf?id=Sy6iJDqlx	Janarthanan Rajendran, Aravind Lakshminarayanan, Mitesh M. Khapra, Prasanna P, Balaraman Ravindran	ドメインが同じ転移学習を凸結合によって行う手法の提案。凸結合の係数を、どのタスクにどのくらい注目すべきかというスコアを出力するattention networkを使って求める。強化学習の方策や価値関数を転移学習させて実験した。転移させない学習と比べて高速に学習できたが、転移学習における他の結合方法との比較はないように見える。
91	DeepWalk: Online Learning of Social Representations	http://dl.acm.org/citation.cfm?id=2623732	Bryan Perozzi, Rami Al-Rfou and Steven Skiena	与えられたグラフ上のノードの埋め込み表現を得る方法を提案。着目するノードを始点とするランダムウォークをコンテキストとして始点ノードを予測するskip-gramによって埋め込み表現を得る。大規模グラフをターゲットとしデータセットとしてブロガー、flickr、youtubeのソーシャルネットワークを使って評価。グラフ中のいくつかのノードを訓練データとして学習し、訓練データ以外のノードラベルを埋め込みベクトルを入力とするlogistic regressionで推定した結果を評価。(当時のまだDNN的手法が一般的でない)既存手法をうわまわる精度。
92	QUASI-RECURRENT NEURAL NETWORKS	https://arxiv.org/pdf/1611.01576v1.pdf	ブログ	RNNでは1wordごとに処理するので並列処理できないし、前の隠れ層からの入力を受け続けることで隠れ層はいろんな単語の情報がミックスされた謎の何かになる。そこでCNNにより並列処理＋隠れ層を前回独立にキープして出力を計算するブロックを発明。その名はQRNN。Chainer実装有。
93	LINE: Large-scale Information Network Embedding	http://dl.acm.org/citation.cfm?id=2741093	Jian Tang^1, Meng Qu^2, Mingzhe Wang^2, Ming Zhang^2, Jun Yan^1, Qiaozhu Mei^3	大規模グラフを想定したノードの埋め込み表現を得る手法LINEの提案。ノードi,jの接続確率p(i,j)が埋め込みベクトルを用いてp(i,j)~exp((v_i,v_j))なるモデルを考え。最近接、第2最近接までの接続確率分布を学習し埋め込みベクトルを得る。negative samplingと非同期SGDアップデートで学習の高速化。共起語ネットワーク、filckr,youtube社会ネットワーク, citationネットワークで評価し既存手法(skip-gram,deepwalk含む)より高性能、高速。スケーラビリティーもよいので大規模グラフに適用化。
94	Learning to learn by gradient descent by gradient descent	https://arxiv.org/abs/1606.04474	Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas	ニューラルネットの学習ってなんだかんだ手作業orルールベースだよね、ということで、「学習のさせ方」のノウハウを学習するという研究。ターゲットのネットワークの重みを、RNN(2層LSTM)を使って推定させる。AdamやRMSpropより優秀な結果。TensorFlow実装有り。
95	Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning	https://arxiv.org/abs/1612.01887	Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher	image caption generateタスクにおいて、attentionの際にwhereだけでなくwhenにも着目した論文。“the”や“of”はvisualizeさせる意味はないため、次の単語を予測する際にattentionを行う必要があるかどうかを判断するvisual sentinel(sentinel gate)をネットワークに追加。whenはsentinel gateが、 whereはspatial attentionが担う。Spatial Attention Modelの提案とvisual sentinelを構成要素として追加したAdaptive Attention Modelの提案を行っている。SoTA。
96	Deep Watershed Transform for Instance Segmentation	https://arxiv.org/abs/1611.08303	Min Bai, Raquel Urtasun	instance segmentationは複雑なpiplineで構成されたものが多かったが、本稿ではシンプルなネットワーク構成で当該タスクの実行を可能とした。watershedアルゴリズム（領域分割アルゴリズム）とDeep Learningモデルのあわせ技で、watershed transformも含めてend-to-endで学習を行う。RNNなどのiterationが必要なアルゴリズムを使用していないため、１画像中のobject数が多くても高速に推定が可能。従来のwatershed transformアルゴリズムとの違いとしては、各instanceのエネルギー分布の高さが大体同じになるように学習を行っている。入力はRGB。SoTA。
97	Enriching Word Vectors with Subword Information	https://arxiv.org/pdf/1607.04606v1.pdf	Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov	FastTextの論文。現在の単語表現を獲得するモデルの多くは単語の形態素を無視している。この論文ではこれら形態素を考慮するために各単語を文字ngramで表現し、それらのベクトル表現を学習している。その評価は単語類似度とアナロジータスクで行った。
98	Bag of Tricks for Efficient Text Classification	https://arxiv.org/pdf/1607.01759.pdf	Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov	fastTextを使ってテキスト分類をしてみた論文。具体的には、タグ予測と評価分析を行っている。accuracyでは深層学習モデルの分類器と同等で学習は数十倍速いという結果になった。
99	Distributed Representations of Sentences and Documents	https://arxiv.org/abs/1405.4053	Quoc V. Le, Tomas Mikolov	いわゆるDoc2vecの基となった論文。bag-of-wordsでは語順や単語の意味を無視するという弱点があったのでそれを克服するためにParagrapheベクトルを提案。評価分析と情報検索で評価したところ、state-of-the-artより良い結果となった。
100	Skip-Thought Vectors	https://arxiv.org/abs/1506.06726	Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler	encoder-decoderモデルを使って文の分散表現を学習しようという話。学習方法はSkip-gramに似ていて、ある文から周りの文を予測することを通じて文の分散表現を学習する。得られたベクトルについては8つのタスクで評価しており、まずまずの結果となっていた。
101	Item2Vec: Neural Item Embedding for Collaborative Filtering	https://arxiv.org/abs/1603.04259	Oren Barkan, Noam Koenigstein	Skip-gramをちょっと変えたものを協調フィルタリングに使った話。Skip-gramのようなモデルを使ってItemに対するベクトルを生成し、Item間の類似度から推薦する。SVDを用いた手法に匹敵する結果を得られた。特にunpopularなitemに関してはSVDより良い結果となった。
104	Two/Too Simple Adaptations of Word2Vec for Syntax Problems	http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf	Wang Ling, Chris Dyer, Alan Black, Isabel Trancoso	品詞タグ付や係り受け解析のようなsyntaxが重要なタスクに適したword embeddingを生成する手法を提案。Mikolovらのモデルでは語順を無視していたのに対して、提案モデルではそのあたりを考慮。結果として品詞タグ付と係り受け解析の性能が向上した。比較相手はSENNA。
105	Multi-Prototype Vector-Space Models of Word Meaning	http://www.cs.utexas.edu/~ml/papers/reisinger.naacl-2010.pdf	Joseph Reisinger, Raymond J. Mooney	単語のあいまい性を考慮して、語義ごとのベクトル表現を生成することを提案している。具体的には、語義ごとのベクトルを生成するために、クラスタリング(movMF)を使う手法を提案している。評価については人間が判断した意味類似度と比較している。
106	StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks	https://arxiv.org/abs/1612.03242	Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, Dimitris Metaxas	テキストから画像を生成する話で、単純なEncode情報+GANからさらに解像度を上げるためのGANを重ねて(Stackして)画像の鮮明度を上げるという話。
107	Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space	http://www.evolvingai.org/ppgn	Nguyen A, Yosinski J, Bengio Y, Dosovitskiy A, Clune J	Encoderを取り替える(Plug&Play)ことで生成する画像を変えられるという話。Encoderは画像だけでなくテキストでもOK。Encoder側の制約(このベクトルなら分類=猫のはず、など)と画像らしさ、2つの制約内で生成することでバリエーション豊かな画像生成ができた。
108	SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth	http://robotvault.bitbucket.org/scenenet-rgbd.html	John McCormac	シーン認識のための学習データセットSceneNet RGB-Dの公開。物理シミュレーターでシーン(部屋の中にものが散らばった環境)を作り、そこでカメラの軌跡を設定し映像を作製、その映像のRGB+Depthをデータ化、という感じで生成
109	Coupling Adaptive Batch Sizes with Learning Rates	https://arxiv.org/abs/1612.05086	Lukas Balles, Javier Romero, Philipp Hennig	batch sizeを調整するCABSというルールを提案。mini batch sizeは確率的勾配のバリアンスに影響し、収束速度に大きく関係するため、重要なパラメタである。learning rateが大きい（小さい）場合はbatch sizeを大きく（小さく）する必要があるが（(19)式の関係）、本手法ではその関係性に基いて自動でbatch sizeの調整を行う。従来手法と比較してSGDでの収束速度が向上。
110	Finding Tiny Faces	https://arxiv.org/abs/1612.04402	Peiyun Hu, Deva Ramanan	small objectをdetectionする際の難しさの解析と改善を行った論文。題材は顔検出。templateのscale、画像解像度、contextの必要性について解析を行っている。小さな画像を引き伸ばすと認識精度は下がり、人間も機械も小さな物体を探すにはcontextをかなり必要としている事を検証。また、one-size-fits-allのようなtemplateのサイズを1つに固定すると認識に限界があるため、入力画像を2倍、0.5倍し、originalとともに独立の3つのネットワークで特徴抽出しdetectionを行い、その後マージするという方法を取っている。SoTA。
111	IMPROVING NEURAL LANGUAGE MODELS WITH A CONTINUOUS CACHE	https://arxiv.org/abs/1612.04426	Edouard Grave, Armand Joulin, Nicolas Usunier	Neural Cache Modelという汎用的なcache modelの提案。既存のneural network language modelに適用可能。pre-trainingされたモデルにattachするだけで良く、fine tune不要。cache対象はhidden stateとwordの対。neural cache language modelでは、現在見ている単語から推定される次の単語の確率と現在までのcacheから推定される確率の加重平均により次の単語の推定が行われる。
112	Model-Free Episodic Control	https://arxiv.org/abs/1606.04460v1	Charles Blundell+, DeepMind	強化学習において、価値観数をパラメトリックに推定するDQNなどの方法は学習の初期段階で非効率的であり、少ない事例に対する汎化能力は低い。これに対して、提案手法はある状態の価値関数を、K-nearest-neighborsのような事例ベースのノンパラメトリックな手法で与えている。このとき、neighborsを決める類似度は変分オートエンコーダのような教師なし学習で得た表現をもとに計算する。論文ではさらに、実際の脳では、ノンパラメトリックな学習を行う海馬がエピソード的で急速な学習を行い、パラメトリックな学習を行う大脳皮質が汎用的で長期的な学習を行うと主張している。
113	Not All Neural Embeddings are Born Equal	https://arxiv.org/abs/1410.0718	Felix Hill, KyungHyun Cho, Sebastien Jean, Coline Devin, Yoshua Bengio	ニューラル機械翻訳モデルによってword embeddingsを獲得した論文。Skip-gramのような単一言語モデルより翻訳モデルの方が概念間の類似度を捉えられるのではないかということを検証している。単一言語モデルと比較した結果、ニューラル翻訳モデルは概念間の類似度をより良く捉えられ、単一言語モデルは関連性をより良く捉えられることが示唆された。
114	Object detection can be improved using human-derived contextual expectations	https://arxiv.org/abs/1611.07218	Harish Katti, Marius V. Peelen, S. P. Arun	人間のシーン情報などのコンテキスト情報の活用の仕方を取り入れて、物体認識の性能を向上させるための提案を行った。HOG, Gaussian blurなどの従来手法を積極的に取り入れ、人間(車)が写ってそうなシーンか、写っていそうな場所であるかのaverage likelihoodを算出。算出した数値とCNNの特徴量を合わせて使用。CNN単体では確信度が低い物体やシーンに対して効果的という結果が得られた。
115	LightRNN: Memory and Computation-Efficient Recurrent Neural Networks	https://arxiv.org/abs/1610.09893	Xiang Li, Tao Qin, Jian Yang, Tie-Yan Liu	RNNにおいてaccuracyを犠牲にせずに時間/空間計算量を削減可能なLightRNNの提案。embedding空間を1単語1vectorで表現するのではなく、tableに分解して表現(Figure1 参照)。これにより単語数がVの場合、2\sqrt{V}個のembedding vectorで空間を表現出来る。したがって、このtableを構築する部分がLightRNNの勘所。構築したtableを使用し、1単語毎にrow, colmn別にhidden stateを計算する。
116	MS MARCO: A Human Generated MAchine Reading COmprehension Dataset	https://arxiv.org/pdf/1611.09268v1.pdf	Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder and Li Deng	Microsoftが公開した質問応答のデータセット(10万件)。質問/回答が、人間のものである点が特徴(Bing=検索エンジンへの入力なのでどこまで質問っぽいかは要確認)。回答はBingの検索結果から抜粋して作成。
117	A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning	http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.8551&rep=rep1&type=pdf	Ronan Collobert, Jason Weston	NLPの様々なタスク(NER, POS, チャンキング, 言語モデル, SRL)を同時に学習させることで汎化性能を向上させようという話。具体的には同時に学習するためのニューラルネットワークの枠組みを提案している。結果として、タスク単体で学習させるより複数のタスクを同時に学習させた方が良い結果となった。
118	Dependency-Based Word Embeddings	https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf	Omer Levy, Yoav Goldberg	Skip-gramのコンテキストは周辺語しか考えていないけどそれを一般化する話。具体的には、コンテキストとして依存関係を用いて実験を行った。実験を行ったところ、依存関係ベースのembeddingsは周辺語ベースのものに比べて機能的類似性が高いことがわかった。
119	Automatically Processing Tweets from Gang-Involved Youth: Towards Detecting Loss and Aggression	https://aclweb.org/anthology/C/C16/C16-1207.pdf	Terra Blevins, Robert Kwiatkowski, Jamie Macbeth, Kathleen McKeown, Desmond Upton Patton, Owen Rambow	ギャングによる暴力を未然に検出、抑制する研究。ギャングにおける中心的な女性メンバと若手のメンバのTwitterのやり取りを分析。どのような対話がエスカレートしやすいのかについて、アノテーションを行いコーパスを作成した。タグは侵略(aggression=相手の攻撃)、喪失(loss=仲間の死など)といったものが付与されている。
120	Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences	https://arxiv.org/abs/1610.09513	Daniel Neil, Michael Pfeiffer, Shih-Chii Liu	何かのイベント(手を上げたらとか)をトリガに発生する時系列データは、挙動があるタイミングが疎のため既存のLSTMでは学習が難しい(混在するならなおさら)。そこで、時間情報から今ONなのかOFFなのかを制御するPhaseゲートを設けて学習する話。TensorFlow実装有
121	Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks	https://arxiv.org/abs/1612.05424	Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan	ラベル付けされたデータを作るのはとても大変だから、データのあるソースからデータのあまりないターゲットの画像が作れたらいいよね、という話。ソース⇒ターゲットの画像生成を行うGANを作成し、ドメイン適用を実現。生成画像だけを使った学習で、ほかの手法に対し最高精度を達成。
122	Language to Logical Form with Neural Attention	https://arxiv.org/abs/1601.01280	Li Dong, Mirella Lapata	Encoder-Decoderモデル(with Attention)を使って、文を論理式に変換するという研究(例: 面先が一番大きい県の人口は？⇒(人口: i (argmax _ (県:k _) (面積:i _)))) など。Lisp的。
123	Data-driven Fluid Simulations using Regression Forests	http://dl.acm.org/citation.cfm?doid=2816795.2818129	L’ubor Ladick´y, SoHyeon Jeong, Barbara Solenthaler, Marc Pollefeys, Markus Gross	粒子法による流体シミュレーションをRegression Forestを使った回帰モデルで近似的に行い高速化した研究。入力は各粒子の位置と速度。NS方程式を基に作られたモデルによって、各粒子の位置と速度から特徴量を生成。特徴量は圧力、表面張力、粘性、非圧縮性制約に関連したもの（圧力などそれ自体の値を求めてはいない）。これをRegression Forestに入力し各粒子の加速度（または速度）の推定を行い、時間ステップを使って数値積分をして、次ステップの粒子位置と速度を計算。速度的には従来手法(PBF)の200倍強高速。
124	Accelerating Eulerian Fluid Simulation With Convolutional Networks	https://arxiv.org/abs/1607.03597	Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, Ken Perlin	オイラー的手法（差分法）とCNNを組み合わせて、非圧縮流体シミュレーションを行う事を提案。CNNを使っているのは通常線形凸最適化手法が適用される圧力算出部分。CNNへの入力は現在の格子点の速度, 位置, 圧力。NS方程式の粘性項を無視。半教師あり学習で使われている手法をloss(推定速度のダイバージェンス)に導入。これにより収束性が相当改善（入れないと収束しない）。計算速度的にはSoTA。（煙のような）スパースな流体で威力を発揮する。
125	Linguistic Regularities in Sparse and Explicit Word Representations	https://www.cs.bgu.ac.il/~yoavg/publications/conll2014analogies.pdf	Omer Levy, Yoav Goldberg	word embeddingにおけるアナロジータスクを解くための類似度計算手法を検証した論文。検証の結果、Neural embeddingでなく従来の単語共起を用いた手法でも、類似度計算手法によってはstate-of-the-artな結果を出せることがわかった。
126	Domain Separation Networks	https://www.cs.bgu.ac.il/~yoavg/publications/conll2014analogies.pdf	Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan and Dumitru Erhan	DNNのラベル無しデータをターゲットとしたドメインアダプテーションにおいてネットワークをドメイン依存部分とドメイン共通部分を分離するようにソース、ターゲット両データで学習し適用するネットワーク構造の提案。識別はドメイン共通素性ネットワークを使う。教師なしドメインアダプテーションのSoTA。
127	Quad-networks: unsupervised learning to rank for interest point detection	https://arxiv.org/abs/1611.07571	Nikolay Savinov, Akihito Seki, Lubor Ladicky, Torsten Sattler, Marc Pollefeys	位置, 回転, スケールについて不変量と共変量を考慮したDLによる特徴点抽出手法を提案。性能としては既存手法と同程度だが、クロスモーダルなデータ間で再現可能性があることが特徴。学習方法は、2つの異なるアングルの画像で同一位置のpatchを切り出して、forward処理。response functionを算出し、2つの値をhingi lossで評価。完全な教師なし学習では、同一画像で異なるaugmentationを掛けたものを入力として評価している。baselineはDoG。
128	Interaction Networks for Learning about Objects, Relations and Physics	https://arxiv.org/abs/1612.00222	Peter W. Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, Koray Kavukcuoglu/ DeepMind	多体問題などの相互作用のある複雑な物理法則を物体とその関係性に分解することで、推論可能とした手法の提案。ネットワークへの入力、は物体（ノード）とノード間の関係（エッジ）のグラフ。1つ目の近似関数（ネットワーク）で物体に対する効果量を計算し、外力とともに相互的な影響を加味した状態推定を行う関数（ネットワークに）に投入される。計算量的に大規模なものには対応出来ていないが、手法としては汎用性が高く一般的な多くの物理現象に適用可能。
129	Learning from Simulated and Unsupervised Images through Adversarial Training	https://arxiv.org/abs/1612.07828	Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, Russ Webb	合成画像で学習できると良いけど独特のクセにより上手くいかない、という点を克服する試み。より「本物らしく」するNN対見破るNNで学習(GAN)。元からかけ離れた「本物化」を防ぐため元画像との差異を利用した正規化などの試みがとられている
130	Language Modeling with Gated Convolutional Networks	https://arxiv.org/abs/1612.08083	Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier	言語モデルのタスクで、CNNでLSTM同等以上の精度を出したという話。畳み込んだ結果をGRUに近い機構で処理し、過去の情報が消失しないようにしている。Google Billion Wordのデータセットでは、LSTMと同等の精度を出す一方計算効率が20倍程度改善された。
131	How to Generate a Good Word Embedding?	https://arxiv.org/abs/1507.05523	Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao	word embeddingの学習に重要なコンポーネントをモデル、コーパス、パラメータの3つとし、既存の手法を分類・比較した論文。比較評価を行い、良いword embeddingを学習するためのガイドラインを示した。
132	Better Word Representations with Recursive Neural Networks for Morphology	http://nlp.stanford.edu/~lmthang/data/papers/conll13_morpho.pdf	Minh-Thang Luong, Richard Socher, Christopher D. Manning	Recursive Neural Network(RNN)とニューラル言語モデルを組み合わせて形態素から単語ベクトル表現を構築することで、まれ語や複合語、未知語を上手く表現する手法を提案している。単語の類似度タスクで評価した結果、ほとんどすべてのデータセットで従来手法を超える性能を得られた。
133	Large-Scale Price Optimization via Network Flow	https://papers.nips.cc/paper/6301-large-scale-price-optimization-via-network-flow.pdf	Shinji Ito, Ryohei Fujimaki/NEC Corporation	商品の価格最適化に関する論文。利益を最大化するには大量の商品に対して価格-需要の関係性を明らかにする必要があるが、混合整数計画問法は計算量的に難しかった。当該論文ではnetwork flow algorithmsを使用することによって計算量の問題を解決。また、収益のsupermodularityと需要の交差弾力性との関係を明らかにし、その関係性による仮定を置くことで計算量を削減。仮説が成り立たない場合でも既存手法と同等の性能。最終的にはBQP(binary quadratic programming)に落とせるのでBQPを解く。
134	Improving Distributional Similarity with Lessons Learned from Word Embeddings	http://www.aclweb.org/anthology/Q15-1016	Omer Levy, Yoav Goldberg, Ido Dagan	word embeddingの性能向上はアルゴリズムでなくハイパーパラメータのチューニングによるところが大きいのではないかということで検証した論文。様々なハイパーパラメータの組み合わせで検証した結果、word similarityタスクにおいて、従来のcount-baseの手法でもpredict-baseの手法と同等の性能を示した。ただし、analogy-taskにおいてはpredict-base手法の方が強かった。
135	Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks	https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf	Tim Salimans and Diederik P. Kingma	DNNの重み更新時に重みベクトルをノルムと正規化したベクトルの向きにパラメトライズしそれぞれの勾配から学習するweight normalization(WN)を提案。画像識別、生成モデル、強化学習タスクでbatch normalization(BN)などと比較評価しタスクによらず学習が高速に収束することを示した。
136	Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors	http://www.aclweb.org/anthology/P14-1023	Marco Baroni, Georgiana Dinu, German Kruszewski	単語ベクトルを獲得するための2つの枠組みであるcount-baseとpredict-baseな手法を初めてシステマティックに比較した論文。ハイパーパラメータを変え、analogyやsemantic relatedness含む5つの意味タスクで比較した結果、predict-baseな手法の方が優れているという結果になった。Don't count, predict!
137	Neural Word Embedding as Implicit Matrix Factorization	https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf	Omer Levy, Yoav Goldberg	Skip-gram with negative samplingで学習したword embeddingが、ある仮定の下ではPMIの行列を分解しているのと等価なことを示した論文。SPPMIを用いて単語を表現したところ単語類似度タスクとアナロジータスクのうちの一つで性能が向上することを示した。
138	Understanding Neural Networks through Representation Erasure	https://arxiv.org/abs/1612.08220	Jiwei Li, Will Monroe, Dan Jurafsky	自然言語において、ニューラルネットが何をどう判断しているのか解釈する試み。基本はある次元を抜いたときどれだけ尤度に影響があるかを調べることで、隠れ層の重要度ヒートマップを作る。ただ、自然言語においては単語の複合も重要。そこで強化学習を用い、主要要素以外を抜く試みも行っている。
139	Conditional Image Synthesis With Auxiliary Classifier GANs	https://arxiv.org/abs/1610.09585	Augustus Odena, Christopher Olah, Jonathon Shlens	画像生成について、単に生成するだけでなく、クラスの識別をさせる補助的なタスクをさせることで、識別性能(=解像度)を上げることができたという話。
140	SENSEMBED: Learning Sense Embeddings for Word and Relational Similarity	http://wwwusers.di.uniroma1.it/~navigli/pubs/ACL_2015_Iacobaccietal.pdf	Ignacio Iacobacci, Mohammad Taher Pilehvar, Roberto Navigli	これまでの単語埋め込みが語義を考慮していない問題に対し、語義曖昧性解消をして語義ごとの埋め込み表現(Sense Embeddings)を学習する話。語義曖昧性解消はBabelfyを使って自動的に行っている。語義ごとのembeddingsを学習した結果、Spearmanの相関係数で性能が向上した。
141	Retrofitting Word Vectors to Semantic Lexicons	https://arxiv.org/abs/1411.4166	Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris Dyer, Eduard Hovy, Noah A. Smith	Skip-gramやGloVeで得た単語ベクトルに対し、WordNetなどの外部知識を用いることで単語ベクトルを洗練する手法を提案。外部知識上で関連する単語を似たベクトルにするために似せたいベクトル間のユークリッド距離を最小化する。意味評価をした結果、一部タスクを除いて性能が向上した。
142	Feedback Networks	https://arxiv.org/abs/1612.09508	Amir R. Zamir, Te-Lin Wu, Lin Sun, William Shen, Jitendra Malik, Silvio Savarese	feedforwardに取って代わる可能性のあるfeedbackネットワーク提案。同一入力に対して繰り返し予測を行い、前の処理の結果を次の処理に反映することでfeedbackネットワークを構築。フィードフォワードと比較した利点として段階的な予測(Early Predictions)が可能である。ラベル空間の階層構造（分類法など）に準拠(Taxnomoy Compliance)しcurriculum learning（簡単なものから順次学んでいく学習）を行う。ネットワークのコアとなる構造はstacking ConvLSTM。lossは各Tでの出力におけるlossと最終出力のlossの荷重平均。CIFAR100を使った実験結果でResNet24超え。
143	FastMask: Segment Multi-scale Object Candidates in One Shot	https://arxiv.org/abs/1612.08843	Hexiang Hu, Shiyi Lan, Yuning Jiang, Zhimin Cao, Fei Sha	one-shotで複数のscaleの物体に対応出来るInstanceを考慮したSemantic Segmentation手法であるFastMaskの提案。役割の違うbody, neck, headという3つの構造を組み合わせてネットワークを構築(Fig.2参照)。ネットワークのコアは重み共有したResNeckモジュール。ResNeck構造でfeature mapをズームアウトして効率的に異なるスケールの物体に対応。既存手法と比較して処理スピードは2〜5倍。提案したhead構造(Attention Head)によって、受容野サイズが物体に合っていない事に起因した背景のノイズの影響を減少させることが出来る。SoTA。
144	End to End Learning for Self-Driving Cars	https://arxiv.org/abs/1604.07316	Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal	NVIDIAによる自動運転。この論文では画像入力からのend-to-endでの走路追従を対象とし出力としてステアリング舵角の値を教師有りの学習をCNNで行う。人間のドライバーの走行データから学習。汎化のため入力画像をaugumentationしそれに対応してステアリング舵角の値も補正し学習する。
146	Document Embedding with Paragraph Vectors	https://arxiv.org/abs/1507.07998	Andrew M. Dai, Christopher Olah, Quoc V. Le	Paragraph Vectorの有効性を文書類似度タスクでLDAと比較した話。WikipediaとarXivを対象に比較した結果、LDAと同等かそれ以上の結果を示した。また、Paragraph Vectorでもベクトルの足し引きで意味のある結果(e.g. "Lady Gaga" - "American" + "Japanese" = "Ayumi Hamasaki")を得ることができた。
147	From Word Embeddings To Document Distances	http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf	Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger	文書間の距離指標Word Mover's Distance(WMD)の提案。分布間の距離指標Earth Mover's Distance(EMD)をベースに文書に含まれる単語の埋め込みベクトルの集合を分布とみなして距離を算出。k-nnによる文書分類タスクで評価しSOTA手法より低エラーを実現。
148	sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings	https://arxiv.org/abs/1511.06388	Andrew Trask, Phil Michalak, John Liu	単語には複数の語義があるのに各単語につき一つの単語ベクトルしか学習していない問題に対して、語義ごとにベクトルを作ることを提案している。手法としてはPoSタガーを用いてタグ付けしたコーパスを用いて、CBOWやSkip-gramで語義を予測させることで学習する。"主観的"な評価の結果、確かに語義ごとのベクトルを得られていた。
149	Supervised Word Mover’s Distance	https://papers.nips.cc/paper/6139-supervised-word-movers-distance.pdf	Gao Huang, Chuan Guo	分類タスクを対象に教師なしの文書間距離指標WMD(
150	Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews	https://arxiv.org/abs/1512.08183	Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao	評価分析を長い映画レビューに対して行うと、既存の文書embeddingではbag-of-ngramを下回るので、超えられる文書embedding手法を提案した話。具体的にはMikolovらが提案したParagraph Vectorの考え方をベースに、文書から単語とbag-of-ngramを予測することで文書ベクトルを獲得している。IMDBのデータセットで評価をしたところ、accuracyで既存の文書embedding手法とbag-of-ngramを上回った。
151	Generative Image Modeling using Style and Structure Adversarial Networks	https://arxiv.org/abs/1603.05631	Xiaolong Wang, Abhinav Gupta	GANで生成を行う際に物体の構造が考慮されるべき、とし、表面のテクスチャ(surface normal map)を生成してからそれを入力に画像生成を行う、という二段構えのGANを考案。その名もS2-GAN。これで通常より高い識別性を持つ画像を生成できた。
152	Recent Advances in Convolutional Neural Networks	https://arxiv.org/abs/1512.07108v5	Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang Wang	2017年年初に送る、これまでのCNNのまとめ。構成方法、最適化手法から適用先まで、幅広くまとめられている。この図だけでもかなりの価値がある。
153	DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker	https://arxiv.org/abs/1512.07108v5	Matej Moravk, Martin Schmid, Neil Burch, Viliam Lis, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling	DNNでポーカーを行い、プロより強くなったという話。ポーカーが、相手の手札がわからない不完全情報ゲームという点でこの意義は大きい。
154	The More You Know: Using Knowledge Graphs for Image Classification	https://arxiv.org/abs/1612.04844	Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta	人間が持っている構造化された事前知識をナレッジグラフの形でディープラーニングのclassificationに導入する手法の提案。Graph Search Neural Networkを使用しEnd-to-Endで学習が可能。
155	Exploring phrase-compositionality in skip-gram models	https://arxiv.org/abs/1607.06208	Xiaochang Peng, Daniel Gildea	skip-gramを使ってフレーズレベルのベクトルを獲得する話。単語ベクトルと単語ベクトルを組み合わせて作るフレーズベクトルの作り方を同時に学習している。単語類似度とアナロジーおよび係り受け解析で評価した結果、単なるskip-gramより性能が若干向上した。
156	Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks	https://arxiv.org/pdf/1701.02720.pdf	Zhang, Pezeshki, Brakel, Zhang, Laurent', Bengio, Courville	CNNだけで音声認識を行う試み。LSTMを使ったときより計算的にefficient。TIMITデータセットではLSTMと同程度の精度を出すことができた。
157	Invariant Representations for Noisy Speech Recognition	https://arxiv.org/abs/1612.01928	Dmitriy Serdyuk, Kartik Audhkhasi, Philmon Brakel, Bhuvana Ramabhadran, Samuel Thomas, Yoshua Bengio	音声認識するニューラルネットの中間の特徴(h)をノイズに対して不変になるように訓練すると、ノイズデータセットが小さくとも精度が上がったという話。音の入力はエンコーダ(E)によりhに変換されるが、このhに音素の識別機Rとノイズの種類の識別機Dがついている。RとDの訓練と並行して、Dが弁別能力をなくするようにEを訓練するというアイデア。
158	A Recurrent Neural Network Without Chaos	https://arxiv.org/abs/1612.06212	Thomas Laurent, James von Brecht	GRUよりさらに単純化したRNNゲートアーキテクチャの提案。hidden stateから次の時刻のhidden stateへの重み行列を単位行列で置き換えた。言語モデルではLSTMやGRUと同等のパフォーマンスを示した。
159	SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity	https://arxiv.org/abs/1608.00869	Daniela Gerz, Ivan Vuli, Felix Hill, Roi Reichart, Anna Korhonen	動詞は文の意味を決定するのに重要な役割を占めるのに、最近の単語の意味研究ではあんまり注目されていないよねということで3500の動詞ペアの類似度を人間が評価したデータセットを提案した話。既存の単語表現学習モデルで分析したところ、低頻度で多義の動詞については非常に低い性能となることがわかった。
160	Learning Word Meta-Embeddings	http://aclweb.org/anthology/P16-1128	Wenpeng Yin, Hinrich Schtze	異なる性質を持つembedding集合を組み合わせてmeta embeddingを得る話。具体的には5つのembedding集合(HLBL, Huang, GloVe, CW, word2vec)を4つの手法(CONC, SVD, 1toN, 1toN+)で組み合わせて実験。これにより、単語類似度タスク、アナロジータスク、POSの性能が向上した。また組み合わせることでボキャブラリのカバレッジをあげられるのもメリット。
161	Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations	https://www.aclweb.org/anthology/P/P16/P16-2068.pdf	Alexandre Salle, Marco Idiart, Aline Villavicencio	LexVecという単語埋め込みベクトル獲得方法を提案した話。具体的にはPPMI行列を分解することによって得る。単語類似度タスクとアナロジータスクで評価した結果、単語類似度タスクではいくつかの評価セットにおいてSGNSを上回ったもののアナロジータスクではSGNSやGloVeの方が良い結果となった。
162	The More You Know: Using Knowledge Graphs for Image Classification	https://arxiv.org/abs/1612.04844	Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta	画像認識について、人間は少ないサンプルですぐに識別ができるが、機械学習モデルはそうはいかない。人間は認識結果をもとにその知識から推論を行っているからそれができる、と仮定し、この「知識」=認識画像間の関係グラフを組み込むことで1-shotの精度を上げようという話。
163	Deep Probabilistic Programming	https://arxiv.org/abs/1701.03757	Dustin Tran, Matthew D. Hoffman, Rif A. Saurous, Eugene Brevdo, Kevin Murphy, David M. Blei	確率モデルを記述するためのフレームワークEdwardの論文。確率変数をつないでグラフ構造を構築するように記載でき、TensorFlowのグラフと統合し爆速で動作可能。VAEやGANなど、確率モデルと統合されたようなモデルが書きやすくなる。
164	Cross-lingual Models of Word Embeddings: An Empirical Comparison	http://aclanthology.info/papers/cross-lingual-models-of-word-embeddings-an-empirical-comparison	Shyam Upadhyay, Manaal Faruqui, Chris Dyer, Dan Roth	多言語情報を使ってword embeddingsを得ることで性能向上することは知られていたが、手法の比較は行われてこなかった。そのため、4つの手法を比較した話(BiSkip, BiCVM, BiVCD, BiCCA)。Intrinsicなタスク(monolingualとcross-lingualでの単語類似度タスク)とExtrinsicなタスク(cross-lingualでの文書分類と係り受け解析)で評価した結果、単言語の類似度タスクではBiSkipとBiVCDは同じくらいだが、cross-lingualなタスクではBiSkipがかなり良い結果となることを示し、対照的にSyntacticなタスクではBiCCAが最も良い結果となった。
165	Symmetric Pattern Based Word Embeddings for Improved Word Similarity Prediction	http://www.aclweb.org/anthology/K15-1026	Roy Schwartz, Roi Reichart, Ari Rappoport	Symmetric Pattern(SP)(たとえばX and Y)に基づく単語ベクトル表現を提案した話。手法としてはSPをコーパスから獲得し、それに基づきPPMIを用いてベクトルを生成する。単語類似度タスクで評価した結果、SimLex999ではSOTAとなった。また、動詞に対して有効であることも分かった。
166	Tweet2Vec: Character-Based Distributed Representations for Social Media	https://arxiv.org/abs/1605.03481	Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl, William W. Cohen	Twitterの投稿内容から投稿についているハッシュタグを予測する文字ベースのニューラルネットワーク(Bi-GRU)を構築する話。文字ベースで予測することで膨大な単語を扱う必要がない、未知語に強い、単語分割が必要ない問いった利点がある。ハッシュタグの予測性能で評価した結果、単語レベルの方法に比べて良い性能を示した。
167	WaveNet: A Generative Model for Raw Audio	https://arxiv.org/abs/1609.03499	van den Oord, Dieleman, Zen, Simonyan, Vinyals, Graves, Kalchbrenner, Senior, and Kavukcuoglu	PixelCNNなどでうまくいっている自己回帰モデルによる生成モデルを音声の生成に適用してみた。dilationが幾何級数的に大きくなるconvolutionの列の導入により波形の直接生成に必要な大きな受容野を計算量を大きくせず実現できた。主観評価でこれまでで最高の品質を実現。音素認識にも使えて、有望な結果を出した。
168	Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder	https://arxiv.org/abs/1607.07514	Soroush Vosoughi, Prashanth Vijayaraghavan, Deb Roy	文字レベルのCNN-LSTM Encoder-Decoderモデルを構築して、Tweetのembeddingsを学習する話。文字レベルで処理を行うことで、Tweetに混じるノイズに頑健になり、他言語でも適用可能となった。Semantic Relatednessと評価分類で評価した結果これまでで最高の性能を達成した。
169	Convolutional Neural Networks for Sentence Classification	https://arxiv.org/abs/1408.5882	Yoon Kim	CNNを用いて文分類を行う話。具体的には文を単語ベクトルの列として表し、それに対してCNNを用いて特徴抽出・分類を行っている。論文では事前学習済みの単語ベクトル(Google Newsをword2vecで学習したもの)を使っている。評価分析や質問タイプ分類を含む7つのタスクで評価したところ、7つ中4つでSOTAな結果になった。
170	LEARNING TO REMEMBER RARE EVENTS	https://openreview.net/pdf?id=SJTQLdqlg	ukasz Kaiser, Ofir Nachum, Aurko Roy,  Samy Bengio	外部メモリを利用したone-shot learningの精度向上手法を提案。one-shot learningタスクではSoTA。hashing trickを使って最近傍法を効率的に実行。memory lossという独自のlossを導入している。
171	Pixel Objectness	https://arxiv.org/abs/1701.05349	Suyog Dutt Jain, Bo Xiong, Kristen Grauman	オブジェクト検知をピクセルレベルで行った話。前景部分の抽出、領域提案といった既存の手法と異なり、ピクセル単位で前景/背景の確率をCNNで計算する。
172	Unsupervised Cross-Domain Image Generation	https://arxiv.org/abs/1611.02200	Yaniv Taigman, Adam Polyak, Lior Wolf	ドメイン(スタイル)トランスファーの研究で、スタイルを変換する関数Gを画像の特徴抽出(f))とスタイル変換(g)に分割し、あたかもアナロジーを行うように(元画像＋スタイル)転化を行っている。これにより汎用性が高くなっている。メインのタスクとして、顔画像をアイコン風に変換している。
173	Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer	https://arxiv.org/abs/1612.03928	Sergey Zagoruyko, Nikos Komodakis	画像を認識するとき、この辺に注目するとええよ、というようにAttentionをTransferするという研究。Activationベース(lossを調整)と、Gradientベース(勾配を調整)の2種類を提案。効果は微少な感じだが、Activationベースの方が有効
174	Automatic Generation of Typographic Font from a Small Font Subset	https://arxiv.org/abs/1701.05703	Tomo Miyazaki, Tatsunori Tsuchiya, Yoshihiro Sugaya, Shinichiro Omachi, Masakazu Iwamura, Seiichi Uchida, Koichi Kise	日本語フォントを作るにはたくさん文字を書かないといけないので、それを楽にしようという論文。基準となる文字セット(Skeleton dataset)を用意し、それと新しいフォントの数文字について比較を行う。そこから構造(傾きなど)とストロークを抽出し、他の文字にも適用するという手法
175	Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts	http://www.aclweb.org/anthology/C14-1008	C´cero Nogueira dos Santos, Ma´ra Gatti	映画レビューやTwitterに対する評判分析をCNN(CharSCNN)を使って行う話。具体的には、単語レベルと文字レベルのembeddingsから文のベクトル表現を構築し、構築したベクトルを入力することで評判のスコアを出力するCNNを構築した。映画レビュー(SSTb)とTwitter(STS)に対するデータセットを用いて実験した結果、SOTAな結果となった。
176	Learning Character-level Representations for Part-of-Speech Tagging	http://jmlr.org/proceedings/papers/v32/santos14.pdf	C´cero Nogueira dos Santos, Bianca Zadrozny	品詞タグ付けをCNN(CharWNN)を使って行う話。具体的には、単語レベルと文字レベルのembeddingsを統合して単語のベクトル表現を構築し、構築したベクトルを入力することで品詞のスコアを出力するCNNを構築した。英語とポルトガル語に対するデータセット(WSJとMac-Morpho)を用いて実験した結果、SOTAな結果となった。
177	SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation	http://aclanthology.info/papers/semeval-2016-task-1-semantic-textual-similarity-monolingual-and-cross-lingual-evaluation	Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, Janyce Wiebe	テキストの類似性を測るタスクであるSemEval-2016 Task 1の説明をしている論文。説明内容はタスクの説明、アノテーション方法の説明、参加者が提出した結果の提示、および総括からなる。2016年はcross-lingualなテキストの類似性を含んだのが特徴。
178	Asynchronous Methods for Deep Reinforcement Learning	http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf	Volodymyr Mnih^1	並列に動作する複数エージェントのサンプルから学習する強化学習手法A3Cの提案。複数のエージェントごとに並列にサンプルを収集し勾配を評価しロスを足しあげ、その勾配を使い共有パラメータを非同期に更新する。また非同期に共有パラメータを各エージェントのパラメータとして取得する。サンプルは保持しないのでexperience replayは必要がない。DQN,DDQNなどと比較しCPUを使い短い学習時間であってもより高性能、この時点のSOTA.
179	Character-level Convolutional Networks for Text Classification	https://arxiv.org/abs/1509.01626	Xiang Zhang, Junbo Zhao, Yann LeCun	文字レベルの畳み込みニューラルネットワークをテキスト分類に使った話。シソーラスを使ってテキスト中の単語を同義語で置換することでデータを増やしている。比較は、伝統的な手法としてbow、bag-of-ngram、bag-of-means、Deep Learning手法として、単語ベースのCNN、LSTMを対象に行っている。8つのデータセットを作成してベースの手法と比較した結果、いくつかのデータセットでは有効性を示せた。
180	Effective Use of Word Order for Text Categorization with Convolutional Neural Networks	http://www.anthology.aclweb.org/N/N15/N15-1011.pdf	Rie Johnson, Tong Zhang	CNNを使って語順を考慮したテキスト分類を行う話。たいていのCNNの手法では入力としてword embeddingを入力するが、この研究では高次元のone-hotベクトルをそのまま入力して、小さなテキスト領域のembeddingを学習する。評判分析(IMDB含む)とトピック分類に関する3つのデータセットでSOTAな手法と比較した結果、提案手法の有効性を示せた。
181	Natural Language Processing (almost) from Scratch	https://arxiv.org/abs/1103.0398	Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa	品詞タグ付け、チャンキング、固有表現抽出、意味役割付与を学習できるニューラルネットワークを提案した話。単純に学習させるだけではベンチマークより性能が下回ったが、ラベルなしデータを用いて言語モデルの学習を事前に行うことで、質の良い単語ベクトルが性能向上に寄与することを示した。さらに各タスクを解くためのモデル間でパラメタを共有してマルチタスク学習を行うことで性能がより向上することも示した。
182	A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification	https://arxiv.org/abs/1510.03820	Ye Zhang, Byron Wallace	CNNのモデルは文分類でいい結果を残しているけど、熟練者がアーキテクチャ決めたりハイパーパラメータを設定する必要がある。これらの変更がどのような結果を及ぼすのかよくわからないので、一層のCNNを使って検証した話。最後に、CNNで文分類するときにモデルのアーキテクチャやハイパーパラメータをどう設定すべきか実践的なアドバイスをしている。
183	#TAGSPACE: Semantic Embeddings from Hashtags	http://emnlp2014.org/papers/pdf/EMNLP2014194.pdf	Jason Weston, Sumit Chopra, Keith Adams	ハッシュタグを教師にして短いテキストの表現を学習する話。具体的にはCNNを用いて、テキストとハッシュタグのペアに対してスコアを出力し、ハッシュタグのランク付けを行う過程でテキストの表現を学習する。ハッシュタグの予測と文書推薦タスクで評価を行った結果、ベースラインの手法よりも良い結果となった。
184	Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding	https://arxiv.org/abs/1504.01255	Rie Johnson, Tong Zhang	テキスト分類のためにCNNを使った半教師あり学習のフレームワークを提案した話。従来モデルでは事前学習済みのword embeddingを畳み込み層の入力に使っていたが、本研究では小さいテキストの領域から教師なしでembeddingを学習し、教師ありCNNにおける畳み込み層の入力の一部として使う。評価分析(IMDB, Elec)とトピック分類(RCV1)で実験したところ、先行研究より高い性能を示した。
185	Neural Architectures for Named Entity Recognition	https://arxiv.org/abs/1603.01360	Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer	言語固有の知識やリソースに依存しないような固有表現認識の手法を提案した話。具体的には2つのモデル(LSTM-CRFとS-LSTM)を提案している。ラベル付きコーパスから学習した文字ベースの単語表現とラベルなしコーパスから学習した単語表現を入力とすることで、4つの言語でSOTAとなった。
186	Wasserstein GAN	https://arxiv.org/abs/1701.07875	Martin Arjovsky, Soumith Chintala, Lon Bottou	VAEやGANなどの生成系のタスクでは、「真の分布」との距離の最小化を目的にしている。つまり「距離」の定義はモデルの精度の大きな要素で、GANではこの自由度が高い反面学習が安定しない要因になっていた。そこでWasserstein距離を使うと勾配が消失せず学習が安定したという話。
187	A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue	https://arxiv.org/abs/1701.04024	Mihail Eric, Christopher D. Manning	Seq2Seqでタスク指向対話を行う話。単純に出力(システム発話)を予測させるだけでなく、Attentionが最も高い入力ベクトルを予測させる(入力からコピーして教師ベクトルにする)。さらに知識ベースに入っている語か否かの情報を加えてそこそこの精度なので、結果はちょっと微妙。
188	Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting	https://arxiv.org/abs/1206.4634	Ning Xie, Hirotaka Hachiya, Masashi Sugiyama	墨絵における筆の運びを強化学習で学習させる話。筆の動き・止め・跳ね・回転をアクションとし、筆の動きの滑らかさを報酬として学習をしている。状態はグロバール上の位置と、それを基に計算するストロークの中における相対情報(Figure2参照)の双方を扱っている(計算に使うのは相対のみ)。
189	Adversarial Learning for Neural Dialogue Generation	https://arxiv.org/abs/1701.06547	Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, Dan Jurafsky	対話モデルについて、チューリングテストのように「生成した発話が人間と区別できないか」を評価する、つまりGANと同じ仕組みの導入を提案。Seq2Seqを基本とし、騙せたかどうかで報酬を得る強化学習の仕組みを導入(生成単語ごとに判定を実施)。既存のSeq2Seqモデルより優秀な結果。
190	The Role of Typicality in Object Classification: Improving The Generalization Capacity of Convolutional Neural Networks	https://arxiv.org/abs/1602.02865	Babak Saleh, Ahmed Elgammal, Jacob Feldman	分類タスクの学習に対して全ての入力データでlossの重みが同一で良いかを検証し、重み付けの手法を提案。同一クラス内のデータに対してTypical, Atypicalなデータを区別するためにTypical Scoreを算出し、lossのweightとして利用。Typical Scoreはクラス内の重心（平均値など）っぽさ（近さ）。weightは非線形なものも試している。
191	PathNet: Evolution Channels Gradient Descent in Super Neural Networks	https://arxiv.org/abs/1701.08734	Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, Daan Wierstra	同構造のネットワーク同士で、互いの学習結果を壊さず相手の学習結果を取り込むことを目指した研究。学習と同時にモジュール(畳込層など)を結ぶパスを遺伝的アルゴリズムで進化させていき、学習が完了したら重みを固定し次のタスクに入る。これで効果的な転移が可能なことを確認(画像&強化学習)。
192	Not All Contexts Are Created Equal: Better Word Representations with Variable Attention	https://www.cs.cmu.edu/~lingwang/papers/emnlp2015-2.pdf	Wang Ling, Lin Chu-Cheng, Yulia Tsvetkov, Silvio Amir, Ramon Fernandez Astudillo, Chris Dyer, Alan W Black, Isabel Trancoso	「ある単語の予測は文脈内のある単語に大きく依存している」という仮説を考慮して単語埋め込み表現を獲得するために、CBOWにAttentionを導入した話。POS Induction(教師なしの品詞タグ付け)、品詞タグ付け、評判分析で評価したところ、POS Inductionに対しては既存の手法(CBOW, Skip-ngram, SSkip-ngram)と比較して良い結果であった。その他タスクでもそこそこの性能を示した。
193	Improving Hypernymy Detection with an Integrated Path-based and Distributional Method	https://arxiv.org/abs/1603.06076	Vered Shwartz, Yoav Goldberg, Ido Dagan	RNNを用いて上位語の検知を行う話。具体的には依存関係のパスをLSTMsを使ってエンコードしてそれを分類している。評価した結果、従来よく行われているDistributionalの方法に匹敵する性能を示した。また、Distributionalな手法と組み合わせることでF1で14ポイントの向上が見られた。
194	A Closer Look at Skip-gram Modelling	http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf	David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, Yorick Wilks	言語におけるデータのスパース性の問題に対処するためにSkip-gramを使ってみた話。一般的なbigramやtrigramに比べて、skip-bigramやskip-trigramを使うことでカバレッジを向上させることができる。実際にカバレッジを比較したところ、データ数を増やすよりskip-gramを使ったほうがカバレッジ向上の役に立っている。
195	Semantic Clustering and Convolutional Neural Network for Short Text Categorization	http://www.aclweb.org/anthology/P15-2058	Peng Wang, Jiaming Xu, Bo Xu, Cheng-Lin Liu, Heng Zhang, Fangyuan Wang, Hongwei Hao	短いテキストは文脈情報の不足やデータのスパース性の問題がある。これら問題に対処するためにクラスタリングとCNNを用いてテキストをモデル化する話。具体的にはembedding空間をクラスタリングすることでsemanticクリークを作成し、そこから得られたsemantic unitをCNNに入力する。文書分類タスクでstate-of-the-artな手法と比較した結果、提案手法の有効性を示すことができた。
196	Tailoring Continuous Word Representations for Dependency Parsing	https://pdfs.semanticscholar.org/666b/639aadcd2a8a11d24b36bae6a4f07e802b34.pdf	Mohit Bansal, Kevin Gimpel, Karen Livescu	係り受け解析の特徴としてword embeddingを使用した話。先行研究と比べてすごいのは、単語表現をタスクに合わせて調整したり、複数のアルゴリズムで得られた単語表現を組み合わせることで性能を向上させられた点。PTBとEnglish Web treebankをデータセットとして検証した結果、Ensembleした表現を使うと一番良い結果になった。
197	Comparative Study of CNN and RNN for Natural Language Processing	https://arxiv.org/abs/1702.01923	Wenpeng Yin, Katharina Kann, Mo Yu, Hinrich Schtze	最近のNLPではCNNかRNNがよく使われているがそのシステマティックな比較は行われてこなかった。そこでこの論文ではCNNとRNNの性能比較を行っている。具体的にはCNN、GRU、LSTMを7つのタスク(Sentiment Classification, Relation Classification, Textual Entailment, Answer Selection, Question Relation Match, Path Query Answering, POS)について評価を行っている。評価した結果、キーフレーズの認識が重要なタスク(Sentiment Detection, Question Answer Matching)以外のタスクについてはRNNの性能が上回った。
198	Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network	https://arxiv.org/abs/1609.04802	Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi	低解像度から高解像度に復元する研究。今までは本物の画像との距離(MSE)を使用していたが、これだと「感覚的」な近さと一致しないことが多かった。そこでPerceptual lossという特徴マップ間の誤差を導入(学習済みVGGを使用)。5段階の主観評価で既存モデルより1スコアUP
199	Tracking the World State with Recurrent Entity Networks	https://openreview.net/forum?id=rJTKKKqeg&noteId=rJTKKKqeg	Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann LeCun	Facebookの文章読解タスクの研究。状況を記憶させるためのメモリユニットを組み合わせたネットワーク(Recurrent Entity Network)を提案。入力に対し更新を行う際key vectorを用いどのメモリに書き込むかを含め学習する。bAbIタスクで完全試合を達成。
200	SampleRNN: An Unconditional End-to-End Neural Audio Generation Model	https://openreview.net/forum?id=SkxKPDv5xl	Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Manuel Rodriguez Sotelo, Aaron Courville, Yoshua Bengio	音楽生成についての研究。長い時系列上での関係を捉えるために、RNNを階層状に積んで上の方ほど長い間隔の依存をとらえるのを担当するような構成を構築(最下層の出力は通常のNN)。音声合成・音楽のデータで検証しWaveNet(CNN)をうわまった、という結果。
201	Pixel Recursive Super Resolution	https://arxiv.org/abs/1702.00783	Ryan Dahl, Mohammad Norouzi, Jonathon Shlens	ピクセル間の依存を考慮しない推定(conditioning network)に、PixelCNNを用いて計算した高解像度画像におけるピクセル間の依存(prior network)を足し合わせることで、高解像度ピクセル推定を行っている。
202	Adversarial Attacks on Neural Network Policies	http://rll.berkeley.edu/adversarial/	Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, Pieter Abbeel	GANの話ではなくリアルにAdversarial(敵対的)な話。分類モデルで使える攻撃で、強化学習でも学習速度を遅くさせることが可能という話。目的関数の勾配にsign関数をかけたものを足すだけでOK(強化学習では選択した行動以外は勾配が入らないので学習時はsoftmaxを使用)
203	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	https://arxiv.org/abs/1702.01417	Jiaqi Mu, Suma Bhat, Pramod Viswanath	分散表現の精度を上げる後処理の話。分散表現の問題として、どの次元でも共通するなにがしかのベクトル量を持っていると指摘。そこでそれらを差っ引いてやることで各次元の特徴を際立たせてやろうという試み。具体的には平均と主成分をひいている。各分散表現で精度の向上を確認、特にGloveで顕著
204	Multi-agent Reinforcement Learning in Sequential Social Dilemmas	https://deepmind.com/blog/understanding-agent-cooperation/	Joel Z. Leibo1	強化学習で、複数エージェントの場合の研究。リンゴをとるタスクで、リンゴをとる以外にビームで相手を止められるようにして実験。最初は撃たなかったが、リンゴが少なくなると敵対が発生＋複雑(賢い)なネットワークほど敵対的という結果に。また、複雑なゲームほど協調が多い傾向が出たとのこと。
205	Graph Analytics for Real-time Scoring of Cross-channel Transactional Fraud	http://fc16.ifca.ai/preproceedings/02_Molloy.pdf	Ian Molloy	オンラインバンキングやP2P paymentなどの複数のチャネルを統合的に扱った不正送金検出手法の提案。全チャネルをまとめてグラフを生成。グラフから仮説ベースの特徴抽出を行い、検出精度に対する実験を行っている。また、事前に特徴量を計算しておくことで、リアルタイムなFraud検出が可能としている。既存手法と比較してfalse positiveを大幅改善。
206	Language Models with Pre-Trained (GloVe) Word Embeddings	https://arxiv.org/abs/1610.03759	Victor Makarenkov, Bracha Shapira, Lior Rokach	RNNを使った言語モデルにword embeddingを組み込むことで性能向上をはかっている話。メモリセルにはGRU、embeddingにはGloVeを使用。n番目の単語ベクトルをn-1個の単語ベクトルから予測している。
207	Stacked Attention Networks for Image Question Answering	https://arxiv.org/abs/1511.02274	Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola	画像に関する質問に答える研究。CNNで画像特徴、LSTMで質問クエリ、二つ合わせて回答するのが鉄板だが、画像内の着目点をよりはっきりさせるため、画像の各領域に対するクエリのAttentionを反復して計算する(Stacked Attention)手法を提案。既存精度を大きく更新。
208	Structured Attention Networks	https://arxiv.org/abs/1702.00887	Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush	Attentionについて、単純にどの地点に注目するかというカテゴリカル分布的な考えでなく、規則性(構造)があると仮定しようという提案。具体的には、CRFの考えを用いてAttentionの候補となる「系列」に対して全体として最適になるように計算を行うという話。これで精度UPを確認。ただし、計算時間が遅くなるというハンデが伴うので注意。
209	Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance	https://www.aclweb.org/anthology/W/W16/W16-2501.pdf	Billy Chiu, Anna Korhonen, Sampo Pyysalo	単語表現の質は、単語類似性の人間の判断との相関を用いてよく評価される。しかし、そこでいい結果でも実際のタスクに適用すると良い結果にならないことを説明している。評価には、10の単語類似性の評価セットと3つのNLPタスク(POS, chunking, NER)を用いて、単語類似性の評価結果とNLPタスクの評価結果の相関を分析している。評価した結果、ほとんどのデータセットでは実際のNLPタスクの結果との間に負の相関があることがわかった。SimLex999だけ例外。このような結果となった原因として、ほとんどのデータセットでは単語の類似性と関連性を区別していないことによるものだと結論付けている。
210	Problems With Evaluation of Word Embeddings Using Word Similarity Tasks	https://arxiv.org/abs/1605.02276	Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer	単語ベクトルの評価手法の一つである単語類似性タスクに関する問題点と既存の解決策を示したサーベイ論文。述べられている問題点は以下の7つ。
211	Energy Saving Additive Neural Network	https://arxiv.org/abs/1702.02676	Afrasiyabi, Yildiz, Nasir, Vural, and Cetin	乗算に代わってef-operationというものを導入してエネルギー効率のよいneural netを作ったという話。ef-operationとは符号は普通の乗算と同じだけど、絶対値は単なる和というもの。MNISTとCIFARで従来と同等の精度を出したという。
212	On the Relevance of Auditory-Based Gabor Features for Deep Learning in Automatic Speech Recognition	https://arxiv.org/abs/1702.04333	Martinez, Mallidi, and Meyer	音声認識では入力特徴量としてMel filterbankを使うより、脳と同様にGaborフィルタを使った方が認識精度が良いという話。WERが11から56%の向上。Aurora4, ChiME2, CHiME3データセット。なおGaborの方が良いという話はこの論文が初めてではなくて、Gaborのどれが良いかをいろいろ比較したということ。
213	Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses	https://arxiv.org/abs/1701.08893	Eric Risser, Pierre Wilmot, Connelly Barnes	StyleTransferを改善する方法についての論文。表現方法(=色の「分布」)の差異を反映できるよう元画像のヒストグラムとの差異を表すlossの項を導入、絵の中のどのスタイルをターゲットのどこに適用するか指示できるようにする、またパラメーターの自動調整を行うといった3本立て。
214	Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models	https://arxiv.org/abs/1702.03275	Sergey Ioffe	Batch Normalizationの改良版。バッチサイズが小さい時の問題、また推定時と学習時とで正規化方法に差異が出る問題を解消する試み。具体的には、最初は通常通りバッチ内で正規化するけど、徐々にデータ全体の正規化パラメーター(移動平均/分散)へシフトしていくという手法。
215	Frustratingly Short Attention Spans in Neural Language Modeling	https://arxiv.org/abs/1702.04521	Micha Daniluk, Tim Rocktschel, Johannes Welbl, Sebastian Riedel	Attentionを行う場合、隠れ層のベクトルは次の単語の予測・Attentionの算出・将来の単語に有用な情報の格納、という3つの役割を担っていることになる。なので出力を3つにして役割分担させるアイデア。併せて、単純に過去の隠れ層を結合して入力するだけでも高精度になることを確認
216	An Empirical Exploration of Recurrent Network Architectures	http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf	Rafal Jozefowicz ,Wojciech Zaremba,Ilya Sutskever	LSTM/GRUよりも優れた構造を持つものはないか？ということを検証した論文。過去の実績から100の優れた構造をピックアップし、これらとLSTM/GRUを4つのタスク(数式意味解釈、XMLタグ予測、言語モデル、音楽生成)でハイパーパラメーターを変えながら検証。結果、LSTM/GRUをすべてのタスクで上回るモデルは発見できなかった。なお、LSTMのforget gateのbiasは1にするとすごいい。
217	Generative Temporal Models with Memory	https://arxiv.org/abs/1702.04649	Mevlana Gemici, Chia-Chun Hung, Adam Santoro, Greg Wayne, Shakir Mohamed, Danilo J. Rezende, David Amos, Timothy Lillicrap	時系列データのモデリング方法についての提案。RNNにより潜在変数を予測する生成モデル(Variational RNN)をベースに、各タイムステップにおける潜在変数を格納したメモリ(=時間ごとのデータのふるまいの記憶)を用意し、Attentionによりそこから読み出す機構を提案。
218	Image Super-Resolution Using Deep Convolutional Networks	https://arxiv.org/pdf/1501.00092.pdf	Chao Dong, Chen Change Loy, Member, IEEE, Kaiming He, Member, IEEE, and Xiaoou Tang, Fellow, IEEE	一枚の画像からその解像度を上げる単画像超解像の論文。DLを使った超解像の研究の起点になったものと思われる。3枚から4枚のCNNを用いてfeature mapを作成し高解像度の画像を生成する。
219	Deeply-Recursive Convolutional Network for Image Super-Resolution	https://arxiv.org/abs/1511.04491	Jiwon Kim, Jung Kwon Lee and Kyoung Mu Lee Department of ECE, ASRI, Seoul National University, Korea	一枚の画像からその解像度を上げる単画像超解像の論文。
220	Accurate Image Super-Resolution Using Very Deep Convolutional Networks	https://arxiv.org/abs/1511.04587	Jiwon Kim, Jung Kwon Lee and Kyoung Mu Lee Department of ECE, ASRI, Seoul National University, Korea	一枚の画像からその解像度を上げる単画像超解像の論文。
221	Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections	https://arxiv.org/abs/1603.09056	Xiao-Jiao Mao†, Chunhua Shen, Yu-Bin Yang††State Key Laboratory for Novel Software Technology, Nanjing University, China School of Computer Science, University of Adelaide, Australia	一枚の画像からその解像度を上げる単画像超解像の論文。
222	Audio Super-Resolution using Neural Networks	https://openreview.net/pdf?id=S1gNakBFx	Volodymyr Kuleshov, S. Zayd Enam, and Stefano Ermon	画像ならぬ音の超解像に挑戦する研究。端的には低いサンプリングレートの音を畳み込み(ResBlock)＋UpSamplingして高サンプリングレートの音に変換する。これで電話の音声などを受信側で高音質にするといったことが可能になる。発音データセットでPSNR40程度でまあ良しの結果
223	Playing SNES in the Retro Learning Environment	https://openreview.net/pdf?id=S1gNakBFx	Nadav Bhonker, Shai Rozenberg, Itay Hubara	SNES(=スーファミの実行環境)で強化学習ができるようにする学習環境RLEを作った話。Atariよりもゲーム数が多く、またバリエーションにも非常に富んでいる。
224	Char2Wav: End-to-End Speech Synthesis	https://openreview.net/forum?id=B1VWyySKx	Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe Santos, Kyle Kastner, Aaron Courville, Yoshua Bengio	これまでの音声合成は、テキスト→発音特徴、発音特徴→音声と二段階に分かれていたが(WaveNetは後者に相当)、これを統合しEnd to Endな音声合成モデルを作成するという話。Attentionを積んだRNNでEncode、階層上のRNN(SampleRNN)でデコードする。他手法との比較結果はまだ記述されてないが、公式サイトで合成された音声を聞くことができる。
225	Shake-Shake regularization of 3-branch residual networks	https://openreview.net/forum?id=HkO-PCmYl&noteId=HkO-PCmYl	Xavier Gastaldi	画像認識のモデルで、ResNetの2つのブロックからの出力をランダムに組み合わせる(Shake)モデルを提案。forward/backward共にShakeすることでCIFAR-10で2.72%のエラーレートを記録(しかもバッチ単位より個別の画像単位で適用したほうが精度が高い)。
226	Neural Expectation Maximization	https://openreview.net/pdf?id=BJMO1grtl	Klaus Greff, Sjoerd van Steenkiste, Jrgen Schmidhuber	Neural Expectation Maximization (N-EM)の提案。EMアルゴリズムにおけるハイパパラメタ推定にニューラルネットワークを使用(RNN-EM)。RNNのhidden stateの更新がM stepに該当。
227	Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure	https://arxiv.org/abs/1611.03641	Oded Avraham, Yoav Goldberg	単語表現を評価するためには単語類似度のデータセットがよく使われている。しかし、既存のデータセットには2つの問題がある。一つは単語の関連性と類似性を区別していない点、もう一つは評価者間でアノテーションスコアがばらつく点である。これらの問題に対処するために単語類似度のためのデータセットを作成・使用する方法を提案している。アノテーションと性能指標を再設計することで、評価の信頼性を改善した。ヘブライ語に対する２つのデータセットを作成したところ、より高い評価者間の一致を達成。モデルに対する細かい分析を行うことができることを示した。
228	Evaluating Word Embeddings Using a Representative Suite of Practical Tasks	https://cs.stanford.edu/~angeli/papers/2016-acl-veceval.pdf	Neha Nayak, Gabor Angeli, Christopher D. Manning	単語表現を評価するためには単語類似性とアナロジータスクがよく使われている。しかし、それらで良い結果でも実際のタスクで良い結果になるとは限らないことが先行研究で示されている。そのあたりを考慮してこの論文ではより実際のタスクに近い評価手法を提案している。具体的にはユーザが訓練した単語ベクトルを6つのタスク(NER, POS, Chunking, Sentiment Analysis, Question Classification, NLI)で評価するためのシステムを構築している。これにより、学習した単語表現が実際に行いたいタスクに近いタスクで有効なのかを簡単に検証できる。このシステムは
229	Hybrid computing using a neural network with dynamic external memory	http://www.nature.com/nature/journal/v538/n7626/full/nature20101.html	DeepMind	ニューラルネットによるコンピュータの模倣。
230	EXPLAINING THE LEARNING DYNAMICS OF DIRECT FEEDBACK ALIGNMENT	https://openreview.net/pdf?id=HkXKUTVFl	Justin Gilmer, Colin Raffel, Samuel S. Schoenholz, Maithra Raghu, and Jascha Sohl-Dickstein	Back propagationは生体的な学習のプロセスとは乖離があるので、もうちょい実際の生物寄りな学習プロセスを実装するとどうか、という話。ここではDFAというBackpropではなく直接重みを更新する手法と、レイヤ個別に学習させるLAFSがほぼ等価であるという紹介をしている
231	Using Deep Learning and Google Street View to Estimate the Demographic Makeup of the US	https://arxiv.org/abs/1702.06683	Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei	都市の統計調査(人種や性別、職業や失業率など・・・)は、個別訪問ベースで行われるため調査結果が出るまで半年ぐらいかかることもあり、実態と乖離することも多い。そこで、街の様子、具体的には街中を走る車の車種からこれらを推計しようという試み。2200万ほどの車種をモデルに利用することで、驚くほど精確に推定ができたという話。
232	Deep and Hierarchical Implicit Models	https://arxiv.org/abs/1702.08896	Dustin Tran, Rajesh Ranganath, David M. Blei	確率モデルとニューラルネットワークの合わせ技的な話。隠れ層のパラメーター(重みやバイアス)を潜在変数とみなし、これとノイズを組み合わせてActivationする、というのを事前分布の推定ととらえ、これを階層状に積むことで階層ベイズモデルと同様の推定を表現する。Edwardで実装済
233	Understanding Synthetic Gradients and Decoupled Neural Interfaces	https://arxiv.org/abs/1703.00522	Wojciech Marian Czarnecki, Grzegorz wirszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals, Koray Kavukcuoglu	Backpropは誤差が出る=Forwardが終了しないと学習できないので、誤差を予測するモデルを組み込み(図中のSG)予測誤差で学習しようという話(SGは別途真の誤差から学習)。特定の問題で収束は確認、通常と異なる学習をするらしい
234	Unsupervised Image-to-Image Translation Networks	https://arxiv.org/abs/1703.00848	Ming-Yu Liu, Thomas Breuel, Jan Kautz	教師なしで画像のドメイン変換を行うという試み。変換元と変換先でそれぞれEncoder->画像生成(VAE)->画像判定(Discriminator)を用意、どっちに入れてもどっちも騙せるように訓練する。元は同じ画像なので画像特徴は同じはず、とし高位層での重み共有の制約を入れている。
235	Deep Forest: Towards An Alternative to Deep Neural Networks	https://arxiv.org/abs/1702.08835	Zhi-Hua Zhou, Ji Feng	ハイパパラメタのチューニングがほぼ不要な決定木のアンサンブルメソッドであるgcForestを提案。構造は下層での複数のforestsからの出力をconcatし、それを次の層の複数のforestsの入力に用いるというカスケードモデル。ディープラーニングと比較して、計算資源, 必要な教師データ数が少なくてよく、異なるドメインから生成されたデータに対しても頑健、並列化が容易という利点がある。
236	Understanding Deep Convolutional Networks	https://arxiv.org/abs/1601.04920	Stphane Mallat	DCNN理解のための数理モデル。DCNNが学習に必要とするデータ量は、そのパラメタ空間の次元からするとかなり小さい。したがってDCNNは（近似する関数）f(x)が定常であるような高次元領域を線形化するような変換Φ(x)を見つけ、線形射影によってデータ空間の次元を潰していると考えられる。本論文ではf(x)の値を変えないような変換として平行移動と微分同相写像を例に挙げ、その数学的性質を検討することでDCNNを理解することを試みる。
237	Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees	https://arxiv.org/abs/1702.08833	Daniel Zoran, Balaji Lakshminarayanan, Charles Blundell	k-NNにおける有用な表現学習の手法を提案。Differentiable Boundary Treesが中心的な役割。ツリー内のトラバーサルを確率論的事象としてモデル化することにより微分可能なコスト関数を形成。効果的な木を構築するために各ノードおよびqueryを写像する全transition共通のディープニューラルネットを使用。
238	Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning	https://arxiv.org/abs/1612.01887	Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher	画像からのその説明を生成する研究について。基本的なモデルはEncoder-Decoder + Attentionだけれど、単語のtheとかofみたいな語を推測する際に画像を参照する必要なくない？ということで、画像を参照するか否か、するとすればどの領域かを判断するゲートを搭載した。
239	Evolving Deep Neural Networks	https://arxiv.org/abs/1703.00548	Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, Babak Hodjat	DNNの構造を人手で決めるのは厳しいので、遺伝的アルゴリズムで探索させようという話。CIFAR-10、 PTBでそれぞれ画像と言語モデル、MSCOCOでイメージキャプションを検証。それだけでなく、実際の雑誌サイトのデータでも検証。リソースがあれば任せるのもありな結果。
240	Least Squares Generative Adversarial Networks	https://arxiv.org/abs/1611.04076	Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang	GANにおいて、Discriminatorは真偽を判定するためその出力は確率値(sigmoid)になっている。ただ、これだと勾配消失が起きやすい。そこで二乗誤差を使用する手法を提案。具体的には、ネットワークの出力値から定数を引く形で誤差を定義する。この最小化が、ピアソンのカイ二乗分布の分散の最小化と等価であることも証明。
241	Neural Episodic Control	https://arxiv.org/pdf/1703.01988.pdf	Alexander Pritzel	Differentiable Neural Computerのメモリの実装を利用した強化学習の提案。State(ゲーム画面をCNNにかけたもの)をkey、その時のQ値をvalueとしてメモリを構成。Stateが来たら各keyとの間で重みを計算し値を読みだす(メモリのサイズが大きい場合はk-nearestを使用)。A3Cより高速に収束
242	HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving	https://arxiv.org/abs/1703.00426	Cezary Kaliszyk, Franois Chollet, Christian Szegedy	HolStep: Googleから公開された、論理推論を学習するための大規模データセット。与えられた情報の中で推論に重要な点は何か、各推論間の依存関係、そこから導かれる結論は何か、などといったものがタスクとして挙げられている。
243	MoleculeNet: A Benchmark for Molecular Machine Learning	https://arxiv.org/abs/1703.00564	Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande	MoleculeNetという、新薬発見のための分子・分子物理・生体物理・生体？という4種類のデータを包含したデータセットが公開。
244	Baselines and Bigrams: Simple, Good Sentiment and Topic Classification	https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf	Sida Wang and Christopher D. Manning	お前たちがベースラインとして使っているSVMとNBで、お前たちの出そうとしているSOTAを記録してやったぜという話。タスクは極性判定と文書分類。bigramのBoWはかなり強力に効く、短い文書だとNB>SVM、そしてNBとSVMを組み合わせるとすごいことになる(全タスクで無双)
245	Evolution Strategies as a Scalable Alternative to Reinforcement Learning	https://arxiv.org/abs/1703.03864	Tim Salimans, Jonathan Ho, Xi Chen, Ilya Sutskever	強化学習における遺伝的アルゴリズムの有用性を示した論文。Virtual batch normalizationを使うことで信頼性up。backprop、value functionなしで計算できてしかも並列化が可能。DQNに比べパフォーマンス＋学習にかかるまでの時間で優位との結果
246	Emergence of Grounded Compositional Language in Multi-Agent Populations	https://arxiv.org/abs/1703.04908	Igor Mordatch, Pieter Abbeel	マルチエージェントの強化学習で、エージェントが他のエージェントに指示を伝えられるようにすることで、エージェントがどう言葉とその意味を開発していくのかを調べたもの。意図した言語獲得にするために、発言にコストをかけたり既存の単語の使用をプラスに評価するなど報酬設計をかなり工夫している
247	Recurrent Highway Networks	https://arxiv.org/abs/1607.03474	Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutnk, Jrgen Schmidhuber	High Wayの手法をRNNに適用する話。High Wayは入力をバイパスするゲートCを設けて、これと隠れ層HをゲートTに通したものを合算させることで入力にない表現のみ学習をさせるような手法。これで伝搬ステップの深いRNNを作る。言語モデル(PTB)とWikipediaの語予測でSOTA
248	Sharp Minima Can Generalize For Deep Nets	https://arxiv.org/abs/1703.04933	Laurent Dinh, Razvan Pascanu, Samy Bengio, Yoshua Bengio	DNNはすごく過学習しやすそうなのになぜ一般解(と思われるもの)を獲得できるのかについて、「最適解近辺がフラットだから」という予測があった。しかし挙動が変化しない類のパラメーター変換でも誤差平面の平坦さは影響を受けることが分かった。つまり、平坦の定義も含め議論の余地ありという話。
249	Learning to Discover Cross-Domain Relations with Generative Adversarial Networks	https://arxiv.org/abs/1703.05192	Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, Jiwon Kim	2つの画像ドメイン間の類似性、言い換えればドメインの変換関数を獲得できるかという研究。互いのドメインの変換を行うに当たり、「変換先のドメインの識別機を騙せる」ように、なおかつ「変換が元の画像をなるべく損なわないようにする」ように最適化を行う。これをDiscoGANと命名。
250	Online Learning Rate Adaptation with Hypergradient Descent	https://arxiv.org/abs/1703.04782	Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, Frank Wood	学習において最重要なパラメーターの一つの学習率αを自動で最適化する話。具体的には、前回との勾配の内積を取り、その大小によりαの更新を行う(=前回と変わらなければ大きく、変わっていれば小さくとる)。この勾配の内積による更新には新たなパラメーターβが必要になるが、これはαほど敏感には影響しないとのこと。
251	OptNet: Differentiable Optimization as a Layer in Neural Networks	https://arxiv.org/abs/1703.00443	Brandon Amos and J. Zico Kolter	ニューラルネットの層としてQP(二次計画)を解く層（入力がQPの各係数等になっていて、出力がそのQPの最適解となるような層)を導入するという話。KKT条件からうまくバックプロパゲーション可能。ミニバッチで学習する際に多量のQPインスタンスを解く必要があるため、GPUベースで複数の問題をバッチで解くソルバを実装。 応用例としてはサイズ縮小版の数独ソルバの学習など。
252	Multi-style Generative Network for Real-time Transfer	https://arxiv.org/abs/1703.06953	Hang Zhang, Kristin Dana	MSGNet:画像のスタイル適用について、複数のスタイルを一モデルで、しかもリアルタイムに適用する。スタイル画像を事前学習済みVGGにかけ、その中間レイヤをコンテンツ側のネットワークに挿入し変換する。この出力をこれも学習済VGGにかけコンテンツ/スタイルのlossを算出し学習する
253	Mask R-CNN	https://arxiv.org/abs/1703.06870	Kaiming He, Georgia Gkioxari, Piotr Dollr, Ross Girshick	画像のセグメンテーションについての研究。領域抽出＞オブジェクト領域＋認識を解くというFaster R-CNNに、さらにオブジェクトマスクのタスクも解かせるという手法。Maskと認識のタスクが競合しないように、sigmoid/binary lossを使うのがポイントとのこと。
254	Deep Photo Style Transfer	https://arxiv.org/abs/1703.07511	Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala	Adobeから出てきた写真のスタイルトランスファーの論文。「写真っぽさ」を維持するために、色の変換が(色の)アフィン変換の範囲で行われるように、そしてスタイルの適用時にセグメンテーションの制限を設ける(ビルにはビル部分のスタイルが適用されるようにするなど)という手法。
255	Fashion Style in 128 Floats: Joint Ranking and Classification using Weak Data for Feature Extraction	http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraCVPR2016.pdf	Edgar Simo-Serra, Hiroshi Ishikawa	ファッションの情報をベクトル(128次元)で表現し、その距離情報で分類を可能にしたという話。クラス識別の学習に加え、似ている/似ていない画像との距離を測れるよう学習を行う。単純なCNNに比べて1.5倍程度の識別性能を達成。
256	Deep Image Matting	https://arxiv.org/abs/1703.03872	Ning Xu, Brian Price, Scott Cohen, Thomas Huang	Adobeによる、背景除去についての研究。前景/背景のマップ(Alpha Matte)を、入力画像とそのTrimap(絶対前景・絶対背景・よくわからんの3領域のマップ)から予測させるというもの。予測用・補正用という二段階での構成。
257	Value Iteration Networks	https://arxiv.org/abs/1602.02867	Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, Pieter Abbeel	DQNに代表されるCNNから直接行動を推定する手法は、「その場の情報」だけで「行動」する形であり未知の状態への適応が困難。そこで鉄板の価値反復法を「微分可能な形」で組み込もうという話(図参照)。これにより汎化性能を向上できた。
258	Who Said What: Modeling Individual Labelers Improves Classification	https://arxiv.org/abs/1703.08774	Melody Y. Guan, Varun Gulshan, Andrew M. Dai, Geoffrey E. Hinton	ラベル付けを行う際、アノテーター同士で意見が割れる場合は多数決のラベルで学習することが多い。が、そうではなくまずアノテーターごとのWeightを学習させて、その上に各アノテーターのWeightをどう組み合わせるかを学習させる、という風にした方が高精度になるという話。
259	Adversarial Autoencoders	https://arxiv.org/abs/1511.05644	Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey	VAEでは獲得する潜在空間が(正規)分布に従うと仮定し、潜在空間と仮定した分布との差異を最小化するが、ここで利用しているKL距離は特定の分布でないと使えない。そこでGAN的な思想で潜在空間と仮定分布からのサンプルの真偽をとり学習を行うと言う手法。
260	Scaling the Scattering Transform: Deep Hybrid Networks	https://arxiv.org/abs/1703.08961	Edouard Oyallon (DI-ENS), Eugene Belilovsky (CVN, GALEN), Sergey Zagoruyko (ENPC)	DNNの下層部分は転移が可能なことからも非常に汎用的な層になっている。だったら学習は不要では？ということでこの一層目を汎用的な特徴が計算できる散乱変換(Scattering Transform)で置き換えるという話、のはず。
261	Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks	https://arxiv.org/pdf/1703.10593.pdf	Jun-Yan Zhu,Taesung Park,Phillip Isola,Alexei A. Efros	通常のimage2imageでは元ドメインと変換先ドメインの画像のペア(線画と着色済みなど)が必要だったが、ペアでなくても変換を可能にしたという話。そのからくりは「元の画像」と、その画像を「変換＋逆変換」して元に戻したものの間の誤差で学習を行うというもの。名付けてCycleGAN
262	Factorization tricks for LSTM networks	https://arxiv.org/abs/1703.10722	Oleksii Kuchaiev, Boris Ginsburg	LSTMの計算高速化のためのテクニックの提案。LSTMは内部で4つのゲートがあるが、これらはまとめて計算が可能⇒4ゲート分をまとめたサイズの重み行列(T)で計算し、計算後に切り分ける。このTを二つの重みW1/W2の積で表現＋入力をグループにわけて並列化する。TFでの実装あり。
263	SEGAN: Speech Enhancement Generative Adversarial Network	https://arxiv.org/abs/1703.09452	Santiago Pascual, Antonio Bonafonte, Joan Serr	GANによって音声の質を改善する研究。図のように、Auto Encoderのような形式でGeneratorを形成して学習を行う。この構造は音声意外にも適用できそうな印象。
264	MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation using 1D and 2D Conditions	https://arxiv.org/abs/1703.10847	Li-Chia Yang, Szu-Yu Chou, Yi-Hsuan Yang	CNN+GANでMIDIを作成しようという試み。時間(長さは1小節分・16部音符単位で16)x音 (MIDIの128音)で2次元で小節を表現し、和音を1次元のベクトルで表現これを不思議な変換で組み合わせてマップを作り、学習する。
265	BEGAN: Boundary Equilibrium Generative Adversarial Networks	https://arxiv.org/abs/1703.10717v1	David Berthelot, Tom Schumm, Luke Metz	GANの学習を安定させる試み。実際のサンプルに近づけるのでなく、Auto-Encoderの誤差分布に近づける(距離はWasserstein)という点と、DとGの間のlossの割合(γ)を導入し、学習の均衡と収束判定を容易にした(lossが低い＋平衡になったらok)。
266	Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders	https://arxiv.org/abs/1704.01279	Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, Mohammad Norouzi	WaveNetベース(non-causal dilated convolution=現時点までの音を、間引きして畳み込む)のAuto-Encoderにより、End-to-Endの音声生成を行ったという話。
267	SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual	https://arxiv.org/abs/1704.01279	Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, Janyce Wiebe	テキストの意味評価に特化したワークショップ 
268	Neural Tree Indexers for Text Understanding	https://arxiv.org/abs/1607.04492	Tsendsuren Munkhdalai, Hong Yu	RNNでなくRecursiveを使うメリットとして構造が扱えるという点があるが、この構造は構文木のパースに依存すると言う問題点があった。
269	Best Practices for Applying Deep Learning to Novel Applications	https://arxiv.org/abs/1704.01568	Leslie N. Smith	深層学習をアプリケーションで利用する際のすすめ方や注意点についての話。問題の定義(inとout)をしっかり行うこと、検証が済んでいるモデル(公開されているコードetc)からはじめること、結果の見える化をしとくこと、などが書かれている
271	Adversarial Generator-Encoder Networks	https://arxiv.org/abs/1704.02304	Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky	GANにおけるGeneratorとAutoEncoderにおけるEncoderを競わせるという手法の提案。BiGAN(
272	A Neural Representation of Sketch Drawings	https://arxiv.org/abs/1704.03477	David Ha, Douglas Eck	GANのようにピクセル単位の画像(ラスタライズ)ではなく、ストローク単位の画像(ベクター画像)を生成する試み(人が絵を描くときは後者に近い)。
273	Room for improvement in automatic image description: an error analysis	https://arxiv.org/abs/1704.04198	Emiel van Miltenburg, Desmond Elliott	画像のキャプション生成タスクにおける誤りをPeople, Subject, Object, Generalの4つに分類しエラー分析を実施。改善可能性や評価方法について探った。多くの誤りがPeopleまたはGeneral。教師の説明文を正しく学習出来るように修正することでBLEUが最大1.0改善。
274	Could you guess an interesting movie from the posters?: An evaluation of vision-based features on movie poster database	https://arxiv.org/abs/1704.02199	Yuta Matsuzaki, Kazushige Okayasu, Takaaki Imanari, Naomichi Kobayashi, Yoshihiro Kanehara, Ryousuke Takasawa, Akio Nakamura, Hirokatsu Kataoka	世界の4つの映画賞におけるポスターのみの情報から作品賞となる映画の予測を試みた論文。Haar-likeのような従来のCV手法や深層学習などいくつかの特徴抽出を試している。予測器にはSVMを使用。LabとEmotionNetでの結果がよく、ポスターの色情報とポスター内の顔の表情の影響が大きいことが分かった。
275	MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications	https://arxiv.org/abs/1704.04861	Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam	モバイル向けにDNNのサイズを小さくしよう(計算コストを軽くしよう)という試み。面の畳み込み(depth wise)とチャンネル方向の畳み込み(point wise)を分け計算コストを削減し、さらに面の広さとチャンネルの深さに係数をかけ、精度と計算量のバランスを調整している。
276	Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning	https://arxiv.org/abs/1704.03976	Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Shin Ishii	正則化をより効果的に行う手法の提案。予測分布を最も大きく変えてしまうような変動を入力データに与え・・・ても、予測ができるように学習をさせる。
277	Learning Character-level Compositionality with Visual Features	https://arxiv.org/abs/1704.04859	Frederick Liu, Han Lu, Chieh Lo, Graham Neubig	文書の分類に、文字の画像情報を利用しようという試み。具体的には、文字をCNNにかけてそれをRNNでエンコードしていき分類を行う。Wikipediaのタイトルを利用し、画像ではない通常の文字単位のembeddingを使うモデル(LOOKUP)と比較して検証。低頻度語に強い分類が可能となった。
278	Get To The Point: Summarization with Pointer-Generator Networks	https://arxiv.org/abs/1704.04368	Abigail See, Peter J. Liu, Christopher D. Manning	要約についての論文で、抜粋型と生成型のいいところ取りをするという手法。Seq2Seqの不正確＋繰り返しが多いという弱点を、生成or抽出(入力文からのコピー)をスイッチする確率p_genの導入＋カバレッジの担保(Attentionの分布を利用)により克服している。
279	A Large Self-Annotated Corpus for Sarcasm	https://arxiv.org/abs/1704.05579	Mikhail Khodak, Nikunj Saunshi, Kiran Vodrahalli	皮肉を検出するための大規模コーパスの公開。Redditという掲示板のデータから、130万のデータが提供。アノテーションは投稿者自身が行っている(皮肉コメントには/sがついている)。Redditには皮肉に/sをつける文化があるらしい(HTMLのタグで囲むようにするのが発祥とのこと)
280	Softmax GAN	https://arxiv.org/abs/1704.06191	Min Lin	GANの学習安定化のためにclassification lossをbinaryでなくてMulticlass cross entropy loss(softmax loss)にした。
281	FAST GENERATION FOR CONVOLUTIONAL AUTOREGRESSIVE MODELS	https://arxiv.org/abs/1704.06001	Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang, Yang Zhang, Mark A. Hasegawa-Johnson, Roy H. Campbell, Thomas S. Huang	RNNを使った自己回帰モデルの（hidden stateの）途中結果をキャッシュしておくことで計算の無駄な繰り返しを無くしWavenetを21倍、PixelCNN++を183倍早くした。
282	Asynchronous Methods for Deep Reinforcement Learning	https://arxiv.org/abs/1602.01783	Volodymyr Mnih, Adri Puigdomnech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu	強化学習において、Asynchronous(非同期)で、Actor-CriticをベースとしてAdvantageを利用して学習するワーカーを、並列に走らせて学習する=A3Cを提唱。Advantageは、行動が推定より良い結果をもたらしたかで表される(R-V(s))。イメージ的には分身の術を使って学習結果を統合する感じ。
283	Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution	https://arxiv.org/abs/1704.03915	Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang	LAPGANなどで用いられているLaplacian pyramid frameworkを活用した超解像手法。SoTA(speed and accuracy)。Feature Extraction BranchとImage Reconstruction Branchの2つのブランチを持つネットワーク構造。bicubic interpolationを使用しないことで計算量の削減と質の向上を図った。lossはl_2 lossでなくCharbonnier lossを用いた。
284	Beating Atari with Natural Language Guided Reinforcement Learning	https://arxiv.org/abs/1704.05539	Russell Kaplan, Christopher Sauer, Alexander Sosa	言葉によるナビで、より高精度のプレイを素早く学習させようという試み。通常の強化学習の仕組みに加えて、「指示を実行できたか」という報酬を追加で与えている。
285	Opening the Black Box of Deep Neural Networks via Information	https://arxiv.org/abs/1703.00810	Ravid Shwartz-Ziv	SGDによる最適化には
286	ADDING GRADIENT NOISE IMPROVES LEARNING FOR VERY DEEP NETWORKS	https://openreview.net/pdf?id=rkjZ2Pcxe	Arvind Neelakantan †, Luke Vilnis †	学習時に勾配にガウシアンノイズを加えると精度が上がる。
287	Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art	https://arxiv.org/abs/1704.05519	Joel Janai, Fatma Gney, Aseem Behl, Andreas Geiger	自動運転に関わる技術は複合的でなおかつ進歩も早いので、初心者にはかなり入りづらくなっている。そこで、自動運転にまつわる画像認識の技術について基礎論文と現時点での最高精度をまとめ、また学習に利用可能なデータセットについてもリストアップ。67pの大作。
288	Understanding deep learning requires rethinking generalization	https://openreview.net/forum?id=Sy8gdB9xx&noteId=Sy8gdB9xx	Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals	DNNが持つ汎化性能の謎に迫る論文。DNNにとっては全てのラベルを覚えてしまうことは簡単なのに、正則化だけでは説明のつかない汎化性能を記録していることを確認。仮説としてSGD自体が汎化性能に貢献している？という提案をしている。
289	A recurrent neural network without chaos	https://openreview.net/forum?id=Sy8gdB9xx&noteId=Sy8gdB9xx	Thomas Laurent, James von Brecht	LSTM/GRUよりシンプルな、input/forgetのみの構成を提案。これにより、LSTM/GRUにおける謎な挙動(※)を回避しつつ精度を出せた。検証は言語モデル(Penn Treebank)。
290	Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling	https://arxiv.org/abs/1611.01462	Hakan Inan, Khashayar Khosravi, Richard Socher	自然言語処理において単語の予測をクラス分類のように0/1でやるのは不自然だということで、予測分布の距離を加味することを提案。また、それにより入力のembeddingと出力のprojectionは使いまわしが可能になることを理論的に証明。これによりパラメーター量も下げられる、はず。
291	Semantic Understanding of Scenes through the ADE20K Dataset	http://people.csail.mit.edu/bzhou/publication/scene-parse-camera-ready.pdf	Bolei Zhou^1, Hang Zhao^1, Xavier Puig^1, Sanja Fidler^2, Adela Barriuso^1, Antonio Torralba^1	セマンティックセグメンテーションの問題において、(1) アノテーションの曖昧性を排除、(2) カテゴリ数の増加(150カテゴリ)、(3) サブカテゴリを導入したデータセットを提案。
292	Adversarial Neural Machine Translation	https://arxiv.org/abs/1704.06933	Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu	GANを翻訳に適用しようという試み。人の翻訳vs機械翻訳(自分の出力)を見分ける識別機と、翻訳生成側(Enc/Dec)を戦わせるという構図。識別には原文と翻訳文をげたマップを畳み込むCNNを使用、生成側は出力が離散なので強化学習の枠組み(識別を騙せたら報酬)を使って学習している
293	Naturalizing a Programming Language via Interactive Learning	https://arxiv.org/abs/1704.06956	Sida I. Wang, Samuel Ginn, Percy Liang, Christoper D. Manning	プログラミング言語と自然言語の橋渡しをする試み。自然言語をどうにか上手く解釈するというトップダウンの方式ではなく、プログラムでの記述を「自然言語化する」というボトムアップのアプローチを取っている。そのデータを集めるために、ブロック積みのゲームVoxelurnを開発したという話
294	U-Net: Convolutional Networks for Biomedical Image Segmentation	https://arxiv.org/abs/1505.04597	Olaf Ronneberger, Philipp Fischer, Thomas Brox	CNNは強力だけどそもそも画像をそんなに用意できないというケースのために、少ない画像でも良く識別できるようなネットワーク構成を提案。畳み込んだ層とアップサンプリングしていった層を合わせることで局所＋グローバルで有効な特徴を学習させる
295	Multimodal Word Distributions	https://arxiv.org/abs/1704.08424	Ben Athiwaratkun, Andrew Gordon Wilson	単語をword2vecのように単一点(ベクトル)ではなく、広がりを持った分布で表現しようという試み。複数の意味を表現するため、分布を複合した多峰分布で表現する。word2vecの学習法を取り入れつつ(近くに来る単語の分布の距離は近いとする)学習。類似度推定等のタスクで最高精度
296	Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning	https://arxiv.org/abs/1705.00557	Yacine Jernite, Samuel R. Bowman, David Sontag	文のエンコードの速度を上げようという話。そのための3つの目的関数(タスク)を提唱。1.文が連続したものか否かの0/1、2.段落冒頭3文に続く一文を5つの中から選択、3.接続詞を抜いた上で、そのタイプを予測させる。※学習データは自動作成(教師なしなので)。これで6~40倍の高速化。
297	Dense-Captioning Events in Videos	https://arxiv.org/abs/1705.00754	Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Juan Carlos Niebles	動画内の複数のイベント（時間的な重複あり）を説明文する話。3D-CNNで特徴抽出→イベント範囲推定→説明文生成の流れ。説明文生成時は他イベントをアテンションしたり、過去や未来のイベントをコンテキスト情報に用いる。ActivityNet CaptionデータセットでSOTA。
298	Efficient Natural Language Response Suggestion for Smart Reply	https://arxiv.org/abs/1705.00754	Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-hsuan Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, Ray Kurzweil	GoogleのSmart Replyの仕組み(具体的には受信メールを元に返信候補をランキングする箇所)について。本文や件名など、複数パートのn-gramから特徴を抽出し、受信/返信候補で内積をとるモデルを採用。基本的なSeq2Seq(ベクトル表現のみ利用)より高速かつ同精度を達成
299	STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset	https://arxiv.org/abs/1705.00823	Yuya Yoshikawa, Yutaro Shigeto, Akikazu Takeuchi	MS COCOの画像に対する日本語キャプションのデータセットが公開。単純に翻訳するより良好な結果。
300	From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood	https://arxiv.org/abs/1704.07926	Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, Percy Liang	自然言語を実行可能な論理式に変換する試み。「理解していないけど最終的な実行結果は合っている」タイプの変換を避けるのを課題としている。このために強化学習と周辺尤度最大化を複合した手法(RANDOMER)を提案。探索的にノイズを加えたビームサーチ・頻度に依存しない重みの付与の二点が肝
301	Visual Attribute Transfer through Deep Image Analogy	https://arxiv.org/abs/1705.01088	Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, Sing Bing Kang	画像上の特徴を転移させる試み。A→A'=B→B'というアナロジーを元にモデルを組んでいる(AとB'が既知で、変換後のA'と変換前(A風の)Bを推定)。学習済みVGGから5つの階層別特徴マップを作成し、最上位から似た特徴点の探索、それによるA'/Bの再構成、を繰り返して作成を行う
302	A Syntactic Neural Model for General-Purpose Code Generation	https://arxiv.org/abs/1704.01696	Pengcheng Yin, Graham Neubig	プログラミング言語のコード自動生成タスクにおいて、言語の抽象構文木(AST)を取り入れ精度向上。SoTA。自然言語をASTに変換するモデルも定義。DecoderにAPPLYRULE, GENTOKENという操作が加えられる。Pythonコード生成タスクにおいて、SoTA手法と比較して10ポイント程度accuracyが向上。
303	Simple Black-Box Adversarial Perturbations for Deep Networks	https://arxiv.org/abs/1704.01696	Nina Narodytska, Shiva Prasad Kasiviswanathan	入力画像に細工を施すことで、誤識別を誘発できるという検証。ネットワークの構成がわかっているホワイトボックス型と不明なブラックボックス型があるが、本研究では後者の手法で検証。ランダムに選択された画像の各所から最も識別に影響を与える箇所を探索する形(greedy local-search)で実装を行い、1~3%程度のピクセルを操作するだけでエラーレートを跳ね上げられることを確認。
304	Convolutional Sequence to Sequence Learning	https://arxiv.org/abs/1705.03122	Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin	Facebookが発表したCNNによる翻訳の研究。翻訳の評価指標であるBLEUスコアを改善できただけでなく、9倍の高速化に成功した。
305	Neural Models for Information Retrieval	https://arxiv.org/abs/1705.01509	Bhaskar Mitra, Nick Craswell	Information Retrieval(クエリに基づいて、検索対象のドキュメントをランキングするような手法)について、ニューラルネットワークだけでなく既存の手法も取り上げ比較を行い、DNNの応用まで言及するという全部入りのありがたいチュートリアル。
306	A Deep Reinforced Model for Abstractive Summarization	https://metamind.io/research/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization	Romain Paulus, Caiming Xiong, and Richard Socher	文章要約で教師有と強化学習を併用したという話。モデルはSeq2Seq+Attention(リピート防止にDecoder側も参照)。単純な教師有だと「正解」に固執する傾向があるため、自由度を持たせた強化学習も導入(ROUGEにより良さを評価)。補完的に働かせ良好な結果が得られた
307	Unsupervised Learning by Predicting Noise	https://arxiv.org/abs/1704.05310	Piotr Bojanowski, Armand Joulin	ディープラーニングを用いた高速で高精度な教師なし学習アルゴリズムを提案。SoTA。低次元の超球からサンプリングしたtarget vectorsに特徴マップ（ベクトル）を近づけるように学習。ハンガリアン法をミニバッチ内で実行すれば高速に割あて問題を解ける。片側の行列をupdate。損失関数は計算高速化のため にseparable square lossを使用。
308	Curiosity-driven Exploration by  Self-supervised Prediction	https://pathak22.github.io/noreward-rl/	Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell	強化学習において、特に高次元になると報酬が得られる機会はとても少なくなる。そこで「好奇心」、つまり新規性のある環境への到達について報酬を設定することで学習速度を上げる試み。これによりベースライン(A3C)よりも高い学習性能を記録することができた。Doomとマリオブラザーズのデモ有
309	Communication-Efficient Learning of Deep Networks from Decentralized Data	https://arxiv.org/pdf/1602.05629.pdf	H. Brendan McMahan Eider Moore Daniel Ramage Seth Hampson Blaise Aguera y Arcas	学習データを中央に集めない分散学習の問題Federated optimizationの定式化。通信量やプライバシーの観点から各モバイルデバイスで収集される学習サンプルをデバイス上で学習し集約すること考える。DNNをターゲットとしSGDベースのアルゴリズムFedAvgを提案。複数のネットワークアーキテクチャで少ない通信回数で学習することを実現した。AISTATS 2017
310	Recurrent Batch Normalization	https://arxiv.org/abs/1603.09025	Tim Cooijmans, Nicolas Ballas, Csar Laurent, alar Glehre, Aaron Courville	Batch NormalizationをRNNにも適用しようという話。具体的には、RNNの再帰部分の重みについても正規化を行う(論文中では、LSTMの重みに対して適用を行なっている)。
311	FOIL it! Find One mismatch between Image and Language caption	https://arxiv.org/abs/1705.01359	Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurelie Herbelot, Moin Nabi, Enver Sangineto, Raffaella Bernardi	画像キャプションの研究について、本当に理解してキャプションを生成しているのかをテストするデータセットの提案。MS COCOのキャプションに意図的に一つのFoil(=間違い)(犬から猫に置き換えなど)を潜ませ、それを検知できるか、また間違いの場所を特定/訂正できるかをテストする。
312	Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning	https://arxiv.org/abs/1705.06769	Nat Dilokthanakul, Christos Kaplanis, Nick Pawlowski, Murray Shanahan	報酬が疎な環境でどうエージェントを学習させるかについての取り組み。先日の好奇心の導入(
313	Exploring the structure of a real-time, arbitrary neural artistic stylization network	https://arxiv.org/abs/1705.06830	Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, Jonathon Shlens	画像の画風変換については、通常変換させる画風ごとに専用のネットワークが必要だった。しかし最近は上手く正規化すれば様々な画風を一つのネットワークで表現できることがわかってきた・・・ので、大量のデータ(8万画像)で学習させて(初見も含めた)画風に対応できるネットワークを構築できた話
314	Generating Sentences from a Continuous Space	https://arxiv.org/abs/1511.06349	Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio	文章に対してVAEを適用し、文全体の情報をベクトル化しそれを文生成に利用しようという話。基本はRNN/Z/RNNという構成のVAEだが、このままだと学習が上手くいかないので、学習初期に正規化の役割を担うKL項を抑制したり、賢すぎるDecoderを制限するといった工夫をしている。
315	Look, Listen and Learn	https://arxiv.org/abs/1705.08168	以下の2つのデータセットを組み合わせた。	動画データセットを活用した教師なし学習。アーキテクチャ自体はシンプルだが、音声の2クラス分類ではベンチマークでSoTA、画像ではImageNetのself-supervisedとしてはSoTAに匹敵する性能をもち、物体検出やfine-grainedの認識タスクもこなせるモデル（L^3-Net）を学習した。
316	Detecting and Explaining Crisis	https://arxiv.org/abs/1705.09585	Rohan Kshirsagar, Robert Morris, Sam Bowman	個人のSNS上の発言などから危機的状況(自殺しちゃいそうなど)かを検知するだけでなく、その状況の度合いを判断(トリアージ)するための根拠を提案するという試み。手法としてはAttentionを参照することで重要語句を抽出する。最近のモデル解釈研究がまとまっているという点でもよい。
317	Iterative Machine Teaching	https://arxiv.org/abs/1705.10470	Weiyang Liu, Bo Dai, James M. Rehg, Le Song	機械学習を行う際、生徒たるモデルに対して与えるデータを「先生」が効率的に選ぶというスタイルの提案(生徒と先生は目的関数を共有)。学習率が高い状態では簡単なもの、学習率が低くなってきた状態では前回データとの一貫性が重要になるとのこと。目的関数からの導出過程がとても丁寧に書かれている
318	Adversarial Generation of Natural Language	https://arxiv.org/abs/1705.10929	Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal, Aaron Courville	文生成にGANを適用してみるという話。GANはWGANで、1-hot vectorの列で表現された文章と、GANから生成した分布を比較する(双方文長ｘ語彙のマップになる)。それなりに学習できているように見えるが、単純なRNNとの比較がなく文長/語彙もかなり絞っているので様子見
319	Optimizing Neural Networks with Kronecker-factored Approximate Curvature	http://proceedings.mlr.press/v37/martens15.pdf	James Martens, Roger Grosse	DNNを対象としたnatural gradientベースの2次オーダ勾配法K-FACの提案。Fisher情報行列をkronecker積を使い近似評価。その逆行列をブロック対角/ブロック3重対角で近似し計算する。Fisher情報行列をHessianとみなし近似(できる)し2次アルゴリズムを構築。Momentum SGDとの比較しイテレーション回数を桁のオーダで削減できるので並列計算に向いている。
320	Deep Learning is Robust to Massive Label Noise	https://arxiv.org/abs/1705.10694	David Rolnick, Andreas Veit, Serge Belongie, Nir Shavit	ニューラルネットでは、ノイズが多く含まれるデータセットでも大丈夫ということを検証した研究。ノイズが一様な場合/偏りがある場合、ノイズとして与えるデータがデータセットに含まれない新規のものかどうか、などで検証しているが傾向に大きな違いは見られないとのこと。
321	IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models	https://arxiv.org/abs/1705.10513	情報抽出において、query->documentの生成過程を学習するgenerativeな手法は特徴量モデリングに優れるがリンク・クリックなどを活用できず、Learn2Rankなどのラベル有り/無しの膨大なデータからランキング関数を学習することはできるが、特徴量のモデリングに劣る。そこで、この論文では、2つの手法を組み合わせたIRGANを提唱している。	IR（情報抽出）にGANを適用。Discriminatorは抽出関すを最大化するようタグ付けされたデータを学習し、GeneratorはDiscriminatorが識別しにくいデータを生成する。IRGANで学習したモデルはweb検索、アイテム推薦でスコアを更新した。
322	DiracNets: Training Very Deep Neural Networks Without Skip-Connections	https://arxiv.org/pdf/1706.00388.pdf	Sergey Zagoruyko, Nikos Komodakis	ResNetのskip-connection構造を表現する重みのパラメトライゼーションDirac parametrizationの提案。convolution層の操作を$(a\delta+bW_{norm})\odot x$とする。$\odot$はconv操作、$W_{norm}$は正規化した通常のconv操作のテンソル、$\delta$はconv操作に対して恒等なテンソルでこれによってskip-connectionを表現する。これとNCReLUを導入したDiracNetを評価した。わずかな計算量の増加でskip-connectionなしで深い層の学習を実現し性能も同等。
323	The Kinetics Human Action Video Dataset	https://arxiv.org/abs/1705.06950	Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman	行動認識において大規模化のみならず網羅性や機械学習のための校正されたデータセットを提案。3Dカーネルを用いたDNNの学習においても有効である。
324	ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation	https://arxiv.org/abs/1606.02147	Adam Paszke, Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello	DNNの精度を落とさず演算速度をどう上げるかという課題へのアプローチ(最近トレンドになっている)。構成は下図の通りで、18倍の速度向上とパラメーター数を1/79に削減。ポイントはdown samplingで、早い層から仕掛けている
325	Reading Wikipedia to Answer Open-Domain Questions	https://arxiv.org/abs/1704.00051	Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes	Wikipediaの知識を使って一般的な質問に回答する試み。回答を含む文章の取得、文章から回答箇所を抜粋するの二段階で、前者はbigramのTF-IDFベクトルを使って検索、後者は学習済み分散表現(Glove)・品詞などの単語特徴・質問中の単語との一致などを入力としたRNNを利用
326	A simple neural network module for relational reasoning	https://arxiv.org/abs/1704.00051	Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap	オブジェクト同士の「関係性」を学習させるためのモジュールを発明したという研究。2つのベクトルを引数に演算する関数と(関係の数だけ総当たりで計算)、その和を基に演算する関数の二つでできている(実験では双方3~4層のNN)。これで画像キャプションなどで驚きの精度を達成。
327	Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments	https://arxiv.org/abs/1706.02275	Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch	マルチエージェントの強化学習について、どのような学習方法が良いのかについての研究。Actor/Criticモデルを適用し、個々のプレイヤー(Actor)にコーチ(Critic)をつける形で学習するのが良いとのこと。コーチは他のプレイヤーの状況を把握でき、その上で最適な指示を行う
329	Attention Is All You Need	https://arxiv.org/abs/1706.03762	Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin	RNN/CNNを使わず翻訳のSOTAを達成した話。Attentionを基礎とした伝搬が肝となっている。単語/位置のlookupから入力を作成、Encoderは入力＋前回出力からAを作成しその後位置ごとに伝搬、DecoderはEncoder出力＋前回出力から同様に処理し出力している
330	Deep reinforcement learning from human preferences	https://arxiv.org/abs/1706.03741	Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei	強化学習で報酬関数の設計を人が行っていると、定義にミスがあった時事故につながったりする。そこで報酬を関数表現でなく直接的に与えることでより明確な学習をさせるというもの。具体的にはエージェントが提示する2つの行動についてどちらが好ましいかを人が教示することでフィードバックを与える。
331	SmoothGrad: removing noise by adding noise	https://arxiv.org/abs/1706.03825	Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vigas, Martin Wattenberg	画像のどの部分が分類に貢献したかを確認するための手法として画像に対する勾配を可視化する手法があるが、そのまま使うととてもノイズが多い。そこでがガウシンアンノイズを加えたn個の入力に対する勾配を平均することでスムージングを行ったという話。
332	Hybrid Reward Architecture for Reinforcement Learning	https://arxiv.org/abs/1706.04208	Harm van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, Jeffrey Tsang	強化学習において攻略困難だったパックマンを攻略したことで話題となった研究。状況からの報酬の推定・行動の決定を一本で行わず、報酬の推定と行動決定の関数を分離。かつ、複数のエージェント(状況認識部分は共有なので、イメージとしては多頭になる)に持たせたそれらの重み付き和から学習を行う。
333	On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima	https://arxiv.org/abs/1609.04836	Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang	なぜ「ミニ」なバッチの方が大きいバッチよりもうまくいくのかを検証した論文。理論的な検証ではなく実際の学習で実地的に検証を行っており、結果ミニなバッチの方が周辺がフラットな最適解の方にたどり着くのに対し、大きいバッチはシャープな最適解にたどり着く傾向があるとのこと。
334	Train longer, generalize better: closing the generalization gap in large batch training of neural networks	https://arxiv.org/abs/1705.08741	Elad Hoffer, Itay Hubara, Daniel Soudry	大きいバッチサイズでも汎化性能を高めることができるということを示した研究。フラットな最適解への到達には更新回数が大きく関係しており、大きなバッチの場合同エポックだとこの更新回数を稼げないのが問題とのこと。また、高めの学習率の設定やBatch Normでも十分抑止できる。
335	Adversarially Regularized Autoencoders for Generating Discrete Structures	https://arxiv.org/abs/1706.04223	Junbo (Jake) Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, Yann LeCun	GANを利用したテキストの生成(と潜在構造の獲得)を試みた研究。encodeして単純にGANを適用するだけではなかなか上手くいかないので、Auto Encoderと並行して学習させるという手法を提案(AE側のencodeを真としてGANを適用)。結果としては微妙なところ。
336	Optimization as a Model for Few-Shot Learning	https://openreview.net/forum?id=rJY0-Kcll	Sachin Ravi, Hugo Larochelle	「学習方法」を学ぶメタラーナーを利用することで、少ない学習データから正答できるようにする(Few-Shot Learning)ことを試みた研究。メタラーナーはLSTM、学習側はCNNで、学習側のlossからメタラーナーが更新パラメーターを決定して、学習側に渡す形になる。
338	One Model To Learn Them All	https://arxiv.org/abs/1706.05137	Lukasz Kaiser, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, Jakob Uszkoreit	1モデルでマルチタスクを解かせるという試み。タスクは画像認識・音声認識・形態素解析・翻訳などの計8タスクで、モデルとしては各入力に対するEncoder・出力に変換するDecoder、それらの間をつなぐ=マルチモーダルな情報をミックスるI/O Mixerという構成。
339	Interactive 3D Modeling with a Generative Adversarial Network	https://arxiv.org/abs/1706.05170	Jerry Liu, Fisher Yu, Thomas Funkhouser	GANを利用し初心者が3Dオブジェクトを構築するのをサポートするツールを開発した話。まずざくっと作った後に「SNAP」コマンドを実行すると、ベテラン達の3Dオブジェクトから学習したGANがいい感じに調整。それを修正してさらにSNAPして・・・と繰り返す。
340	A Neural Parametric Singing Synthesizer	https://arxiv.org/abs/1706.05170	Merlijn Blaauw, Jordi Bonada	音声合成の研究で、WaveNetを踏襲しつつ生の音声でなく音響特徴量(図はスペクトログラムに見えるが、WORLDというソフトウェアで抽出しているよう)を使用することで計算時間を短縮しつつ、ピッチや音色といった要素を個別に扱えるようになった。これを利用した歌声もデモで公開されてる
341	Annotating Object Instances with a Polygon-RNN	https://arxiv.org/abs/1704.05548	Lluis Castrejon, Kaustav Kundu, Raquel Urtasun, Sanja Fidler	画像のアノテーションを楽にするために、候補領域を予測しポリゴンで囲み、ユーザーが行わないといけないのはポリゴンの頂点の編集だけにするという試み。ポリゴンは学習済みVGGで特徴量を抜いてConvLSTMで予測している。これでアノテーションの精度を保ちつつ5~7倍の速度向上を実現。
342	CortexNet: a Generic Network Family for Robust Visual Temporal Representations	https://arxiv.org/abs/1706.02735	Alfredo Canziani, Eugenio Culurciello	動画において時系列で認識結果がぶれないようにすることを目指した研究。単純なConvだけでなくDeconvの結果も取り入れる構造を考案しさらに階層上に積んでいる。これをフレーム間マッチ(MatchNet)と時系列での予測差異(TempoNet)の2種で学習させている。
345	cGAN-based Manga Colorization Using a Single Training Image	https://arxiv.org/abs/1706.02735	Paulina Hensman, Kiyoharu Aizawa	GANをベースに、1つの白黒/カラーのペアからの学習だけで漫画を塗りきるという研究。そもそも同じキャラは同じ色、同じパーツ(顔、服etc)は同じ色なので、何枚もいらないでしょということでパーツごとにcrop(セグメンテーション)を行い学習を行っている。結果はかなり衝撃的。
346	A Closer Look at Memorization in Deep Networks	https://arxiv.org/abs/1706.05394	Devansh Arpit, Stanisaw Jastrzbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, Simon Lacoste-Julien	DNNは確かにラベルを覚えきる力があるが、本物とノイズではデータの性質が異なり(ノイズはデータ間の関連がない)、学習プロセスにもそれが影響しているのでは？という検証。結果として、共通パターンを先行して覚えることが示唆されている。
347	Programmable Agents	https://arxiv.org/abs/1706.06383	Misha Denil, Sergio Gmez Colmenarejo, Serkan Cabi, David Saxton, Nando de Freitas	ロボットアームに論理形式の記述(GET[AND[赤い, ボール]]で赤いボールをとる、みたいな)を実行させる研究。各オブジェクトの情報(環境情報)を入力にしてオブジェクトと特徴(赤い、など)のマトリクスを作成。そこから行動を推定するネットワークを構築し、強化学習で学習させている。
348	Do GANs actually learn the distribution? An empirical study	https://arxiv.org/abs/1706.08224	Sanjeev Arora, Yi Zhang	GANは与えている画像をあまり学習していないのではという話。きちんと学習していれば生成画像は無限の組み合わせのパターンがあるはずなのに、実際はごく少ない生成画像の中で重複がたやすく見つかる。Discriminatorのサイズを大きくすることでこの問題は回避できるかも？としている
349	YellowFin and the Art of Momentum Tuning	https://arxiv.org/abs/1706.03471	Jian Zhang, Ioannis Mitliagkas, Christopher R	SGDが最近見直されてきているが、重要なパラメーターであるMomentumについてはあまり議論がされていない。滑らかな勾配の曲面における最適なMomentumの値は数理的に証明が可能であり、これに伴い学習率の低減についても一定値が求まる。これを応用したYellowFinという最適化法を発明し、検証を行ったところ既存の最適化手法よりも高速に収束することが確認できた。
350	The E2E Dataset: New Challenges For End-to-End Generation	https://arxiv.org/abs/1706.09254	Jekaterina Novikova, Ondej Duek, Verena Rieser	End-to-Endの対話システムを構築するためのデータセットが公開。50万発話でが含まれ、ドメインはレストラン検索となっている。発話に対しては固有表現(slot)的なアノテーションもされている(「フレンチが食べたい。500円くらいで」なら、種別=フレンチ、予算=500円など)。
351	Methods for Interpreting and Understanding Deep Neural Networks	https://arxiv.org/abs/1706.07979	Grgoire Montavon, Wojciech Samek, Klaus-Robert Mller	DNNの判断を理解するための研究のまとめ。ネットワークが反応する入力を見つける方法(Activation Maximizationなど)、判断根拠となった特徴を入力にマップする方法(Relevance Propagationなど)などを紹介、説明力の比較方法についても記載している
352	Learning by Association - A versatile semi-supervised training method for neural networks	https://arxiv.org/abs/1706.07979	Philip Husser, Alexander Mordvintsev, Daniel Cremers	ラベルなしデータを活用した学習方法の提案。同じラベルのデータは当然近いベクトル表現になるはずなので、例えば(数字の)1とラベルされた画像のベクトル表現→近い表現をラベルなしから探す→さらにそれに近いものをラベルありから探す=1とラベルされた画像に戻るはず、という仮定から学習を行う
353	Noisy Networks for Exploration	https://arxiv.org/abs/1706.10295	Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, Shane Legg	強化学習を行う際に内発的報酬などを組み込む手法があるが、これは実際に環境から得られる報酬とは異なるため場合によっては学習結果をゆがめてしまう可能性がある。そこで、ランダムな探索をより意図的に行うためにネットワークの伝搬にノイズを組み込むことを提案(ノイズを組み込むため、ε-greedyによる探索も必要なくなる)。これにより多くのゲームでスコアを改善できた
354	Efficient Attention using a Fixed-Size Memory Representation	https://arxiv.org/abs/1707.00110	Denny Britz, Melody Y. Guan, Minh-Thang Luong	AttentionをDecode時に毎回計算するのでなく、Encoder時に(数を絞って)計算しておくことで計算速度を向上させるという話。最初の予測には最初の方、最後の予測には最後の方に注目させるためPosition Encodingも併せて行っている。
356	Dual Supervised Learning	https://arxiv.org/abs/1707.00415	Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, Tie-Yan Liu	機械学習における対称性に注目した研究。翻訳で日->英に対し英->日があるように、あるタスクには対となるタスクが存在する。であれば同時に学習させたほうが良いのではという主張。対象関係のタスクの条件付確率が等しくなるような制約をかけて学習させ、翻訳・画像・文判定の3つで検証している。
357	Online and Linear-Time Attention by Enforcing Monotonic Alignments	https://arxiv.org/abs/1704.00784	Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck	(soft) Attentionの計算範囲を限定して処理速度を向上させる話。Attentionは過去のEncoderの状態全てに対してどこが重要か計算するが、実際重要な箇所はDecoderの生成に伴い時間軸上を徐々に右にずれていく単純な推移になる。ならその範囲に限定しようという話
358	Meta-Learning with Temporal Convolutions	https://arxiv.org/abs/1707.03141	Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel	メタラーニングを手軽に行うための追加レイヤの提案。通常の学習ではデータの分布を学習させるが、メタラーニングではタスクの分布を学習させる(以前の入力や状況と似ているか)。そこで各タスク(入力＋判定)をDilated CNNで畳み込むレイヤを追加。画像認識と強化学習で検証し、精度向上
359	NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles	https://arxiv.org/abs/1707.03501	Jiajun Lu, Hussein Sibai, Evan Fabry, David Forsyth	画像に微細な変更を加えることで誤認識をさせる試みがあるが、実世界での運用上は心配しなくてもいいのではという研究。誤検知を誘発する変更は画像をとった距離/角度に固有のもので、少しそれらが変わると正しく認識されるとのこと。自動運転などでは対象画像までの距離/角度はすぐに変わるのでOKという
360	Creatism: A deep-learning photographer capable of creating professional work	https://arxiv.org/abs/1707.03491	Hui Fang, Meng Zhang	写真をプロ級に加工する仕組みの紹介。実際のプロの写真に意図的にフィルタをかけてネガティブサンプルを作り、その修正方法をGANを利用して学習させている。
361	Be Careful What You Backpropagate: A Case For Linear Output Activations & Gradient Boosting	https://arxiv.org/abs/1707.04199	Anders Oland, Aayush Bansal, Roger B. Dannenberg, Bhiksha Raj	ニューラルネットでは層を積むほど複雑性が増し最適化が難しくなる。なので必要以上に層を積まない方がいいが、この不必要の筆頭が表現力に貢献しないが非線形である出力層(softmaxなど)だ！という指摘。出力は線形、伝搬誤差は累乗(強調)することで精度と収束速度の向上が確認できた。
362	Learning to select data for transfer learning with Bayesian Optimization	https://arxiv.org/abs/1707.05246	Sebastian Ruder, Barbara Plank	転移学習は通常は1:1で行われることが多いが、これだと個別に学習を行う必要がある。そこで転移のための学習データを選択するモデルを独立に構築することを提案。通常はドメイン間の類似度のみが指標として使われることが多いが、複数の類似度とデータ間の多様性の指標を使いモデルを構築している。
363	Learning from Demonstrations for Real World Reinforcement Learning	https://arxiv.org/abs/1704.03732	Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, Joel Z. Leibo, Audrunas Gruslys	DQNは学習に時間がかかるので、お手本を与えておくことでその時間を短縮しようという試み。お手本でまずは学習し、その後の実際の学習でもお手本の情報はReplay bufferにキープし、なおかつ重みをつけて学習する。
364	On the State of the Art of Evaluation in Neural Language Models	https://arxiv.org/pdf/1707.05589.pdf	Gbor Melis, Chris Dyer, Phil Blunsom	よくチューニングされたLSTMはSOTAを出したと言われるモデル(ここではRecurrent Highway Network)に比肩するという話。下図が4-layer LSTMで上位の成績を収めたパラメーターの設定図となる。この範囲内であれば、perplexityの変動は~3程度に収まる。
365	Optimizing the Latent Space of Generative Networks	https://arxiv.org/abs/1707.05776	Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam	GANはCNN+Adversarialな学習という点に特徴があるが、後者の形態は学習を難しくしている。そこでdiscriminator(以下D)をとってしまい、ノイズとサンプルから元画像を復元する。そしてDの持つ判定力を模倣するため、ラプラシアン階層ごとの差分を利用し学習する。
366	Proximal Policy Optimization Algorithms	https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf	John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov	Policy gradientは様々なタスクで利用されているが、戦略の更新幅の設定が難しく、小さいと収束が遅くなり大きいと学習が破綻する問題があった。そこで、TRPOという更新前後の戦略分布の距離を制約にするモデルをベースに、より計算を簡略化したPPOという手法を開発した。
367	Learning Cross-modal Embeddings for Cooking Recipes and Food Images	http://im2recipe.csail.mit.edu/	Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar Weber, Antonio Torralba	料理の画像からそのレシピを推察するという研究。このため、料理画像とレシピのペアのデータセットを作成している(総計100万、料理種は80万)。モデルにおいては、自然言語側は材料をencode・手順をencodeして全結合。画像側はCNNにかけてベクトル化。双方の距離と、同一の食品カテゴリの分類で最適化を行っている。
368	A Distributional Perspective on Reinforcement Learning	https://arxiv.org/abs/1707.06887	Marc G. Bellemare, Will Dabney, Rmi Munos	強化学習で使用されるBellman Equationについて、報酬の期待値ではなく状況/行動に対する「分布」を使用しようという提案。「期待値」を利用する場合どんな状況における報酬も最終的には平均化されてしまうが、分布なら個別の状況に応じて報酬を推定することができる、という。
369	Adversarial Sets for Regularising Neural Link Predictors	https://arxiv.org/abs/1707.07596	Pasquale Minervini, Thomas Demeester, Tim Rocktschel, Sebastian Riedel	エンティティ間の関係(is-a)を学習させる際に、敵対的なサンプルを使うことで正規化を行うという手法。これまでの手法では猫＝ネコ科、ネコ科＝動物という個別の関係は学習できるが、猫＝動物という推移関係には弱かった。そこで、A・B、B・Cに加えC・Aまで含めて学習を行っている。
370	Parameter Space Noise for Exploration	https://arxiv.org/abs/1706.01905	Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, Marcin Andrychowicz	ノイズの入れ方で学習効率が大きく変わるという話。通常の強化学習では行動を決定した後にノイズで散らすが、提案手法は行動を決定するネットワーク自体にノイズを乗せる。既存の手法ではエージェントの意思決定とは無関係にノイズが作用するので、予測不能な探索をする可能性があったとのこと。
371	Machine Teaching: A New Paradigm for Building Machine Learning Systems	https://arxiv.org/abs/1707.06742	Patrice Y. Simard, Saleema Amershi, David M. Chickering, Alicia Edelman Pelton, Soroush Ghorashi, Christopher Meek, Gonzalo Ramos, Jina Suh, Johan Verwey, Mo Wang, John Wernsing	機械学習を利用したいというニーズに応えていくには、機械学習モデルの構築作業を分業していく必要があるという提言。現在は一人の職人がデータ収集から前処理、モデルの構築まで全部を行い、そのプロセスが属人的になることが多い。なので、最低限アルゴリズム構築と学習は分けようという。
372	Robust Physical-World Attacks on Deep Learning Models	https://arxiv.org/abs/1707.08945	Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, Dawn Song	道路標識を誤認させるサンプルを作成するという研究。生成した停止の標識のサンプル(をプリントしたもの)を、速度制限の標識に100%誤認させることが可能だったという結果。手法としては誤認識させる最小かつ印刷可能な変動を、標識の範囲内のみという制約(Mask)をかけて計算している。
373	Learned in Translation: Contextualized Word Vectors	https://arxiv.org/abs/1708.00107	Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher	学習済み機械翻訳モデルのEncoderを用いることで、単語だけでなく(単語ではWord2Vecのような分散表現がよく用いられる)文脈の転移学習を行おうという研究。入力に単語・文脈それぞれのベクトルを結合したものを用いることで、感情や質問分類、Q&Aといったタスクで効果を確認。
374	Natural Language Processing with Small Feed-Forward Networks	https://arxiv.org/abs/1708.00214	Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, Slav Petrov	モバイルにも組み込めるNLPのネットワークを目指した研究。単語ベースの分散表現だとベクトル/辞書が大きくなるため、文字ベースのn-gram(bi/tri)を特徴量とし、2-layerで予測を行なっている。これで言語特定や形態素解析といったモバイル上で使うようなタスクで高精度を維持
376	Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm	https://arxiv.org/abs/1708.00524	Bjarke Felbo, Alan Mislove, Anders Sgaard, Iyad Rahwan, Sune Lehmann	自然言語における感情分析はネガポジの様に二値で行うことが多いが、これでは感情の機微を表現できない。ただ、多感情にするとラベル付けが大変。そこで、Twitterにつけられた絵文字を予測させる形で学習を実行。12億ツイート(!!)でbi-LSTM+Attentionのモデルを学習。
377	Understanding Black-box Predictions via Influence Functions	https://arxiv.org/abs/1703.04730	Pang Wei Koh, Percy Liang	DNNの判断根拠を理解するための試みで、ある学習データ(サンプル)がなかった場合のモデルへの影響を手がかりにする。通常だと該当サンプルを抜いての再学習が必要だが、該当サンプルのlossを増減させた場合の最適解を既存の最適解から導出するという技を使いこれをクリアしている。
378	Audio Super Resolution using Neural Networks	https://arxiv.org/abs/1708.00853	Volodymyr Kuleshov, S. Zayd Enam, Stefano Ermon	音声における低解像度(低サンプリングレート)⇒高解像度の研究。サンプルの間を補完させる形で予測を行う。モデルはシンプルなdown sampling/upsamplingのCNNを組み合わせたもの。
379	Learning Visual Importance for Graphic Designs and Data Visualizations	https://arxiv.org/abs/1708.00853	Zoya Bylinskii, Nam Wook Kim, Peter O'Donovan,Sami Alsheikh, Spandan Madan, Hanspeter Pfister,Fredo Durand, Bryan Russell, Aaron Hertzmann	グラフやグラフィックにおいて、人が重要と認識する箇所を予測する研究。BubbleViewという手法(靄のかかった画像から見たい個所をクリックしてもらう。二場面の図参照)や実際に囲ってもらうことでデータセットを作成。手法はFCNベース。
380	Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?	https://arxiv.org/abs/1708.02657	Xiang Zhang, Yann LeCun	中国語圏の言語（CJK)ではテキストのエンコーディングの単位として，UTF-8 bytes, 文字，単語，ローマ字書きの文字，ローマ字書きの単語の５種類があるが、どの単位でエンコーディングするのがテキスト分類のパフォーマンスにとって良いのか、英語、中国語、日本語、韓国語の４つで比較検討した研究。
381	Regularizing and Optimizing LSTM Language Models	https://arxiv.org/abs/1708.02182	Stephen Merity, Nitish Shirish Keskar, Richard Socher	シンプルなLSTMを言語モデル用に限界までチューニングしてみるという研究。メインの工夫は、リカレントの接続にDropConnectを適用する＋SGDで更新を行う際一定期間の平均を利用するASGDを、一定間隔の性能チェックで悪化していた場合に行うようにしたNT-ASGDの2点。
382	SEARNN: Training RNNs with Global-Local Losses	https://arxiv.org/abs/1706.04499	Rmi Leblond, Jean-Baptiste Alayrac, Anton Osokin, Simon Lacoste-Julien	RNNにおいて、一定箇所まで一旦予測し(Roll in)、そこから終端までを予測した結果(Roll out)を実際のデータと比較して学習を行う手法の提案。
383	Key-Value Retrieval Networks for Task-Oriented Dialogue	https://arxiv.org/abs/1705.05414	Mihail Eric, Christopher D. Manning	外部知識を利用したタスク指向対話をEnd2Endで学習させる試み。外部知識は(対象 属性 値)のような形で格納し(打合せ 時間 5時、など)、対象/属性をキーとしてAttentionにより引いてくる。これを語彙に含め予測する。
384	DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction from a Single Image	https://arxiv.org/abs/1708.04672	Andrey Kurenkov, Jingwei Ji, Animesh Garg, Viraj Mehta, JunYoung Gwak, Christopher Choy, Silvio Savarese	2次元の画像から3Dモデルを生成する試み。既存の研究は直接2D=>3Dを生成モデルで行う形だったが、これだと生成の精度があまり良くなかった。そこで2Dの画像と「似ている3D」をまず検索し、この「似ている3D」を足掛かりに生成を行うというモデルを提案している。
385	The Shattered Gradients Problem: If resnets are the answer, then what is the question?	https://arxiv.org/abs/1702.08591	David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, Brian McWilliams	shattered gradients problem（近しい入力に対する勾配が大きく異なる問題）を定義し、深いフィードフォワードネットワークではこれが起こりやすいこと、またskip connectionはこの問題を回避できることを解析している。
386	Deep Interest Network for Click-Through Rate Prediction	https://arxiv.org/abs/1706.06978	Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Xiao Ma, Yanghui Yan, Xingya Dai, Han Zhu and Junqi Jin	DLでのCTR予測。ユーザ素性の各成分ごとに埋め込みベクトルを作成し、attentionをはりユーザベクトルを作成することで、多様なユーザ行動と1部の行動履歴のみがクリックに寄与することを表現した。attention重みは広告の素性ベクトルと埋め込みベクトルの類似度とした。データスパースネスに対応し低頻度素性ほど強い正則化をかけ過学習を防ぐ。GAUC(AUCの拡張)を評価し既存手法をうわまわる。
387	Recent Trends in Deep Learning Based Natural Language Processing	https://arxiv.org/abs/1708.02709	Tom Young (1), Devamanyu Hazarika (2), Soujanya Poria (3), Erik Cambria (4) ((1) School of Information and Electronics, Beijing Institute of Technology, China, (2) School of Computing, National University of Singapore, Singapore, (3) Temasek Laboratories, Nanyang Technological University, Singapore, (4) School of Computer Science and Engineering, Nanyang	NLPにおけるdeep learningの手法を網羅的に解説したレビュー論文。紹介されている手法は、分散表現系（word2vecとその前身）、CNN系（Basic CNN, time-delay neural network, dynamic CNN, multi-clumn CNN, dynamic multi-pooling CNN, hybrid CNN-HMM)、RNN系（Simple RNN, LSTM, GRU, Dual-LSTM, MemNet）、Recursive neural network、 強化学習系、教師なし学習系（seq2seq）、生成モデル（VAEs, GANs）、メモリ増設系（memory networks, dynamic memory networks）など。
389	Simple Open Stance Classification for Rumour Analysis	https://arxiv.org/abs/1708.05286	Ahmet Aker, Leon Derczynski, Kalina Bontcheva	短い文書(Twitterなど)におけるスタンスの検知について。スタンスには様々なものがあるが、今回は嘘か真実かの検知としている。BoWやPOS、固有表現などベーシックなものから始まり特徴エンジニアリングを駆使しており(モデルは決定木)、LSTMベースのものと比較し優位な結果。
390	SSD: Single Shot MultiBox Detector	https://arxiv.org/abs/1512.02325	Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg	物体検出を行う研究で、あのYOLOより速く正確にできたとの報告。物体領域を直接予測するのでなく、あらかじめ定められた領域(default boxes)をどれだけ動かすかを予測するという方式。この領域はマルチスケールで用意し、予測のための特徴マップもそれと対応させ用意している。
391	Large-Scale User Modeling with Recurrent Neural Networks for Music Discovery on Multiple Time Scales	https://arxiv.org/abs/1708.06520	Cedric De Boom, Rohan Agrawal, Samantha Hansen, Esh Kumar, Romain Yon, Ching-Wei Chen, Thomas Demeester, Bart Dhoedt	Spotifyの関わる、楽曲推薦についての論文。プレイリストを文、プレイリストの中の楽曲を単語と見立て、プレイリストから楽曲の分散表現を作成。これをユーザーの再生履歴に沿いRNNで合成することでユーザーの嗜好表現を作成している(実際に再生した楽曲の分散表現を予測するように学習)。
392	BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain	https://arxiv.org/abs/1708.06520	Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg	DNNのモデルに対するハッキングについて、考えられるケースをまとめた研究。学習データへの介入は難しいと思うが、悪意ある学習済みモデルを利用させることで特定ケースのみ精度を下げるといったことが可能という報告(道路標識の特定箇所にシールが貼ってある場合のみ異なる標識に誤認させるなど)。
393	Revisiting the Centroid-based Method: A Strong Baseline for Multi-Document Summarization	https://arxiv.org/abs/1708.07690	Demian Gholipour Ghalandari	抜粋により文章を要約する研究。基本の手法は各文をTF-IDFで重みづけしたBoWで表現し、文章全体から計算した重心と近い文を選択する。これに文単体でなく作成した要約と重心の距離を比較すること、候補文を事前に選択する手法を組み合わせることで簡単な実装で高いRougeスコアを記録
394	Deep Learning for Video Game Playing	https://arxiv.org/abs/1708.07902	Niels Justesen, Philip Bontrager, Julian Togelius, Sebastian Risi	深層学習x強化学習でゲームを攻略する研究のまとめ。どんな手法がどんな種類のゲームに使われているかなどもまとめられている。
395	Adversarial Examples for Evaluating Reading Comprehension Systems	https://arxiv.org/abs/1708.07902	Robin Jia, Percy Liang	機械学習で質問回答を行う際に、モデルが本当に文章を理解したうえで回答しているかを検証する手法の提案。文章に対して回答に影響を与えないAdversarialな変更を行っても精度を維持できるか検証するのが主眼で、ただ文の言い換えは非常に高度なので「付けたし」に重きを置いている。
396	Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning	https://einstein.ai/static/images/layouts/research/seq2sql/seq2sql.pdf	Victor Zhong, Caiming Xiong, Richard Socher	自然言語からSQLを生成する研究。問合せとテーブルが与えられた時に、問合せ中の単語・テーブルのカラム・SQLで使用されるコマンド(SELECTやWHERE)らを組み合わせることで集計句・選択列・選択条件を生成しSQLを組み立てる。選択条件の学習には強化学習が利用されている。
397	Position-aware Attention and Supervised Data Improve Slot Filling	https://nlp.stanford.edu/pubs/zhang2017tacred.pdf	Yuhao Zhang, Victor Zhong, Danqi Chen,Gabor Angeli, Christopher D. Manning	文章から各オブジェクトの関係性を抽出する研究(AはBと友達、など)。主語と目的語の位置情報をネットワークに含めることで、例え位置が離れていても関係性を類推できるようにしている(ただ事前に主語/目的語のアノテーションが必要)。学習に使用したデータ(TACRED)は公開されるとのこと
398	On the Protection of Private Information in Machine Learning Systems: Two Recent Approaches	https://arxiv.org/abs/1708.08022	Martn Abadi, lfar Erlingsson, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Nicolas Papernot, Kunal Talwar, Li Zhang	センシティブな学習データをどう守るかという話。学習済みモデルから学習データが推定できたり、入力or出力を複数与え学習に使用されたデータか検証できてしまうことを予防する。対応として、ノイズを加えた学習(NoisySGD)と分割したデータで学習しアンサンブルする手法(PATE)を提案
399	Voice Synthesis for in-the-Wild Speakers via a Phonological Loop	https://arxiv.org/abs/1707.06588	Yaniv Taigman, Lior Wolf, Adam Polyak, Eliya Nachmani	RNNよりシンプルな構成で音声認識を行う研究。基本となるベクトルは、Attentionで作成したコンテキスト、話者ベクトル、前回入力、バッファをNNにかけて作成する。バッファは固定長のキュー形式で、入れると末端がでる形。
400	Neural Networks Regularization Through Invariant Features Learning	https://arxiv.org/abs/1709.01867	Soufiane Belharbi, Clement Chatelain, Romain Herault, Sebastien Adam	学習用のデータ数が少なくても、クラスごとに不偏的な特徴量抽出をするための正則化手法を提案。本手法では同一クラスのサンプルは似た特徴量を持っているというヒューリスティックを使って、中間表現に対して制約を課す。1ループで同一クラスの2つのサンプルを投入し、通常のロスとは別に2つの中間表現に対してコスト（距離）を計算。
401	Dynamic Filters in Graph Convolutional Networks	https://arxiv.org/abs/1706.05206	Nitika Verma, Edmond Boyer, Jakob Verbeek	local filteringアプローチによる動的なGCNを提案。Conv層のフィルタの考え方を再構成することで、不規則な構造に対するlocal graph convolutionを可能にした。エッジの重みはNNで求める。3D shape correspondanceタスクでSoTA。
402	Learning to Compose Domain-Specific Transformations for Data  Augmentation	https://arxiv.org/abs/1709.01643	Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hussain, Jared Dunnmon, Christopher R	augmentationをgenerative adversarial approachで行う事を提案。学習はlabel無しデータを投入したGANモデルの学習ステップと、学習済みのGeneratorを使用してデータ変換を施したデータを入力（識別モデルの前にGeneratorを通すだけ）とする通常の教師あり学習ステップに分かれる。
403	Squeeze-and-Excitation Networks	https://arxiv.org/abs/1709.01507	Jie Hu, Li Shen, Gang Sun	ILSVRC2017の画像分類タスクで一位を記録した論文。空間における関係性を明示的にモデルに組み込み成果が出たため、今度はチャンネル間の関係性を明示的に組み込んだという話。具体的にはチャンネル単位(1x1xC)で切り出し平均をとり、それを重み的に使い特徴マップを作成する。
404	Entity Linking via Joint Encoding of Types, Descriptions, and Context	http://cogcomp.org/papers/GuptaSiRo17.pdf	Nitish Gupta, Sameer Singh, Dan Roth	Entity Linking(Wikipediaで単語にリンクが張られるような機能)に関する研究。リンク対象の単語の前後をLSTMでエンコードしたもの(これは局所で、他に文全体のリンク対象の情報をマージ)、リンク先の辞書的な記載のエンコード、リンクの種別の計3つを統合して推定する
405	Deep Learning Techniques for Music Generation - A Survey	https://arxiv.org/abs/1709.01620	Jean-Pierre Briot, Gatan Hadjeres, Franois Pachet	音楽生成にDNNを使った研究のサーベイ、というか本(108ページ)。「音楽生成」と一口に言ってもメロディなのか伴奏なのか、生成結果の形式はMIDIなのかピアノロールなのかと様々なバリエーションがあるので、それらを区分けしつつ手法およびモデリングの仕方についてまとめた大作。
406	Inductive Representation Learning on Large Graphs	https://arxiv.org/abs/1706.02216	William L. Hamilton, Rex Ying, Jure Leskovec	高速かつ初見node/graphにロバストなGraph構造におけるnode embeddingアルゴリズムの提案。
407	Automatic Semantic Style Transfer using Deep Convolutional Neural Networks and Soft Masks	https://arxiv.org/abs/1708.09641v1	Huihuang Zhao, Paul L. Rosin, Yu-Kun Lai	StyleTransferを行う際に、各パーツに適したスタイルを適用するようにする研究。「各パーツ」は具体的には目や鼻といったもので、これをセグメンテーションするのにCNNに組み込めるCRF-RNNという手法を使用。これで作成したパーツごとのマスクと特徴マップを結合して使用する
408	Towards the Automatic Anime Characters Creation with Generative Adversarial Networks	https://arxiv.org/pdf/1708.05509.pdf	Yanghua Jin	アニメ顔画像特化したGANモデルを検討した。DRAGANの損失関数、生成器をResNetにし、データセットを選別し、学習率を変化させ、SRGANを使用した。高品質なモデルを作成できた。下記ウェブサイトからモデルを一般に公開した。
409	Training Deep AutoEncoders for Collaborative Filtering	https://arxiv.org/abs/1708.01715	Oleksii Kuchaiev, Boris Ginsburg	協調フィルタリングのタスクにAutoEncoderで取り組む話。具体的には、userのレーティング結果であるxを復元するように学習させる。これにより、ユーザーが新しく評価を行ったときにそれをネットワークに入れれば新しい評価を元にした他のアイテムに対する評価が得られる。
410	Massive Exploration of Neural Machine Translation Architectures	https://arxiv.org/abs/1703.03906	Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le	250,000時間にも及ぶGPUの酷使により得られた、機械翻訳におけるハイパーパラメーターの知見が公開されている。
411	A Discriminative Feature Learning Approach for Deep Face Recognition	http://ydwen.github.io/papers/WenECCV16.pdf	Yandong Wen, Kaipeng Zhang, Zhifeng Li, Yu Qiao	画像分類において、単純に分類確率(softmax)だけでなく、当該クラスの中心からの距離をlossに組み込もうという提案(中心は最初に計算してしまうのでなく、学習中ミニバッチにより随時更新されていく)。
412	Training RNNs as Fast as CNNs	http://ydwen.github.io/papers/WenECCV16.pdf	Tao Lei, Yu Zhang	RNNの計算を高速化する試み。ポイントとしては、ゲートの重みを更新する際に前回の隠れ層を使わずに済ませることで、並列で計算できるようにした点(下図赤線が時系列で並列に計算可能)。後は要素積＋和算なので高速に計算を行うことができる。これをSimple Recurrent Unit (SRU)と命名。
413	A Deep Reinforcement Learning Chatbot	https://arxiv.org/abs/1709.02349	Iulian V. Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary Ke, Sai Mudumba, Alexandre de Brebisson, Jose M. R. Sotelo, Dendi Suhubdy, Vincent Michalski, Alexandre Nguyen, Joelle Pineau, Yoshua Bengio	人間とある程度長い時間(20分ほど)対話できるボットを競うAmazon Alexa Prizeで、準決勝をトップクラスで通過したモデル(平均ターン数は14.5~16とかなり長い)。
416	Saliency Revisited: Analysis of Mouse Movements  versus Fixations	http://openaccess.thecvf.com/content_cvpr_2017/papers/Tavakoli_Saliency_Revisited_Analysis_CVPR_2017_paper.pdf	Hamed R. Tavakoli† Fawad Ahmed‡ Ali Borji‡	顕著性(画像のどこに眼を付けやすいか)の研究において、データセットの作成方法であるマウス追跡と視線追跡でどのような差異が発生するか比較を行った。
417	Embedded Binarized Neural Networks	https://arxiv.org/abs/1709.02260v1	Bradley McDanel, Surat Teerapittayanon, H.T. Kung	組込機器などの小メモリ環境でNNを動かす研究。Binary NNは重みを0/1で持つことで省メモリ化したが、中間の計算結果は浮動小数点でありこれがメモリを大きく食っている。そこで畳み込みをシリアルに行っていくことでこれを解消した。BNNをEmbedded向きにした、名付けてeBNN。
418	StarSpace: Embed All The Things!	https://arxiv.org/abs/1709.03856	Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, Jason Weston	分類にもランキングにもレコメンドにも、様々なタスクで汎用的に利用できる分散表現を作成した研究。手法自体はシンプルで、BoWを含む様々な離散表現について同じEntityか否かで類似度が適切になるよう学習する(類似度はコサインor内積)
419	Playing for Benchmarks	http://vladlen.info/papers/playing-for-benchmarks.pdf	Stephan R. Richter	25万枚以上の高解像度のゲーム動画に対して、各種手法を適用しベンチマークを提供。
420	TorontoCity: Seeing the World with a Million Eyes	https://arxiv.org/abs/1612.00423	Shenlong Wang, Min Bai, Gellert Mattyus, Hang Chu, Wenjie Luo, Bin Yang, Justin Liang, Joel Cheverie, Sanja Fidler, Raquel Urtasun	従来に無い多視点のデータセットを提案 : ドローン、航空写真、車載カメラ
421	Neural Optimizer Search with Reinforcement Learning	https://arxiv.org/pdf/1709.07417.pdf	Google  Brain.	強化学習の仕組みをOptimizerに適用し、従来のOptimizerであるAdamなどよりも良いパラメーター更新のルールを実現。
422	pix2code: Generating Code from a Graphical User Interface Screenshot	https://arxiv.org/abs/1705.07962	Tony Beltramelli	画面からコードを生成するという研究。直接コード、ではなく独自に定義したDSLを生成するようにしており、最終的なコードはDSLから生成する。モデルは画像はCNNで、LSTMはコンテキスト処理用と生成用(Decoder)で分けている。
423	The Loss Surface of Deep and Wide Neural Networks	https://arxiv.org/pdf/1704.08045.pdf	Quynh Nguyen	深層学習では最適化の空間が凸な関数ではないため、局所最適解しか求められません。この論文では局所最適解がほぼ大局的な最適解と同一であることを示しています。
424	Class-Splitting Generative Adversarial Networks	https://arxiv.org/abs/1709.07359	Guillermo L. Grinblat, Lucas C. Uzal, Pablo M. Granitto	GANを行う際にクラス情報を使うのは有効な手法だが(見分けがつかない＋クラスに分類されるようにする)、ラベルがない場合もある。そこでGANの学習中に得られる潜在表現からクラスタリングを行い、その情報をバイアスに組み込む手法を提案。
425	Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping	https://arxiv.org/abs/1709.07857	Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey Levine, Vincent Vanhoucke	物をつかむロボットの学習では、実際の画像を用意するのが大変。そこで、シミュレーター上の画像をGANで本物っぽくすることで学習データの量産を試みた研究。これにより必要なデータを1/50にできた他、GANの合成画像のみで約100万枚の実画像で学習した結果と同等の成果が出せることを確認
427	Generating Sentences by Editing Prototypes	https://arxiv.org/abs/1709.08878	Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, Percy Liang	文生成を行う際に、一から生成するのでなくて文テンプレートを編集する形で行うという研究。文テンプレートは実データからサンプリングしてEncodeし、編集ベクトルと併せて生成を行う。編集ベクトルは、文テンプレートから追加/削除された単語分散表現を結合したものとして表現される。
428	Dynamic Entity Representations in Neural Language Models	https://arxiv.org/abs/1708.00781	Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi, Noah A. Smith	言語モデルに、固有表現とその参照を組み込む研究(「スタバ」に行った、「そこ」では、など)。固有表現か否かのフラグ(当否・出現番号・残りの単語数)を導入し、生成時もこのフラグで分岐して生成を行っている。
429	Diversity driven attention model for query-based abstractive summarization	https://arxiv.org/abs/1704.08300	Preksha Nema; Mitesh M. Khapra; Anirban Laha; Balaraman Ravindran	クエリベースの生成型要約に向けてクエリ及びドキュメントに対してAttentionを使うEncoder-Decoderベースのモデルを考案した。
430	Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x)) Alternative	https://arxiv.org/abs/1708.01729	Zhiming Zhou(1), Weinan Zhang(1), Jun Wang(2)	Inception ScoreとMODE Scoreが透過であり，本来Mode Scoreで考慮されるべき事前分布を導入したAM Scoreを提案．また，log(1-D)にLabel Smoothingを導入してもSmoothingされたラベルへ最適化されることはなく，-log(D)の方がよい性質を持つことを示した．
431	Generating Sentences by Editing Prototypes	https://arxiv.org/pdf/1709.08878.pdf	26 Sep 2017	深層学習による言語生成をプロトタイプの生成空間と編集の空間に分けて生成の際に合わせて言語生成することで従来のモデルよりもパープレキシティを改善した。また人手評価でも高い評価を取得
432	Improving image generative models with human interactions	https://arxiv.org/pdf/1709.10459.pdf	2017.09.29	GANでは美しい画像を作成するのが困難だったため、人手のイテレーションを入れて改善を測った研究
433	Need for Speed: A Benchmark for Higher Frame Rate Object Tracking (ICCV2017)	https://arxiv.org/pdf/1703.05884.pdf	Hamed Kiani Galoogahi1	240FPSでの物体追跡ベンチマークを提案
434	The Numerics of GANs	https://arxiv.org/abs/1705.10461	Lars Mescheder, Sebastian Nowozin, Andreas Geiger	GANの学習は、GとDの間にナッシュ均衡を見つける問題と見立てられる。これを見つけるにはsimultaneousなGDが用いられるが、両者の勾配ベクトルが特定の固有値を持つ場合収束しない。そこで正則化項を導入しこれを防いでいる。
435	LSTM: A Search Space Odyssey	https://arxiv.org/abs/1503.04069	Klaus Greff, Rupesh Kumar Srivastava, Jan Koutnk, Bas R. Steunebrink, Jrgen Schmidhuber	LSTMに関する大規模な調査(CPU換算で15年分)。各ゲートの有り無しやpeepholeの有無など8つの変形と、隠れ層のサイズや学習率といったハイパーパラメーターを調査。音声認識、手書き文字認識、音楽学習の3つのタスクで検証し、結果LSTMを大きく上回るものはなかったという
436	Context Embedding Networks	https://arxiv.org/abs/1710.01691	California Institute of Technology, Pasadena, CA	クラウドソーシングでグリッド中に表示された複数の画像データをクラスタリングしてもらう際に、ワーカーによって乗ってしまうバイアスを考慮しつつデータのベクトル表現を得る試み。
437	Scene Graph Generation from Objects,  Phrases and Region Captions  (ICCV2017)	http://people.csail.mit.edu/bzhou/publication/ICCV_scenegraph.pdf	Yikang Li1	画像を説明するグラフを自動生成するDNN(MSDN)を提案し、Sotaを3%更新。
438	Generative Encoder-Decoder Models for Task-Oriented Spoken Dialog Systems with Chatting Capability	https://arxiv.org/abs/1706.08476	Tiancheng Zhao, Allen Lu, Kyusong Lee and Maxine Eskenazi	タスク指向の対話システムをencoder-decoderモデルで構築するためのフレームワークの提案。システム/ユーザ発話内の固有表現および知識ベースの結果を一旦形式化する。その後encoder-decoderモデルで発話生成して、元に戻す(語彙化する)というやり方。
439	Rainbow: Combining Improvements in Deep Reinforcement Learning	https://arxiv.org/pdf/1710.02298.pdf	Matteo Hessel/DeepMind	今まで出てきたDQNの手法を組み合わせて'Atari 2600 benchmark'でState of artsを達成
440	Neural Color Transfer between Images	https://arxiv.org/pdf/1710.00756.pdf	Mingming He/Hong Kong University of Science and Technology,	画像の間の色の変換をニューラル表現のマッチングに加えて局所空間の異なりと全体の一貫性を保つため、局所線形モデルを局所的な制約と全体の制約に用いた。
441	Automated Crowdturfing Attacks and Defenses in Online Review Systems	https://arxiv.org/pdf/1708.08151.pdf	Yuanshun Yao/University of Chicago	レビューシステムは下記のようにクラウドソーシングもしくは機械学習によって本来利用した人とは異なるレビューが出て来るなどの問題があった。この論文ではRNNとWordNetを利用した文章の改善を行った偽のレビュー文章に対して機械が作成した文章だと判断する手法を提案している。
442	Following Gaze in Video(ICCV2017)	http://carlvondrick.com/videogaze.pdf	Adria Recasens Carl Vondrick Aditya Khosla Antonio Torralba `	動画内において人物が見ている箇所は、多くの場合、同一のフレーム内に存在していない。そのため異なるフレームをまたいでどこを見ているかを推定できる手法を提案
443	Am I a Baller? Basketball Performance Assessment from First-Person Videos(ICCV2017)	https://arxiv.org/abs/1611.05365	Gedas Bertasius1	FPVを用いたバスケット選手の評価手法を提案
444	Neural Task Programming: Learning to Generalize Across Hierarchical Tasks	https://arxiv.org/abs/1710.01813	Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei, Silvio Savarese	複雑なタスクを、実行可能なステップの組み合わせにばらして実行する仕組みの提案。具体的にはタスク->ベクトル表現->サブプログラム/実行終了の予測->サブプログラムが実行可能なタスクなら引数を生成し実行、まだ崩せるなら実行可能な単位になるまで再帰呼び出し、という形で進行する。
445	Emergent Complexity via Multi-Agent Competition	https://arxiv.org/abs/1710.03748	Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, Igor Mordatch	シンプルな環境であっても、マルチエージェントである場合は複雑な行動をとるようになるとの研究。マルチエージェントの学習にはWorkerを使った分散学習(中身はPPO)、基礎動作を学ばせるための初期報酬の付与、対戦エージェントのサンプリングといった3つの工夫が取られている。
446	Monotonic Calibrated Interpolated Look-Up Tables	http://jmlr.org/papers/v17/15-243.html	Maya Gupta, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojciech Moczydlowski, Alexander van Esbroeck	モデルが予測不能な動きをしないように、単調な傾向になるような補完を行うという手法。学習データが足りなくて過適合しそうな場合には、単調傾向を持つ補完空間からデータを補完することで、過適合を防ぎつつ予想外の予測をしないようにする。
447	RoomNet: End-to-End Room Layout Estimation  (ICCV2017)	https://arxiv.org/abs/1703.06241	Chen-Yu Lee Vijay Badrinarayanan Tomasz Malisiewicz Andrew Rabinovich	end-to-endで部屋のレイアウトを推定するCNNを提案。
448	DeepXplore: Automated Whitebox Testing of Deep Learning Systems	https://arxiv.org/abs/1705.06640	Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana	DeepLearning(DL)Systemに誤った振る舞いを起こさせるような画像を自動的に生成し、ラベル付けができるフレームワークを提案している（論文ではDeepXploreと呼ぶ）。DeepXploreは誤認識画像を生成し、既存のモデルを再度学習させ精度を向上させることに成功している。
449	Neural Person Search Machines(ICCV2017)	https://arxiv.org/abs/1707.06777	Hao Liu 1	Conv-LSTMを用いて現実世界での人物検索手法を提案
450	Multi-Document Summarization using Distributed Bag-of-Words Model	https://arxiv.org/abs/1710.02745	Kaustubh Mani	Distributed Bag-of-Words(PV-DBOW)を使った複数文書要約を行うモデル。
451	Detect to Track and Track to Detect	https://arxiv.org/pdf/1710.03958v1.pdf	Christoph Feichtenhofer/Graz University of Technology	検出とトラッキングの手法が年々、精度向上のために複雑になっていきているため、それを抑えるため畳み込みニューラルネットワークのアーキテクチャに置き換えて単純化と精度向上を達成した論文
452	TextRank: Bringing Order into Texts	https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf	Rada Mihalcea and Paul Tarau	Googleの編み出したWebページを評価するPageRankの仕組みを、要約に応用するという研究。PageRankはページ間のリンクを元にそのスコア(実質的にはページ閲覧者がページに滞在する確率)を算出するが、ページを「単語」や「文」に置き換えることで、文章中において重要な単語や文を抽出しようという手法。
454	The Automatic Creation of Literature Abstracts	http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf	HP Luhn	出現頻度を元に算出した重要語が、文中にどれぐらい出てくるかで重要文を決定するという試み。重要語をベースとしたスコアは文を一定範囲(bracket)に区切った単位で算出を行う。文からとれるいくつかのbracketのうち、値が最大のものを文のスコアとする。
455	LexRank: Graph-based Lexical Centrality as Salience in Text Summarization	https://www.aaai.org/Papers/JAIR/Vol22/JAIR-2214.pdf	Gne Erkan, Dragomir R. Radev	文をノード、文類似度をエッジとしてグラフを構築し、PageRankと同等の手法で要約を作成するという手法。
456	Text summarization using Latent Semantic Analysis	https://www.researchgate.net/publication/220195824_Text_summarization_using_Latent_Semantic_Analysis	Makbule Gulcin Ozsoy, Ferda Nur Alpaslan, Ilyas Cicekli	LSAベースの要約に関する研究のまとめ。LSAベースとは、端的には文章のトピックを算出し、各文をそのトピックへの関連度でスコアリングして選択するという手法。簡易な手法としては行に文、列に単語/フレーズをとった行列をSVDにかけて固有ベクトル(主成分)を算出し、第一主成分からスコアが高い順に文を抜いていけば要約を作成できる。もちろん、SVDの結果得られる値の評価方法(=文のスコアリング方法)には様々な種類があり、本論文中で紹介されている。
457	PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents	http://aclweb.org/anthology/P/P17/P17-1102.pdf	Corina Florescu and Cornelia Caragea	論文からキーフレーズを抽出する教師なしの手法。
458	Graph Convolutional Networks for Classification with a Structured Label Space	https://arxiv.org/abs/1710.04908	Meihao Chen, Zhuoru Lin, Kyunghyun Cho	通常の多クラス問題はラベル同士の階層構造を気にしていないが、ImageNetはもちろん文書分類はなおさら階層構造がある。そこで、入力とラベル双方をEncodeしてグラフ畳み込みを行うことで階層構造の予測を行わせようという試み。
459	Selective Encoding for Abstractive Sentence Summarization	https://arxiv.org/abs/1704.07073	Qingyu Zhou; Nan Yang; Furu Wei; Ming Zhou	生成型要約を行うためにseq2seqを拡張したモデル。
460	DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations	http://goldberg.berkeley.edu/pubs/DDCO-CoRL17-camera-ready.pdf	Sanjay Krishnan*, Roy Fox*, Ion Stoica, Ken Goldberg	お手本から、長い行動をどう学習させるかについての解説。長い行動はいくつかの短い行動の組み合わせで表現できる(積む＝つかむ＋のせる、など)。そこで、お手本の中から所作の切り替わりを推定し(TSC)、推定区間における報酬関数を逆強化学習で推定する(SWIRL)という手法をとっている。
461	Dilated Recurrent Neural Networks	https://arxiv.org/abs/1710.02224	Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark Hasegawa-Johnson, Thomas Huang	RNNでskip connectionを行う際に、間隔をあけて接続する(前回からの接続は受けない)＋レイヤーを重ねる際に上位のレイヤほど開ける間隔を長くするという提案。間が開くことでノードが節約できるため、メモリ的にも効率が良い。検証はMNISTとPTBで上位の精度。
462	DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars	https://arxiv.org/abs/1708.08559	Yuchi Tian, Kexin Pei, Suman Jana, Baishakhi Ray	自動運転車のテストを自動化させるために、天候や気候変化などの現実で起こりうる状況を加味したテストデータを自動生成できるツールを提案している。(論文ではDeepTestと呼ぶ)
463	Swish: a Self-Gated Activation Function	https://arxiv.org/abs/1710.05941	Prajit Ramachandran, Barret Zoph, Quoc V. Le	ReLUを超えるシンプルかつ強力な活性関数を発明すべく生み出されたSwishの紹介。その式はf(x) = x・σ(x)というこの上なく単純なもので、論文の名の通りSelf-Gatedな形となっている。画像認識、翻訳の各モデル・各データセットで検証しReLUに全勝したという。
464	A Survey Of Cross-lingual Word Embedding Models	https://arxiv.org/abs/1706.04902	Sebastian Ruder, Ivan Vuli, Anders Sgaard	Cross-lingualな分散表現を作成する手法についてのサーベイ。各手法の類型化を行うだけではなくて、その評価方法とクロス(2言語)をこえてマルチに拡張する方法についても書かれている。
465	Unsupervised Sentence Representations as Word Information Series: Revisiting TF--IDF	https://arxiv.org/abs/1710.06524	Ignacio Arroyo-Fernndez, Carlos-Francisco Mndez-Cruz, Gerardo Sierra, Juan-Manuel Torres-Moreno, Grigori Sidorov	分散表現をTF-IDFで重みづけして利用するという話。Word2Vec、FastText、GloVeといった分散表現について何次元のものを使うか、TF-IDFをどこから算出するか、平均するか合計するかといったバリエーションについて文間類似度のタスクで検証。他手法より高精度を達成。
466	SLING: A framework for frame semantic parsing	https://arxiv.org/abs/1710.07032	Michael Ringgaard, Rahul Gupta, Fernando C. N. Pereira	文を入力に、その構造をJSONを拡張したスキーマで表現するという手法。このスキーマには、文に含まれる単語、その固有表現、単語間の関連が含まれる。手法はSeq2Seqベースで、単語＋接頭/尾語・大文字小文字といった形情報を使用している
468	Tracking the Dynamics in Crowdfunding (KDD2017)	http://home.ustc.edu.cn/%7Ezhhk/DataSets.html	Hongke Zhao, Hefu Zhang	クラウドファンディングで将来的にどれくらいのプロジェクトが立ち上げられ、どれくらいの資金を獲得するか予測する研究。
469	Revisiting Unreasonable Effectiveness of Data in Deep Learning Era	https://arxiv.org/abs/1707.02968	Chen Sun(Google Research), Abhinav Shrivastava(Google Research, Carnegie Mellon University), Saurabh Singh(Google Research), and Abhinav Gupta(Google Research, Carnegie Mellon University)	3億枚の画像を学習して、データ量とパフォーマンスの関係を解明した。データ量に対数的に比例してパフォーマンスが向上する。
470	Generative Adversarial Networks: An Overview	https://arxiv.org/abs/1710.07035	Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, Anil A Bharath	とてもよくまとまったGANの解説。仕組みの解説からそのバリエーション、応用例までがカバーされている。
471	Interpretable Predictions of Tree-based Ensembles via Actionable Feature Tweaking	https://arxiv.org/abs/1706.06691	Gabriele Tolomei, Fabrizio Silvestri, Andrew Haines, Mounia Lalmas	予測結果がNGだったとき、ではどうすればOKに持って行けるのか？に答えを出すことを試みた論文。決定木を用いたアンサンブル学習をベースとした手法で、各決定木においてNGなxをOKに持って行くための変更量を算出し、その中から最小の変更量を求めることで最小コストでOKにする変更を求める
472	One pixel attack for fooling deep neural networks	https://arxiv.org/abs/1710.08864	Jiawei Su, Danilo Vasconcellos Vargas, Sakurai Kouichi	数ピクセルの編集で画像分類のモデルをだますという研究。1ピクセルでも70%程度の確率で誤識別させることができ、さらに20%程度で特定のクラスに識別させることも可能(5ピクセルならもっと上がる)。元の画像に対するピクセルの変動量は、GE(差分進化法)により最小の値を求めている。
473	Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning	https://arxiv.org/abs/1602.05765	Shoaib Jameel, Steven Schockaert	同じ意味を持つ単語をまとめる意味空間の埋め込み表現を作る研究。空間内の方向・距離に意味があるようにも設計している(国空間なら、人口の大小を表す軸がある的な)。GloVeをベースに、同意味空間の距離は短く、トリプルを持つ場合維持されるよう制約をかけている。アナロジーで顕著な効果。
474	Learning how to explain neural networks: PatternNet and PatternAttribution	https://arxiv.org/abs/1705.05598	Pieter-Jan Kindermans, Kristof T. Schtt, Maximilian Alber, Klaus-Robert Mller, Dumitru Erhan, Been Kim, Sven Dhne	DNNの判断根拠を理解するための新しい手法の提案。ネットワークの重みの役割は入力内の余分な情報(d)をフィルタし出力yに寄与する部分(s)を取り出すフィルタであると定義。この寄与部分sをxからどれだけ取り出せるかを計測する新しい評価指標を定義すると共に、既存手法の改良を行っている
475	Progressive Growing of GANs for Improved Quality, Stability, and Variation	http://research.nvidia.com/publication/2017-10_Progressive-Growing-of	Tero Karras	GANによる画像生成で、高解像度の生成を実現する新しい学習方法を提案。最初は低解像度・少ないレイヤで学習していき、徐々に高解像度・深いネットワークにしていく。
477	mixup: Beyond Empirical Risk Minimization	https://arxiv.org/abs/1710.09412v1	Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz	モデルの汎化性能を上げるためにData Augmentationが用いられることがあるが、これは同じクラス内の水増しにとどまりドメイン知識も必要になる(画像の左右反転や回転など)。
478	Understanding Hidden Memories of Recurrent Neural Networks	https://arxiv.org/abs/1710.10777	Yao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, Huamin Qu	RNNの可視化を行うツールであるRNNVisで用いられている手法の解説。入力された単語に対する隠れ層の反応、またタイムステップごとにどの隠れ層がどの単語表現によっているかを可視化している。
479	One-shot and few-shot learning of word embeddings	https://arxiv.org/abs/1710.10280	Andrew K. Lampinen, James L. McClelland	未知語の分散表現をOne/Few shotで学習する試み。基本的には、未知語用の重み以外をフリーズして学習する。PTBで出現頻度の低い語を含む文をいくつか選び、それ以外の文で学習した後に予測に挑戦。結果としてはCentroidで初期化してOutputの埋込表現を学習すると効果大
480	On Pre-Trained Image Features and Synthetic Images for Deep Learning	https://arxiv.org/abs/1710.10710	Stefan Hinterstoisser, Vincent Lepetit, Paul Wohlhart, Kurt Konolige	物体検知のモデルで合成データを学習に使う方法の提案。実物で学習された事前学習済みの重みを固定した上で、合成データのみで学習すると実物のみで学習と同等の結果が得られたという(合成のみでゼロから学習すると低精度)。なお、合成データは背景画像に3Dモデルを置くという手法で作成している。
481	TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning	https://arxiv.org/abs/1710.11417	Gregory Farquhar, Tim Rocktschel, Maximilian Igl, Shimon Whiteson	強化学習で行動を選択する際、行動を取った場合のシミュレーションを行ってから選択するようにするという試み。このためモデル内に状態遷移とそこでの報酬を予測するネットワークを組み込み、一定ステップまで再帰的に行動の試行を行い、価値関数でそれらを評価している。
482	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	https://arxiv.org/abs/1711.00155	Pavlos Vougiouklis, Hady Elsahar, Lucie-Aime Kaffee, Christoph Gravier, Frederique Laforest, Jonathon Hare, Elena Simperl	トリプルで表された知識を元に説明文を生成する試み(「東京」-[首都]->「日本」という関係を「東京は日本の首都です」とするなど)。モデルはオーソドックスなEncoder-Decoderモデルだが、各知識には系列的な相関はないのでEncode結果はコンカチしている。
483	Extractive Summarization Using Multi-Task Learning with Document Classification	http://aclweb.org/anthology/D17-1222	The University of Tokyo	抽出型要約と文書分類をマルチタスク学習で同時に学習を行うモデル。
484	How Transferable are Neural Networks in NLP Applications?	https://arxiv.org/abs/1603.06111	Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin	NLPタスクにおける転移学習とマルチタスク学習に関する研究。
485	Google Vizier: A Service for Black-Box Optimization (KDD2017)	https://research.google.com/pubs/pub46180.html	Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, D. Sculley	Googleが採用しているブラックボックス最適化手法についての論文。
486	TFX: A TensorFlow-Based Production-Scale Machine Learning Platform (KDD2017)	http://stevenwhang.com/tfx_paper.pdf	Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque,	TensorFlowを基盤にしたプロダクション環境でスケーリングするプラットフォーム、TFXを提案。
487	DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks (ICCV2017)	http://www.vision.ee.ethz.ch/~timofter/publications/Ignatov-ICCV-2017.pdf	Andrey Ignatov1	6000枚のDSLRと低スペックの写真をセットにしたデータセットも公開
488	Dynamic Routing Between Capsules	https://arxiv.org/abs/1710.09829	Sara Sabour, Nicholas Frosst, Geoffrey E Hinton	入力されたデータの特徴を捉えるため、個々の特徴を検出するニューロンをまとめた「カプセル」を利用するという提案。なお、通常のネットワークと異なりカプセル内の各ニューロンへの入力は値ではなくベクトルであり、その出力もベクトルとなっている。このベクトルは重みだけでなくほかのニューロンとのカップリング係数が乗じられ、この係数は入力/出力の類似度に基づき調整される。
489	How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions	https://arxiv.org/abs/1609.00070	Arun Tejasvi Chaganty, Percy Liang	数値表現を「たとえ」に変換するという研究。具体的には、x平方キロメートルという表現を東京ドームy個分、といった表現に直すといったこと。数値表現の知識ベースを作り(東京ドーム: 4.7haなど)そこから数値表現に該当する式(xがy個分)を導き、その式をRNNを用いて自然な言葉に変換している。
490	Neural Discrete Representation Learning	https://arxiv.org/abs/1711.00937	Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu	VAEを利用して離散の潜在表現を学習する試み。Encoderの出力に近いベクトルを埋め込み空間から検索し、そこからDecoderで復元する。分布はEncoderの出力に近いところに1、それ以外が0であるone-hotな分布として定義される。これにより離散表現を獲得できるだけでなく、Decoderが強力すぎる場合に潜在表現が学習されない問題を克服している。
492	Predicting Behaviors of Basketball Players from First Person Videos(CVPR2017)	http://openaccess.thecvf.com/content_cvpr_2017/papers/Su_Predicting_Behaviors_of_CVPR_2017_paper.pdf	Shan Su1	FPV(一人称視点のカメラ)の動画からバスケット選手の軌道と注視される空間を予測した。
493	Non-Autoregressive Neural Machine Translation	https://einstein.ai/static/images/pages/research/non-autoregressive-neural-mt.pdf	Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li& Richard Socher	翻訳文の単語を順番にではなく並列して出力できる機械翻訳モデル。
494	Synthesizing Robust Adversarial Examples	https://arxiv.org/abs/1707.07397	Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok	機械学習モデルをだます「実物」(3Dモデル)の作成を試みた研究。通常のAdversarial Exampleはちょっとした変更(回転や拡大縮小)に弱いが、これに対応するため、そうした変更に相当する様々な変換をかけた上でも誤認識させられるよう訓練を行う。
495	Compressing Word Embeddings via Deep Compositional Code Learning	https://arxiv.org/abs/1711.01068	Raphael Shu, Hideki Nakayama	単語埋め込み行列の圧縮について。例えばdogとdogsで別々にとっているのはばからしいので、埋め込み行列を一つではなく複数の行列(=コードブック)の組み合わせで表現することを提案。さらに、これをAutoEncoderのような形式で既存の埋め込み行列から自動的に学習する。元の精度を維持し94%以上の圧縮を達成。
496	The (Un)reliability of saliency methods	https://arxiv.org/abs/1711.00867	Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schtt, Sven Dhne, Dumitru Erhan, Been Kim	DNNの判断根拠を出力する手法を、ネットワークの重みは同じだが扱う入力は異なる(一方は通常の入力で片方は反転させた入力を受け取るが、出力は同じで重みも同じ)ネットワークで説明が異なるか評価(入力不変性)。結果、出力への貢献度を算出して伝搬するスタイルの手法(LRPなどの)がNGだった。
497	A Hybrid DSP/Deep Learning Approach to Real-Time Full-Band Speech Enhancement	https://arxiv.org/abs/1709.08243	Jean-Marc Valin	計算資源が限られている中でも音声ノイズを除去しようという試み。普通にEnd-To-Endでやるとパラメーター数が多くなってしまうので、帯域ごと(22band)に音声の強調/減衰を推定し、それを適用するという形でモデルを組んでいる(音声区間検出(VAD)と同時学習させている)。
498	Anormaly Detection With Encoder Decoder	https://arxiv.org/abs/1607.00148?context=cs	Pankaj Malhotra, Anusha Ramakrishnan, Gaurangi Anand, Lovekesh Vig, Puneet Agarwal, Gautam Shroff	Encoder Decoderを利用して異常値検知を行う
499	Convolutional 2D Knowledge Graph Embeddings	https://arxiv.org/abs/1707.01476	Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel	CNNで知識グラフのリンクを予測する(エンティティと関連から、関連先エンティティを予測する)という試み。適用自体はストレートで、それぞれベクトル化してCNNで扱いやすいよう2次元にreshpeして畳み込み、潜在表現にしたのち候補の全エンティティとの比較を同時に行う
500	Unsupervised Learning of Important Objects from First-Person Videos (ICCV2017)	http://openaccess.thecvf.com/content_ICCV_2017/papers/Bertasius_Unsupervised_Learning_of_ICCV_2017_paper.pdf	Gedas Bertasius1	教師無しでFPVにおける重要な物体を検出するVisual-Spatial Network を提案し、比較実験では従来の教師あり手法と同等もしくは優れた結果を達成
502	Initializing Convolutional Filters with Semantic Features for Text Classification	http://www.aclweb.org/anthology/D17-1201	Shen Li1,2	CNNによる文書分類において、畳み込み層の初期化に意味特徴を用いる提案。N-gramによって得られた特徴をK-meansで分割し、各クラスタのcentroid vectorをconv filterの初期値とする。ランダムに設定するより精度がよく、文書分類の他にに感情分析やトピック分類にも活用出来る。
504	YOLO9000: Better, Faster, Stronger	https://arxiv.org/pdf/1612.08242.pdf	Joseph Redmon/ University of Washington	物体検出における精度向上、高速化、物体クラス数の著しい増加を達成した方法
505	Controllable Abstractive Summarization	https://arxiv.org/abs/1711.05217	Angela Fan, David Grangier, Michael Auli	要約生成において出来上がる文章をある程度コントロールできるようにしたモデル。
506	DLPaper2Code: Auto-generation of Code from Deep Learning Research Papers	https://arxiv.org/abs/1711.03543	Akshay Sethi, Anush Sankaran, Naveen Panwar, Shreya Khare, Senthil Mani	DNN関係の論文に掲載されている図や表から、Keras/Caffeのコードを生成するという夢の研究。図表がまずDNNのモデルに関するものかどうか、関するものであればOCR等を使用して情報を抽出し、レイヤー定義が記載されたJSONにフォーマットする。そこからコードを生成するという流れ。初期段階の図表がモデルに関するものかどうかの特定はうまくいっている模様。
507	The Lifted Matrix-Space Model for Semantic Composition	https://arxiv.org/abs/1711.03602	WooJin Chung, Samuel R. Bowman	ツリー構造を扱うモデルの提案。再帰的なモデル(TreeRNN)の要素と、語間の関係をよく表せる行列積の要素を組み合わせている。語を行列で表現するとパラメーターが多くなってしまう問題を、事前学習済みベクトルをサイズが等価な行列(=長さが次元の平方根)に変換する処理(LIFT)を挟むことで解消している
508	Breaking the Softmax Bottleneck: A High-Rank RNN Language Model	https://arxiv.org/abs/1711.03953	Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen	言語モデルにおいて、次の単語の出現確率は文脈の潜在表現h x 単語埋め込みベクトルwの積に比例し、表現力はこのサイズに抑えられてしまうと証明。そこで潜在表現に行列をかけてシンプルに複数個に拡張し、それらから出現確率を計算することで問題点を克服した(=混合分布)
509	Dataset Augmentation in Feature Space	https://arxiv.org/abs/1702.05538	Terrance DeVries, Graham W. Taylor	Data Augmentationを潜在空間上で行おうという話(これができれば、自然言語にも適用できる、かもしれない)。単にノイズを入れるだけだと元のラベルから外れてしまう可能性があるので、同じクラス/クラスタのサンプルの方向にちょっと寄せる、という形で変更を加えている。
510	Fixing Weight Decay Regularization in Adam	https://arxiv.org/abs/1711.05101	Ilya Loshchilov, Frank Hutter	AdamがSGDに比べて汎化性能が低い原因の検証。一般的な実装だとL2正則=Weight Decayとして扱われており、これは単純なSGDの場合そうなるがAdamのように学習率を自動調整する場合意図したWDの値にならない。これは学習率への依存が生じているためで、これを解消すると結果良好
511	Hybrid approach of relation network and localized graph convolutional filtering for breast cancer subtype classification	https://arxiv.org/abs/1711.05859	Sungmin Rhee, Seokjun Seo, Sun Kim	バイオインフォマティクスにおいて、複雑な病気のメカニズムの理解、特にガンに対して、ネットワークを用いた研究はとても有用であった。しかし、ネットワークバイオロジーは深い知識を要求しており、現在の限られた知識では本質的な部分に着手することが難しい。
512	Emotion Recognition in Context (CVPR2017)	http://openaccess.thecvf.com/content_cvpr_2017/papers/Kosti_Emotion_Recognition_in_CVPR_2017_paper.pdf	Ronak Kosti	文脈を考慮した26つの感情推定をCNNを用いて行い、感情推定のためのデータセットEMOTICも公開した。
513	Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks	https://arxiv.org/abs/1703.03400	Chelsea Finn, Pieter Abbeel, Sergey Levine	クラス分類・回帰・強化学習といった様々なドメインに適応可能なメタ学習手法MAMLの論文。論文では新たなタスクを学習する際に最も効果的な勾配を生成するようなメタ学習モデルを提示している。クラス分類に適応した結果で、従来のmemory-augmentedな手法などよりも良い結果が出ている。
514	Deep Visual Foresight for Planning Robot Motion	https://arxiv.org/abs/1610.00696	Chelsea Finn, Sergey Levine	ロボットが物体を押したときの予測画像を学習させ、そのモデルを用いてMPC(モデル予測制御)によって軌道生成を行うモデルベース強化学習の論文。モデルは入力がロボットの状態と物体を押すベクトルおよび現在画像で、出力が予測画像となっている。MPCを用いてロボットは任意の場所に物体を移動させる動作を生成できる。
515	Graph Attention Networks	https://arxiv.org/abs/1710.10903	Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li, Yoshua Bengio	グラフの畳み込みを行う際にAttentionを導入したもの。各ノードを表すベクトルに対し共通重みWをかけて処理し、「ノードAにとってノードBがどれくらい重要か」を計算するために計算結果のベクトルをコンカチしてAttentionを計算する(計算は隣接ノード分のみ対象)。
516	Representation Learning by Learning to Count (ICCV2017)	http://openaccess.thecvf.com/content_ICCV_2017/papers/Noroozi_Representation_Learning_by_ICCV_2017_paper.pdf	Mehdi Noroozi1 Hamed Pirsiavash2 Paolo Favaro1	画像内の基本要素のカウンティングを行う教師なし手法を提案。
517	What Is Around The Camera? (ICCV2017)	http://openaccess.thecvf.com/content_ICCV_2017/papers/Georgoulis_What_Is_Around_ICCV_2017_paper.pdf	Stamatios Georgoulis	画像を撮影時に、前景物体などに写り込んだ情報からカメラの周りの画像を再構築する研究
518	Simple Nearest Neighbor Policy Method for Continuous Control Tasks	https://openreview.net/forum?id=ByL48G-AW	?(レビュー中)	強化学習の難しさは、タスク自体の難しさと実際の経験をポリシー(多くの場合パラメーターの集合)に圧縮する難しさの2つの側面がある。そこで、ポリシーの関数を定義することなく、純粋に蓄積済みの(成功)経験から現在の状況に近いものをサンプリングする手法を提案。これで攻略困難環境を1つを除き攻略
519	Neural 3D Mesh Renderer	https://arxiv.org/abs/1711.07566	Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada	2Dの画像を3Dメッシュ(頂点と面で3Dを構築した画像)に変換する研究。2D上のピクセルと3D上の頂点/面間で差分(=勾配)を定義する=微分可能なレンダリングを定義するアプローチをとっている。ただ頂点の移動に対する色の変化は離散的(面に入ると唐突に色が変わる)ので、境界を徐々に変化させている。
520	Style Transfer in Text: Exploration and Evaluation	https://arxiv.org/abs/1711.06861	Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, Rui Yan	自然言語でも転移学習を行う試み。ニュース/論文のタイトル、ネガポジの2つの転移について検証。モデルはマルチDecoderと、スタイルを個別に学習させるようにしたものの2つ。何れもEncoderの出力がスタイル情報を持たないようチェックをかけ、Decoder/スタイル行列に集約されるようにしている
522	Multi-scale Convolutional Neural Networks for Crowd Counting	https://arxiv.org/abs/1702.02359	Lingke Zeng, Xiangmin Xu, Bolun Cai, Suo Qiu, Tong Zhang	人の集まりを認識する際、既存のボックスをスライドしていく方式だと人の密集度が高いためうまく検出できない。そこで、マルチスケールのCNN(サイズの異なる複数のフィルタで畳み込み)で顔の位置(ポイント)を予測するという手法を使用。
523	One-Shot Imitation Learning	https://arxiv.org/abs/1703.07326	Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, Wojciech Zaremba	模倣学習において人の1回のデモで目的のタスクを教示する論文。色のついたブロックを指定通りに積み重ねるというタスクにおいて、予め様々な積み重ね方のタスクを学習させ、未知な積み重ね方に対して1度人がデモを行い、現在状態とデモを入力として行動を生成するネットワークを用いて動作生成を行っている。
524	Parallel WaveNet: Fast High-Fidelity Speech Synthesis	https://arxiv.org/abs/1703.07326	Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov and Demis Hassabis	WaveNetは精度は良いものの、生成は逐次的(過去の自身の生成結果を利用する)ためとても生成に時間がかかるという問題があった。そこで、IAF(Inverse Autoregressive Flow)という再帰的な実行で分布近似を行うようなモデルを利用し、(自身の生成結果でなく)ノイズから徐々にあるべき音の分布へと近づけていき、最終的に訓練されたWaveNetの分布と近くしく成るように学習させるという手法をとっている。
525	Reverse Curriculum Generation for Reinforcement Learning	https://arxiv.org/abs/1707.05300	Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel	迷路や鍵の差し込みのようなゴールまで到達しないと報酬を得られないようなタスクにおいて、スタート地点を学習の進みに応じて変化させることで効率的に強化学習を行う方法を提案。最初はゴール付近から開始し、付近の状態を探索しながら、学習時にとある範囲の報酬の総和が得られた状態のみを残していくことで、調度良い難しさのスタート地点から学習できるようにしている。
526	Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora	https://arxiv.org/abs/1606.02820	William L. Hamilton, Kevin Clark, Jure Leskovec, Dan Jurafsky	「かわいがり」は一般的にはポジティブだが相撲ではNGなど、ドメインによって極性が変ることは多い。そこで特定ドメインにおける極性を教師なしで得る試み。分散表現における近さを基にグラフを構築し、ドメイン内の極性をランダムウォークで伝搬させる手法をとっている。
527	SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods	https://arxiv.org/abs/1610.03771	Marzieh Saeidi, Guillaume Bouchard, Maria Liakata, Sebastian Riedel	観点を含めたセンチメントのデータセットの作成と、それに対するベースラインとなるモデルでの精度の検証。「x地区の家賃は安い」ならば「x地区」「家賃」「Negative」というように、対象・属性・センチメントの3点がアノテーションされている(一文には最大2つの対象が入る)。モデルでは、属性ごとにモデルを作り対象に関する極性をうまく推定できるかを検証している。
529	KeystoneML: Optimizing Pipelines for Large-Scale Advanced Analytics	https://arxiv.org/abs/1610.09451	Evan R. Sparks, Shivaram Venkataraman, Tomer Kaftan, Michael Franklin, Benjamin Recht	Apache Sparkにて動く。
530	EX2: Exploration with Exemplar Models for Deep Reinforcement Learning	https://arxiv.org/abs/1703.01260	Justin Fu, John D. Co-Reyes, Sergey Levine	強化学習において効率的に状態を探索するための新しい手法としてEX2という手法を提案している。特定の状態を識別するDiscriminatorのアンサンブルを訓練することによって状態の訪問密度を近似し、まれに訪問した状態に報酬ボーナスを割り当てることで効率的な探索を実現する。
531	Population Based Training of Neural Networks	https://arxiv.org/abs/1711.09846	Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, Koray Kavukcuoglu	ハイパーパラメーターの探索を効率的に行う手法の提案。今まではランダムに設定した値をパラレルに走らせるか、ちょっと変えて学習をシリアルに繰り返すかだったが、この2つを合わせる。具体的には最初はパラレル、一定ステップ後に優秀なモデルの重みを採用し、そこから再度パラレルに探索を繰り返す
532	What Can Help Pedestrian Detection? (CVPR2017)	http://openaccess.thecvf.com/content_cvpr_2017/papers/Mao_What_Can_Help_CVPR_2017_paper.pdf	Jiayuan Mao†	HyperLearnerと呼ばれるCNNで得られた複数の特徴量(オプティカルフロー、深度特徴、物体検出、etc.)を統合して歩行者検出でSOTAを達成。Baselineの手法と比較しても速度の低下も防げている。
533	Improving Palliative Care with Deep Learning	https://arxiv.org/abs/1711.06402	Anand Avati, Kenneth Jung, Stephanie Harman, Lance Downing, Andrew Ng, Nigam H. Shah	最後に死ぬときは自宅で、と願う人は多いが(80%)実際は20%程度しかその願いを叶えられず、60%は救急搬送されて最後まで病院で治療を受けている。
534	Distilling a Neural Network Into a Soft Decision Tree	https://arxiv.org/abs/1711.09784	Nicholas Frosst, Geoffrey Hinton	ニューラルネットの解釈性を上げるために、レイヤーをノードと見立てた決定木を作って、分類過程をわかりやすくするという試み。Activationの結果をもとに木をたどっていき、最終的にラベル数分のLeafの出力でSoftmaxをとる形。分類精度は、MNISTで多層よりはよくCNNよりは低いといった中間の精度。
535	SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability	https://arxiv.org/abs/1706.05806	Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein	2つのDNNの類似性を調べる試み。同じレイヤー内のノードについて、全入力に対する出力を計測し、出力結果をSVDにかけて固有ベクトルを抽出、それらの相関をカノニカル相関でとるという形で出力の類似性を比較する。これにより比較を行えるほか、学習前後の層を比較することで学習状況を可視化できる。
536	Are GANs Created Equal? A Large-Scale Study	https://arxiv.org/abs/1711.10337	Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet	これまでいろいろなGANが編み出されてきたが、それらの結果にほとんど差異はないという調査結果。どのGANも、ハイパーパラメーターの調整とrandom restartでほぼ同等の結果を得ることができるという。
537	Neural Text Generation: A Practical Guide	https://arxiv.org/abs/1711.09534	Ziang Xie	Encoder-Decoderモデルについて、その概要と学習、前処理、また評価を行う際の注意点をまとめた文書。
538	Learning to Segment Every Thing	https://arxiv.org/abs/1711.10370	Ronghang Hu, Piotr Dollr, Kaiming He, Trevor Darrell, Ross Girshick	セグメンテーションの検知は学習データを用意するのが大変なので、バウンディングボックスで学習したパラメーターを変換する形で転移学習を行うという手法を提案。バウンディングボックスは3000クラスのVisual Genomeから、セグメンテーションはMS COCOの80クラスから学習し、結果セグメンテーションできるクラスを大幅に増やしている。
539	Model-based Reinforcement Learning with Neural Network Dynamics	https://arxiv.org/abs/1708.02596	Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, Sergey Levine	強化学習はモデルフリーで多くの成果が出ているが、大量の学習データが必要となる。一方環境をエミュレートするモデルベースはその分サンプルが少なくて済むものの適切な近似関数を見つけるのが難しい。そこでモデルベースとし環境の遷移を、予測が容易な短期分だけ予測するようにしたNNを使用して学習
540	Machine Learning that Matters (ICML2012)	http://www.wkiri.com/research/papers/wagstaff-MLmatters-12.pdf	Kiri L. Wagstaf JPL	意味のあるデータを収集して機械学習を行うべきと示唆、皆いつまでもアヤメの花の分類をして満足しては駄目という啓蒙的論文。Research Questionを持って意味のあるデータを使って研究を行いましょうという示唆をしている。
541	Deep Sets (NIPS2017)	https://arxiv.org/pdf/1703.06114.pdf	Manzil Zaheer 1 2 Satwik Kottur 2 Siamak Ravanbakhsh 2 Barnabas Poczos 2 Ruslan Salakhutdinov 2	教師つき学習は表現が有限な次元という問題点を解消するために有限なベクトル空間を置換可能な不変集合と考えて学習を行うDeepSetsを提案
542	Pose Guided Person Image Generation (NIPS2017)	http://papers.nips.cc/paper/6644-pose-guided-person-image-generation	Liqian Ma1 Xu Jia2 Qianru Sun3 Bernt Schiele3 Tinne Tuytelaars2 Luc Van Gool1,4	任意の姿勢をクエリにした画像生成の研究。Stage1でクエリ画像の姿勢に近い画像を生成、Stage2でStage1で生成された低解像度の画像を高解像度化を行ってより精錬された画像を生成する。
543	Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrier (KDD2015)	http://www.cs.cornell.edu/~bindel//blurbs/edgeppr.html	Wenlei Xie, David Bindel, Alan Demers, Johannes Gehrke	ページランクを求める際に、10年前ではデファクトだった数値計算手法が最近でも使われているが近年の高速な数値計算手法に置き換えることでページランクの計算パフォーマンスを劇的に向上。
544	High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs	http://www.cs.cornell.edu/~bindel//blurbs/edgeppr.html	Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro	2048x1024というサイズの、しかも鮮明な画像をGANで生成したという研究。結果は驚異的。小サイズの画像で事前学習したResNetを一回り大きい大サイズのCNNでサンドイッチいていく形で構成、またDiscriminatorをマルチスケール対応のため複数用意し、lossを特徴マップレベルでも取っている。
545	Embodied Question Answering	https://arxiv.org/abs/1711.11543	Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra	3D環境の中にエージェントを置き、その環境についての質問をし、エージェントは環境の中を探索することで回答する、というタスクを提案。House3Dというデータセットから小さすぎる物体を抜いたり同じような物体をマージして環境を作成し、質問は質問生成のための関数を通じて生成している。
546	FaSTrack: a Modular Framework for Fast and Guaranteed Safe Motion Planning	https://arxiv.org/abs/1703.07373	Sylvia L. Herbert, Mo Chen, SooJean Han, Somil Bansal, Jaime F. Fisac, Claire J. Tomlin	リアルタイムで、安全な行動計画を立てるための研究。リアルタイムに計画を立てていると今すぐ直角にカーブ、といった物理的に無理な軌跡が発生することがあり事故につながる。そこで認識した障害物の大きさに安全圏を足したうえでまず計画を行い、それを「追尾」する形にして安全性を増す手法を提案。
547	Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm	https://arxiv.org/abs/1712.01815	David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis	AlphaGo Zeroを少し変え、チェスと将棋に対応した研究。何れも既存の最強ソフトを(AlphaGo含め)、1日以内の学習で勝ち越した(ただ5000TPUv1+64TPUv2)。囲碁と違い双方盤面に向きがあるので方向を変えた画像複製をしない、self-playの相手は単純に更新するなどの違いがある
548	Learning to Generate Reviews and Discovering Sentiment	https://arxiv.org/abs/1704.01444	Alec Radford, Rafal Jozefowicz, Ilya Sutskever	センチメントの分類をするときに、Wikipediaみたいな文書で学習させた分散表現がいいのか？という提言。感情が含まれていると想定されるAmazonレビューで文字レベルの言語モデル(LSTM)を学習させて、それで分類をしてみたという研究。分類性能が良かったのはもちろん、文生成にも使えたという結果。
549	AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks	https://arxiv.org/abs/1712.02029	Aditya Devarakonda, Maxim Naumov, Michael Garland	学習率ではなく、バッチサイズを調整することで同等の結果が得られるという検証結果。実験では、バッチサイズをエポック数に応じて倍々にしていくことで精度が上がることを確認できた(ただ、学習率の低減を使用していないわけではない)。
550	Protein Interface Prediction using Graph Convolutional Networks	http://papers.nips.cc/paper/7231-protein-interface-prediction-using-graph-convolutional-networks.pdf	Alex Fout, Department of Computer Science, Colorado State University	タンパク質と受容体で分けてそれぞれGraphCNNし、マージしたものからタンパク質と受容体の接地面の予測をしている
551	Progressive Neural Architecture Search	https://arxiv.org/pdf/1712.00559.pdf	2017/12/2	CNNのパラメータ探索でA*探索に似たような手法を用いてCIFAR10のデータ・セットにおいて従来の強化学習よりも3.41%のエラーレートの改善を行い２倍程度の速度改善を行った。また遺伝的アルゴリズムに対しては3.63%のエラーレートの改善と５倍程度の速度改善を達成した。ImageNetでも動作し、
552	Toeplitz Inverse Covariance-Based Clustering of Multivariate Time Series Data	https://arxiv.org/pdf/1706.03161.pdf	David Hallac, Sagar Vare, Stephen Boyd, Jure Leskovec  Stanford University	多変量時系列の部分列クラスタリングは、time seriesの繰り返しパターンを発見するのに有用である。
553	A Fast Unified Model for Parsing and Sentence Understanding	https://arxiv.org/abs/1603.06021	Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, Christopher Potts	RNNとRecursiveNN双方のいいところどりをして、(構造情報を組み込んだ)よりリッチな潜在表現を得る試み。シーケンシャルに処理できるShift-Reduce(入って来た単語を先行単語とマージするか別のものとして積むかを逐次判断する)をRNNで再現している。なお、S/Rの判断はパーサーの判断を模倣するよう学習させた別モデルを使用する。
554	Variational Deep Q Network	https://arxiv.org/abs/1711.11225	Yunhao Tang, Alp Kucukelbir	DQNを行う際に、ネットワークのパラメータθに事前分布(qθ(φ))を仮定することで、効率的な探索を可能にするという研究。これにより、価値関数の分布を真の分布に近づけていくという分布近似(KL距離を詰める)の形で解けることを示している。
555	An Ensemble Model with Ranking for Social Dialogue	http://alborz-geramifard.com/workshops/nips17-Conversational-AI/Papers/17nipsw-cai-ensemble.pdf	Ioannis Papaioannou, Amanda Cercas Curry, Jose L. Part, Igor Shalyminov, Xinnuo Xu, Yanchao Yu, Ondrej Duek, Verena Rieser, Oliver Lemon	「20分間人と話せる」というAmazon Alexa Prizeのお題を、複数の対話ボットのアンサンブルで挑戦したという話(機械学習を使ったものもあれば、ルールベースのものもある)。どのボットの返答が良いかは、実際の人のフィードバックを使って評価している。
556	Variational auto-encoding of protein sequences	https://arxiv.org/pdf/1712.03346.pdf	Sam Sinai Harvard University	タンパク質の配列から情報を抽出し突然変異が起きたときの影響の予測は医学生物学両面において重要である。しかし、実際に行おうとすると配列と機能との関係性は複雑で理解が難しい。
557	The Case for Learned Index Structures	https://arxiv.org/pdf/1712.01208.pdf	2017/12/11	インデックス構造をディープラーニングで学習することで従来のB木に比べ70%近くの速度向上を達成。
558	Efficient Vector Representation for Documents through Corruption	https://arxiv.org/abs/1707.02377	Minmin Chen	単語だけでなく文章の性質もよく表す分散表現を得るために、学習時に近傍の単語だけでなくグローバルに選択してきた単語も加味するという手法。このグローバルコンテキストを(文書全体の単語からでなく)サンプリングで得ることにより、速度up+正則化をかけている。
559	Simple And Efficient Architecture Search for Convolutional Neural Networks	https://arxiv.org/abs/1707.02377	Thomas Elsken, Jan-Hendrik Metzen, Frank Hutter	CNNの構造を自動設計する研究。まずランダムに構造を作ってもCIFAR-10で誤差率6~7%は出てしまうことを確認(既存の自動設計手法と同等の結果)。次いで、ネットワークの変形手法を4つに定式化し、何個か変化させる⇒ベストを採用しそれをベースにまた何個か変化、と繰り返していくNASHという手法を提案
560	ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices	https://arxiv.org/abs/1707.01083	Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun	モバイル上での動作を想定したネットワークの提案。計算効率を上げるための手法としてDepth(チャンネル)で分割してパラレルに計算するという手法があるが、これだと他のチャンネルを考慮できない(RGBならRしか考慮できないなど)。そこで、チャンネルをシャッフルしてグルーピング・畳み込む手法を提案
561	Capsule Network Performance on Complex Data	https://arxiv.org/abs/1712.03480	Edgar Xi, Selina Bing, Yang Jin	Capsule Networkについて、CIFAR10データセットでの追試を行ったもの。計算時間の都合で50epoch終了時点での比較となっているが、最高精度を出した畳み込みx2+4アンサンブルでも71.5%という結果。オリジナルでは7アンサンブルで89.4だが、アンサンブルを増やしてもそこにはいけそうにないとの結果。
562	Visual to Sound: Generating Natural Sound for Videos in the Wild	https://arxiv.org/abs/1712.01393	Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, Tamara L. Berg	動画に自然な音をつけるという研究(犬がほえてる動画に鳴き声をつける等)。Googleの公開したAudioSetを使用し、音声は階層型のRNN(SampleRNN)、画像はVGG19で特徴を抽出しこれらを結合、さらに時系列の動画特徴をSeq2Seqで別途学習し、そのEncode結果を合わせ生成を行っている
563	Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation	https://arxiv.org/abs/1703.09902	Albert Gatt, Emiel Krahmer	自然言語の生成について、近年の研究をまとめたもの。最近はサッカー動画の説明、センサーデータの説明、医療画像から患者の状態を説明、といったように別ドメインのデータを変換する形での応用が広がっており、その重要性を増している。また口調の変化やジョークの生成といった事例も取り上げられている
564	Incremental Learning Through Deep Adaptation	https://arxiv.org/abs/1705.04228	Amir Rosenfeld, John K. Tsotsos	ドメインの異なる10の画像分類のデータセットを同時に解くチャレンジで1位を獲得した手法。単にClassifierを複数用意するだけでなく、ベースとなるネットワークに対してレイヤごとのCNNのフィルタをタスクに合わせて生成するControllerの機構を盛り込んでいる。
565	Born Again Neural Networks	http://metalearning.ml/papers/metalearn17_furlanello.pdf	Tommaso Furlanello, Zachary C. Lipton, Laurent Itti,Anima Anandkumar	ネットワークの継承ともいうべき手法。モデルの蒸留はモデルサイズを小さくするために用いられることが多いが、この研究ではラベル＋親の出力に近くなるように学習させることで、親をしのぐ精度の子ネットワークの学習に成功している。DenseNetからResNetで継承を行いCIFAR-100でエラー率15.5%を達成。
566	MentorNet: Regularizing Very Deep Neural Networks on Corrupted Labels	https://arxiv.org/abs/1712.05055	Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, Li Fei-Fei	より汎化性能を高めるために、どのサンプルからどれぐらい学習すべきかの重みを決めるメンターを使うという研究。メンターは正解ラベル、現在のepoch、現在のネットワーク(生徒に該当)におけるmini-batchに対するlossとその偏差を元に、学習状況に応じた重みを求めている。
567	State-of-the-art Speech Recognition With Sequence-to-Sequence Models	https://arxiv.org/abs/1712.01769	Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani	End-to-Endで音声認識を行うモデル。単語表記を使った認識では辞書外の単語を認識できない、音素では別途言語モデルが必要、というジレンマを克服するため、サブワードレベルの表記を単語表記に追加して対応している。また、複数Attentionにより精度を向上させている。
568	Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples	https://arxiv.org/abs/1704.07433	Haw-Shiuan Chang, Erik Learned-Miller, Andrew McCallum	それぞれのサンプルの難易度に合わせてサンプリング確率およびLossに掛けるweightを調整する研究。既存の研究を概観しつつ、SGD-WPV、SGD-WTC, SGD-SPV、SGD-STCという新しい手法を提案している。
570	Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions	https://arxiv.org/abs/1712.05884	Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, Yonghui Wu	Googleが以前発表した音声合成の手法であるTacotronとWaveNetを融合させてできたともいえる、Tacotron2。テキストを文字ベースでエンコードし、前回の予測と合わせてフレームのスペクトログラムを予測する(合成の終端も同時に予測する)。こここから、WaveNetライクなデコーダーで実音声へ変換する。
571	Train Once, Test Anywhere: Zero-Shot Learning for Text Classification	https://arxiv.org/abs/1712.05972	Pushpankar Kumar Pushp, Muktabh Mayank Srivastava	文書分類をZero-shotで行うという研究。文書分類を各カテゴリへの0/1分類として扱い、これにより得られる各カテゴリとの相関を元にクラスのベクトルを出力している(Zero-shotは分類がわからない前提なので、クラスのベクトルを出力する)。分類精度はSOTAと30%ほど開きがあり、まだまだといった印象。
572	Fast, Lean, and Accurate: Modeling Password Guessability Using Neural Networks	https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_melicher.pdf	William Melicher, Blase Ur, Sean M. Segreti, Saranga Komanduri,	RNNでパスワードを生成するという試み。既存の文法的なもの、またMarkovモデルといったものよりも良い働きをすることを確認。
573	Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning	https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_melicher.pdf	Tianmin Shu, Caiming Xiong, Richard Socher	強化学習において、複雑なタスクを段階的にばらして取り組めるようにする試み。前回から一段階ばらされたタスク、ばらしていないもの、このどちらを実行するかをスイッチしながら実行していくような形になっている。
574	A Flexible Approach to Automated RNN Architecture Generation	https://arxiv.org/abs/1712.07316	Martin Schrimpf, Stephen Merity, James Bradbury, Richard Socher	RNNの構造の自動構築を行う試み。ネットワークを記述する簡単なDSLを定義し、それをまずはランダム/既存のものに変更を加える形で構造を作る。その後各構造のパフォーマンスを予測し、予測結果が良好なもののみフルに学習する(結果は、パフォーマンス予測モデルにフィードバックされる)。
575	Pafnucy -- A deep neural network for structure-based drug discovery	https://arxiv.org/abs/1712.07042	Marta M. Stepniewska-Dziubinska1 Piotr Zielenkiewicz1,2 Pawel Siedlecki1,2,*	リガンド-受容体複合体の結合親和性の推定をCNNを用いて行なった。
576	Deconvolutional Paragraph Representation Learning	https://arxiv.org/abs/1708.04729	Yizhe Zhang Dinghan Shen Guoyin Wang Zhe Gan Ricardo Henao	パラグラフの潜在表現を学習するモデル
577	Learning Deep Representations for Graph Clustering (AAAI2014)	https://www.microsoft.com/en-us/research/publication/learning-deep-representations-for-graph-clustering/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F226627%2F%255baaai2014%255d%2520dnn%2520for%2520graph%2520cut.pdf	Fei Tian	自己符号化器とSpectral Clusteingの関連性を示した論文。
578	Improving Generalization Performance by Switching from Adam to SGD	https://arxiv.org/abs/1712.07628	Nitish Shirish Keskar, Richard Socher	Adamは収束が速いけど最終的な汎化性能はSGDに負ける、という点を、最初Adamで途中からSGDにスイッチするという手法で解決を試みる研究。Adamの方向と直交する方向にSGDが十分向かい始めたタイミングでスイッチする、という形。
579	CNN Is All You Need	https://arxiv.org/abs/1712.09662	Qiming Chen, Ren Wu	GoogleのAttention is All~の仕組み(Tensor2Tensor)を利用し、CNNのアーキテクチャに置き換えて検証した論文。dilated x residualというWaveNetの構成を下地に、position encode + cross attentionを導入した形になっている。結果はTransformerをしのぐものになっている。
580	Visualizing the Loss Landscape of Neural Nets	https://arxiv.org/abs/1712.09913	Hao Li, Zheng Xu, Gavin Taylor, Tom Goldstein	DNNにおける様々な手法について、それらが実際目的関数の探索空間にどんな影響を及ぼしているのかを可視化を通じて明らかにしようとする試み。パラメーターの次元と同じサイズで、各フィルターのノルムで正規化したベクトル2つを用いて、2次元等高線図を描画。skip-connectionの効果などを可視化できた
581	Deep Learning: A Critical Appraisal	https://arxiv.org/abs/1801.00631	Gary Marcus, Departments of Psychology and Neural Science, New York University.	現在のDeep Learningができることとできないこと，これから目指すところをまとめた論文．DeepLearningは内挿問題しか解くことができない．よってデータ数が少ない場合や，抽象的な取り扱い，因果関係，構造の理解などが必要な問題は取り扱えない．従って，汎用人工知能を目指すためには，記号処理や古いAIで培われてきた分野との融合や教師無し学習の活用，認知・発達心理学の知見の応用など，新しい発見が必要であると考えられる．
582	A Survey of Model Compression and Acceleration for Deep Neural Networks	https://arxiv.org/abs/1710.09282	Yu Cheng, Duo Wang, Pan Zhou, Tao Zhang	機械学習モデルをより低コストで動かすための研究についてのサーベイ。冗長パラメーターの削除、行列因子分解による計算コスト削減、省エネ特化モデルの作成、蒸留といった4つの類型に分類されて紹介されている。
583	What’s your ML test score? A rubric for ML production systems	https://research.google.com/pubs/pub45742.html	Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, D. Sculley	機械学習モデルのテストに関するチェックリスト。特徴量/データセット、モデルの開発・評価プロセス、モデルの運用保守インフラ、パフォーマンス監視の4つの観点でまとめられており、実運用を行う際は3-4ポイントでギリ、5ポイント-の獲得が望ましいとのこと。
584	Evading Machine Learning Malware Detection	https://www.blackhat.com/docs/us-17/thursday/us-17-Anderson-Bot-Vs-Bot-Evading-Machine-Learning-Malware-Detection-wp.pdf	Hyrum S. Anderson,  Anant Kharkar, Bobby Filar, Phil Roth	(ウイルスである)実行可能ファイルを挙動を変えない範囲で編集して、ウイルス対策ソフトの検知を回避するという試み。これに、ファイルを編集してだませたら報酬が入るという強化学習を適用している。結果、検知率を約50%低減できた。
585	Visual Dialog	https://arxiv.org/abs/1611.08669	Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos M. F. Moura, Devi Parikh, Dhruv Batra	画像についての質問に答えるVQAから、さらに継続的な対話を行うというVisual Dialogというタスクを提案。そのためのデータセットであるVisDialと、ベースラインとなるモデルの実装を公開。モデルは、単純に画像/質問/対話履歴を結合するものと、対話履歴を個別に積む、Memory Networkで引くという3種類
586	Character-level Recurrent Neural Networks in Practice: Comparing Training and Sampling Schemes	https://arxiv.org/abs/1801.00632	Cedric De Boom, Bart Dhoedt, Thomas Demeester	文字ベースCNNについて、学習のさせ方の類型とそれによるパフォーマンスの差異をまとめたサーベイ。lossの計算をシーケンス単位で一括で行うか個別の合算で行うか(Single/Multi)、予測結果を次のinputに使うか(Progressive)などで分類されている。基本Multiが良いとのこと。
587	Recent Advances in Recurrent Neural Networks	https://arxiv.org/abs/1801.01078	Hojjat Salehinejad, Julianne Baarbe, Sharan Sankar, Joseph Barfett, Errol Colak, Shahrokh Valaee	RNNの基礎的な仕組み、学習方法、LSTM/GRUを代表とする構造の工夫、応用例といった点をまとめたサーベイ論文。RNNについての代表的な発表をまとめた年表もついている。
588	Panoptic Segmentation	https://arxiv.org/abs/1801.00868	Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollr	画像認識における、セグメンテーションと物体検出のタスクを統合したタスクPanoptic Segmentationの提案。領域はStuff/Thingに分けられ、Stuff内のピクセルは単一クラス、Thingは各認識クラスが割り当てられる。タスクの指標として、セグメント品質と検出精度を掛け合わせたスコアを提案している。
589	A practical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models, software and guidelines	https://arxiv.org/pdf/1801.01586.pdf	2018/01/04	特徴量を合成して減らす技術としてオートエンコーダーモデルと他のモデルを比較し、オートエンコーダーの作成のガイドラインから、オートエンコーダーモデル作成に関連するソフトウェアの紹介までしている論文
590	It Takes Two Neurons To Ride a Bicycle	http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.3781&rep=rep1&type=pdf	Matthew Cook	自転車を制御するのに、2つのニューロンだけのネットワークで達成できたという話。自転車のコントロールを実際に人にやらせてみて、ハンドルの角度とモーメントの相関に着目し、角度=>モーメントの2ニューロンのネットワークを作成し、強化学習モデルより正確に制御できた。
591	Neural Speed Reading via Skim-RNN	https://arxiv.org/abs/1711.02085	Minjoon Seo, Sewon Min, Ali Farhadi, Hannaneh Hajishirzi	人の速読のスタイルを参考に、重要な箇所は大サイズ、そうでない箇所は小サイズのRNNとスイッチして学習するRNNを提案。スイッチ機構の導入のみで計算コストを下げることができ、通常のLSTMに比べて1.5~ほど高速に動作する。なおスイッチ操作は微分不可能なのでgumbel-softmaxで対応している。
592	Attacking Speaker Recognition With Deep Generative Models	https://arxiv.org/abs/1801.02384	Wilson Cai, Anish Doshi, Rafael Valle	話者認識のモデルを騙す音声を作成しようという試み(認識モデルはMel-SpectrogramをCNNにかけて認識するモデル)。音声の生成にはWaveNet/SampleRNNの2つを試しており、これで生成した音のMel-SpectrogramをWGANで改変するという流れ(特定のクラスに割り当てられるようにするtargetの実験のため、話者の違いを認識する項をGANのlossに導入している)。
593	Adversarial Spheres	https://arxiv.org/abs/1801.02774	Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Wattenberg, Ian Goodfellow	Adversarialな攻撃が成功する要因を調査した研究。条件をシンプルに、ノルムが1 or Rであるような次元dの球面上の点を生成し、それぞれラベルを0, 1と割り当てる。それを2層のReLUで分類してみた。この状態でも攻撃は成功し、決定境界の歪みを確認。汎化性能と攻撃回避の間のトレードオフも確認した。
594	DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning	https://arxiv.org/abs/1707.06690	Wenhan Xiong, Thien Hoang, William Yang Wang	知識グラフをたどるパスの学習に、強化学習を利用した試み。SourceからTargetにたどり着けた場合に報酬を与え、パスの短さと既存パスとは異なることにも報酬を与えている。普通に学習すると収束しなかったため、初代AlphaGoで使用されていた教師ありによるポリシーの学習を採用している。
595	Supervised and Unsupervised Tumor Characterization in the Deep Learning Era	https://arxiv.org/pdf/1801.03230.pdf	Sarfaraz Hussein, Maria M. Chuquicusma, Pujan Kandel, Candice W. Bolan, Michael B.	放射線画像からの特徴検出・予測・リスク回避などは、速さと精度の両方が重要視されている。
596	FUSE: Full Spectral Clustering (KDD2016)	http://www.kdd.org/kdd2016/subtopic/view/fuse-full-spectral-clustering	Wei Ye*, University of Munich; Sebastian Goebl, University of Munich; Claudia Plant, ; Christian Boehm, University of Munich	べき乗法と独立成分分析を用いたデータのマルチスケールに頑強なスペクトラルクラスタリング手法の提案
597	Call center stress recognition with person-specific models (ACII2011)	https://link.springer.com/chapter/10.1007/978-3-642-24600-5_16	Javier HernandezRob R. MorrisRosalind W. Picard	ストレスのレベルを測定するマシンを自作、実際にコールセンターのスタッフのストレスを測定後にアンケートでストレスレベルを答えてもらい学習データを作成し、SVMで判定を行う。
598	Analyzing Free-standing Conversational Groups: A Multimodal Approach (ACMMM15)	http://xavirema.eu/wp-content/papercite-data/pdf/Alameda-ACMMM-2015.pdf	X. Alameda-Pineda, Y. Yan, E. Ricci, O. Lanz, and N. Sebe	近接センサーと監視カメラを用いてスタンディングディスカッション形式を行っているグループに対してマルチモーダルな会話の解析・評価を行った。
600	Where To Look: Focus Regions for Visual Question Answering (CVPR2016)	http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shih_Where_to_Look_CVPR_2016_paper.pdf	Kevin J. Shih, Saurabh Singh, and Derek Hoiem	VQA datasetに対して、提案手法を適用。従来手法を(当時は)全て上回った。
601	Slicing Convolutional Neural Network for Crowd Video Understanding (CVPR2016)	http://www.ee.cuhk.edu.hk/~jshao/SCNN.html	Jing Shao1, Chen Change Loy2, Kai Kang1, and Xiaogang Wang1	xy- : 空間的特徴
602	Graphical Models for Processing Missing Data	https://arxiv.org/abs/1801.03583	Karthika Mohan, Judea Pearl	既存の欠損値を埋める手法は欠損がランダムに起きることを前提としているが、実際にはそうではなかったりする。数はG=性別、A=年齢、O=肥満度だが、十代の子は恥ずかしがって肥満度を報告しない、といったケースなどが考えられる(下図c。単純に報告するのを忘れた=ランダムの場合はb)。
603	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	https://arxiv.org/abs/1710.06280	Jun Hatori, Yuta Kikuchi, Sosuke Kobayashi, Kuniyuki Takahashi, Yuta Tsuboi, Yuya Unno, Wilson Ko, Jethro Tan	ロボットに自然言語で指示を出して行動させるという研究。箱の中のオブジェクトを別の箱に移動させるというタスクで、最初の指示で対象を特定できなかった場合、追加で指示を受ける。この指示と各オブジェクトでスコアを算出し、一定閾値より大きい場合に候補としている。
604	Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results	https://arxiv.org/abs/1703.01780	Antti Tarvainen, Harri Valpola	生徒と教師の2つのモデルを利用した、半教師あり学習の研究。生徒、教師はたがいに別々のノイズが付与されたデータをもとに予測を行い、生徒はラベルと教師の予測との一致を元に学習を行う。教師側は、エポック終了後に生徒の学習結果をマージする(exponential moving average)。
605	Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs	https://arxiv.org/abs/1703.01780	W. James Murdoch, Peter J. Liu, Bin Yu	LSTM内部の計算を、その単語自身の貢献とコンテキストの貢献に分割して、出力に対する各単語の貢献をわかるようにするという研究。ゲートの処理については、各セルの更新式を線形和にすることで貢献の分離が維持されるようにしている。
606	Game-theory insights into asymmetric multi-agent games	https://www.nature.com/articles/s41598-018-19194-4	Karl Tuyls, Julien Perolat, Marc Lanctot, Georg Ostrovski,  Rahul Savani,  Joel Leibo, Toby Ord, Thore Graepel and Shane Legg.	ゲーム理論/非対称情報ゲームにおけるナッシュ均衡の導出を、各プレイヤーの報酬表を元に対象ゲームに分割することで簡単に求められることを示した研究。これにより、ポーカーなどの非対称情報ゲームにも応用ができるとされている。
607	Interpretable Explanations of Black Boxes by Meaningful Perturbation	https://arxiv.org/abs/1704.03296	Ruth Fong, Andrea Vedaldi	DNNの画像認識モデルがどこを重要としているのかを可視化する研究。仕組み的には、重要と判断されるところを隠す(マスクする)ように学習を行う。これにより、モデル内部のアーキテクチャいかんにかかわらず利用することができる。
608	Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift	https://arxiv.org/abs/1801.05134	Xiang Li, Shuo Chen, Xiaolin Hu, Jian Yang	正則化の手法としてよく使われるDropoutとBatch Normalizationは、併用するとパフォーマンスの悪化が起こることがあるが、その原因について検証した研究。悪化の理由として、Dropoutを行うことで学習時と評価時で分散が変わってしまう一方、Batch Normalizationは学習で得られた分散を評価時もキープしてしまうため齟齬が生じることが原因と指摘された。
609	A graph-embedded deep feedforward network for disease outcome classification and feature selection using gene expression data	https://arxiv.org/pdf/1801.06202.pdf	Yunchuan Kong and Tianwei Yu, Department of Biostatistics and Bioinformatics, Emory University	遺伝子発現データは特徴量が多い割にサンプル数が少ないため、モデル構築が難しい。
610	Fine-tuned Language Models for Text Classification	https://arxiv.org/abs/1801.06146	Jeremy Howard, Sebastian Ruder	文書分類で転移学習をする試み。一般的なデータで学習した言語モデル(LSTMベース)を、対象ドメインのデータで転移学習し、線形レイヤをいくつか足して分類機を作る。言語モデルの転移ではレイヤを後ろから徐々に学習させる、分類機の学習ではBPTT的な学習(前回隠れ層持ち越し)をするなどの工夫がある
611	Global overview of Imitation Learning	https://arxiv.org/abs/1801.06503	Alexandre Attia, Sharone Dayan	熟練者の行動をまねるように学習させる、模倣学習についてのサーベイ。最新の研究というよりは、基礎的な手法が丁寧にまとめられている。限られた熟練者の行動を、学習＋生徒の行動を矯正する形で使うDAgger(性能がいい)についてOpenAI Gymを使った検証結果もまとめられている
612	Interpretable R-CNN	https://arxiv.org/abs/1711.05226v1	Tianfu Wu, Xilai Li, Xi Song, Wei Sun, Liang Dong, Bo Li	R-CNNで検知した領域を分割していき、検知においてどの領域が重要だったかを明らかにするという試み。領域の分割と選択はAND/ORのノード(ノード数は分割数に依存)で行い、ANDで分割、ORで選択を行う(逆に見えるが、上から下にステップを踏んでいくのでこのような形になる)。
613	A Graph-Based Approach to Analyze Flux-Balanced Pathways in Metabolic Networks	https://arxiv.org/pdf/1703.06496.pdf	Mona Arabzadeh, Department of Computer Engineering and Information Technology,	生物の代謝経路はつなげることでネットワーク構造をとる。食べ物を食べる→Aという反応が起こる→Bという反応が起こるという形である。この時バイオインフォマティクスではAという反応の反応速度を調べたい、ということがよくある。
614	Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning	https://arxiv.org/abs/1712.06567	Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, Jeff Clune	強化学習において、勾配ではなく、遺伝的アルゴリズムを用いてDNNのパラメーター更新してみた研究。パラメーターの更新は非常にシンプルなものだが、DQN/A3Cに匹敵するスコアを記録するケースも見られた。
615	Poincar Embeddings for Learning Hierarchical Representations	https://arxiv.org/abs/1705.08039	Maximilian Nickel, Douwe Kiela	階層構造をもつ自然言語情報を、階層情報の表現に適した双曲空間に埋め込むという研究。200次元の分散表現を5次元にまで圧縮できる。
616	MaskGAN: Better Text Generation via Filling in the ______	https://arxiv.org/abs/1801.07736	William Fedus, Ian Goodfellow, Andrew M. Dai	GANを利用してテキストを生成する試み。文の欠損をDにばれないよう埋めるのがGの仕事。生成はEncoder-Decoderの形だが、穴埋め箇所はDecoderの出力?を元にサンプリングする。この操作は微分不可能なため、学習にはActor-Critic的な強化学習の仕組みを使用。人の評価で言語モデルよりかなり高い評価
617	DroNet: Learning to Fly by Driving	http://rpg.ifi.uzh.ch/dronet.html	A. Loquercio, A.I. Maqueda, C.R. Del Blanco, D. Scaramuzza	ドローンを操作するためのネットワーク。基本構成はResNetで、ステアリングの角度と衝突確率を出力する。問題はデータセットで、町中でドローンを飛ばして収集するのは現在困難。そこで、車載カメラ/自転車といった地上からの画像を使用して学習を行っている。このデータセットも公開されている。
618	A Regularized Framework for Sparse and Structured Neural Attention	https://arxiv.org/abs/1705.07704	Vlad Niculae	Attentionの解釈性を高めるためにスパース性を導入するフレームワークの提案。
619	Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses	https://arxiv.org/abs/1708.07149	Ryan Lowe Michael Noseworthy	対話システムの応答を自動評価するモデル「ADEM」を提案。
620	OSVOS: One-Shot Video Object Segmentation	http://people.ee.ethz.ch/~cvlsegmentation/osvos/	S. Caelles*, K.K. Maninis*, J. Pont-Tuset, L. Leal-Taix, D. Cremers, and L. Van Gool	指定したオブジェクトのセグメンテーション(前景/背景分類)をOne-Shotで行う研究。ImageNetで学習させたモデルをセグメンテーションのデータで転移学習し、その後一枚の画像からだけ学習し、残りのフレームを前景/背景に分類する(輪郭検出のモデルも併用するが、こちらは1-Shotの学習を行わず固定する)
621	Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition	https://arxiv.org/pdf/1801.07455.pdf	Sijie Yan, Yuanjun Xiong, Dahua Lin	人体の骨格のダイナミクスは人間の行動認識において重要な情報を伝えることが出来る。しかし、スケルトンをモデリングする従来の手法では様々な制約により一般化が難しい。
622	Convolutional Invasion and Expansion Networks for Tumor Growth Prediction	https://arxiv.org/pdf/1801.08468.pdf	Ling Zhang, Le Lu, Senior Member, IEEE, Ronald M. Summers, Electron Kebebew, and	腫瘍の増殖は臨床での実測値を元に数理モデルを作成する形で行われてきた。
623	NiftyNet: a deep-learning platform for medical imaging	https://arxiv.org/pdf/1709.03485.pdf	Eli Gibson a,b,1, Wenqi Li a,1,  , Carole Sudre b , Lucas Fidon a , Dzhoshkun I. Shakir a , Guotai Wang a , Zach Eaton-Rosen b , Robert Gray c , Tom Doel a , Yipeng Hu b , Tom Whyntie b , Parashkev Nachev c , Marc Modat b , Dean C. Barratta,b, S´ebastien Ourselin a , M. Jorge Cardosob,2, Tom Vercauterena, 2	医療画像用のディープラーニングのプラットフォームを開発した。
624	Deep Interactive Evolution	https://arxiv.org/abs/1801.08230	Philip Bontrager, Wending Lin, Julian Togelius, Sebastian Risi	ユーザーとのインタラクションを通じてGANで画像を生成する研究。複数の潜在ベクトルを元に生成した複数の画像をユーザーに提示し好みのものを選択してもらい、選択された画像の生成元になったベクトルを元に進化戦略を用いて更新していくというアプローチ。
625	Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis	https://arxiv.org/abs/1801.05091	Seunghoon Hong, Dingdong Yang, Jongwook Choi, Honglak Lee	テキストから画像を生成する際に、レイアウトを生成してからそれを元に実画像を生成するという研究。Encodeしたテキストを元にBounding Boxを生成、その中のSegmentation Maskを生成、最後に画像を生成、とアノテーション情報をフルに活用した生成手法となっている。
626	Learning Explanatory Rules from Noisy Data	http://www.jair.org/media/5714/live-5714-10391-jair.pdf	Richard Evans, Edward Grefenstette	認識と推論を組み合わせた処理を微分可能な形式で解く研究。具体的には、手書き数字画像を認識しどちらが大きいか答えるといったタスク。入力を値に変換するconvert、与えられた言語セットから要素処理を作成するgenerate、それらを組み合わせ処理するinfer、出力を値化するextractで構成される(多分)
627	Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery	http://arxiv.org/abs/1703.05921	Schlegl, Thomas	GANを使った異常検知の研究。まず正常系の画像を生成するGeneratorを学習する。次に検査対象xに近い画像を生成するzを勾配法で求め、復元誤差 |x-G(z)| やfeature matching loss |f(x)−f(G(zγ))| を用いて異常度を推定する。
628	Recurrent Pixel Embedding for Instance Grouping	http://arxiv.org/abs/1712.08273	Kong, Shu	個体を区別したセグメンテーションを行う研究。各ピクセルを超球面上に埋め込み、Mean-Shiftクラスタリングする。Mean-ShiftクラスタリングはRNNとして表現可能なのでEnd-to-Endで学習できる。
629	A Bayesian Perspective on Generalization and Stochastic Gradient Descent	http://arxiv.org/abs/1710.06451	Smith, Samuel L.	ベイズ証拠を観察すれば汎化性能を予測できるよという研究。
630	Word translation without parallel data	https://openreview.net/forum?id=H196sainb	Guillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, Herv Jgou	教師なし学習で、単語間の翻訳を行う研究。2つの言語で個別に学習された分散表現を、敵対的学習の枠組みでソース言語の単語ベクトルをDに見破られないようターゲット言語の単語ベクトルに似せて変換する(教師なしのためペアはランダムに選ばれる)。これにより幾つかの言語で教師ありの結果を上回った。
631	Unsupervised Machine Translation Using Monolingual Corpora Only	https://openreview.net/forum?id=rkYTTf-AZ	Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc'Aurelio Ranzato	教師なしで翻訳を行う試み。ソース・ターゲットでそれぞれノイズを入れた文を復元するEncoder-Decoderを作成し、翻訳結果(＋ノイズ)をターゲットのEncoderで潜在表現にしたものがソースのDecoderで復元できるよう学習する。ソース/ターゲットの潜在空間が近しくなるよう、敵対的学習のlossを加えている
632	Assertion-based QA with Question-Aware Open Information Extraction	https://arxiv.org/abs/1801.07414	Zhao Yan, Duyu Tang, Nan Duan, Shujie Liu, Wendi Wang, Daxin Jiang, Ming Zhou, Zhoujun Li	質問に対し、単純に一語や該当箇所の抜粋で答えるのでなく、「XがYをZした」というような主語/述語/目的語などを伴った回答(Assertion)を行うためのデータセットとベースラインの提案(知識抽出からの生成に近い形になる)。ベースラインは生成型・抽出型の2種を構築している。
633	Nested LSTMs	https://arxiv.org/abs/1801.10308	Joel Ruben Antony Moniz, David Krueger	LSTMにおいて層を重ねるのでなくネストした方が良いという研究。具体的には、LSTMのcontextを更新する代わりに内部のLSTMへのinputとして送り込む(外側のforgetとinputからの出力は通常合算でcontextの更新に使われるが、これをコンカチし内部のinputに送る)。PTB、特に中国詩データセットで顕著な効果
634	Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling	https://openreview.net/forum?id=H1cWzoxA-	Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang	Self-Attentionは並列計算可能で長期依存も考慮できるというCNN/RNNの特性を兼ね備えたモデルではあるが、系列が長くなるにつれ要素同士の組み合わせ数が増えメモリを食うという弱点があった。これを系列をブロックに分割しブロック内・外でAttentionを貼ることで削減＋Bidirectionalにすることを提案
635	Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning	https://openreview.net/forum?id=B18WgG-CZ	Sandeep Subramanian, Adam Trischler, Yoshua Bengio, Christopher J Pal	文のベクトル表現をマルチタスクで学習する試み。Seq2Seqがベースで、共通のEncoderを持ちタスクごとにDecoderを切り替える(タスクが文間関係予測=分類問題の場合別)。前後の文予測・翻訳・構文解析・文間関係予測の4タスクをランダムに切り替えて学習を行い、様々な評価タスクで高いスコアを記録
636	Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling	https://arxiv.org/abs/1412.3555	Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio	LSTMとGRU、どちらが良いかの検証を行った論文。対象のタスクは音楽の学習(イメージ的にはsin波での検証)で、結果はどちらが良いともいえないという結果だった。
637	Generating Wikipedia by Summarizing Long Sequences	https://arxiv.org/abs/1801.10198	Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, Noam Shazeer	Wikipediaは、参考文献(Reference)のサマリとみることができる。そこで参考文献の文書から、WikipediaのLeadを生成してみるという研究(Lead以外にタイトルで検索しヒットしたサイトも加えている)。元文が多いため、最初に文抽出をしてその後DecoderのみのSelf-Attentionネットを利用して生成している。
638	DensePose: Dense Human Pose Estimation In The Wild	https://arxiv.org/abs/1802.00434	Rza Alp Gler, Natalia Neverova, Iasonas Kokkinos	2次元画像上のピクセルを、人の3Dモデル上にマッピングする研究。Mask-RCNNをベースに候補領域を検出し、CNNを通じ3Dモデルのどこのパートか、またそのパート上の座標の2つを予測させる。また、アノテーションされていない点をinpaintのように埋めることで、精度をさらに高めることができたとのこと。
639	How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation	https://arxiv.org/abs/1802.00682	Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, Finale Doshi-Velez	わかりやすい説明についての研究。機械学習で推薦されたアイテムについて、昼で空腹なら牛丼、というように「A, BならC」といった複数の説明を提示し、説明量、バリエーションによる反応の変化を検証した。基本長いほど低下だが、繰り返しが有効、入力にない概念の複数提示が良いことありといった結果
640	Scalable and accurate deep learning for electronic health records	https://arxiv.org/abs/1801.07860	Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M. Dai, Nissan Hajaj, Peter J. Liu, Xiaobing Liu, Mimi Sun, Patrik Sundberg, Hector Yee, Kun Zhang, Gavin E. Duggan, Gerardo Flores, Michaela Hardt, Jamie Irvine, Quoc Le, Kurt Litsch, Jake Marcus, Alexander Mossin, Justin Tansuwan, De Wang, James Wexler, Jimbo Wilson, Dana Ludwig, Samuel L. Volchenboum, Katherine Chou, Michael Pearson, Srinivasan Madabushi, Nigam H. Shah, Atul J. Butte, Michael Howell, Claire Cui, Greg Corrado, Jeff Dean	患者の電子健康記録(EHR)から死亡率や再入院確率、入院期間などを高精度で予測できたという研究。各電子記録をFHIRというフォーマットに整え時系列に並べ、予測を行う。医療記録の種別によりイベント頻度が大きく異なるため(バイタルは高頻度など)、3つのモデル(各RNN/NN/決定木ベース)で対応している
641	Visual Interpretability for Deep Learning: a Survey	https://arxiv.org/abs/1802.00614	Quanshi Zhang, Song-Chun Zhu	CNNの判断根拠を明らかにする方法のサーベイ。5つのカテゴリに分けて紹介されており、隠れ層の活性を利用する、inputのFlipなどにより重点箇所を探る、判断根拠の共起/相関を探る、あらかじめ解釈性の高いモデルを作る、人とのインタラクションから判断根拠を明らかにする、というように分けられている
642	IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures	https://arxiv.org/abs/1802.01561	Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu	強化学習で大規模な分散学習を行う研究。A3Cでは各エージェントは勾配を中央サーバーに送るが、提案手法(IMPALA)では経験(状態/行動/報酬)をそのまま中央(Learner)に送りそこで学習する。よって末端エージェントはoff-policy学習となるが、各経験に重要度をふるためのV-traceという手法を提案している
643	Regularized Evolution for Image Classifier Architecture Search	https://arxiv.org/abs/1802.01548	Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le	CNNの構造探索について強化学習と進化戦略を同じハードウェアスペック・探索空間など条件をそろえて比較した研究。結果は進化戦略の方が良好なモデルを作成し、探索の結果得られたAmoebaNetsはCIFAR-10でSOTA。進化戦略はトーナメント戦略と最も古いものを除く方式の2つ、強化学習はTRPOを使用している
644	Deceptive Games	https://arxiv.org/abs/1802.00048	Damien Anderson, Matthew Stephenson, Julian Togelius, Christian Salge, John Levine, Jochen Renz	どんなゲームが強化学習にとって難しいかを検証した論文。基本的に報酬が高い方へ動くのを逆手に取り、途中にトラップを置く、低い報酬にしか到達できない、またしばらく待たないと最高報酬を得られないといった単純なルールによる阻害などを検証。エージェントによって得手不得手が見られたという結果
645	DeepType: Multilingual Entity Linking by Neural Type System Evolution	https://arxiv.org/abs/1802.01021	Jonathan Raiman, Olivier Raiman	固有表現認識のように各単語のカテゴリ(型)を推定する研究。通常相当量の教師データが必要だが、Wikipediaの記事とそのカテゴリを利用することで代替している。15万カテゴリからAUC(=どれだけその型を予測できるか)を使用し100程度に絞っている。最終的には周辺文から該当単語のカテゴリを予測している
646	Personalizing Dialogue Agents: I have a dog, do you have pets too?	https://arxiv.org/abs/1801.07243	Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston	対話システムで発話内容に一貫性を持たせる＋相手のペルソナも考慮する研究。クラウドソーシングを使い1155人文のペルソナ(猫を飼っている、など自分に関する5つ以上の文)を収集し、さらに語彙などが偏らないよう別の人による書き換えを行い、そこから選んだペルソナによる10,981の対話を収集して学習。実装はEncodeしたペルソナの情報にAttentionを張ったシンプルなモデル。
647	On the Convergence of Adam and Beyond	https://openreview.net/forum?id=ryQu7f-RZ	Sashank J. Reddi, Satyen Kale, Sanjiv Kumar	学習率を自動調整するアルゴリズム(Adam/RMSPropなど)は、しばしば最適解に至らないケースがある。その原因として、ごく稀に大きな勾配が発生するケースにおいて、指数移動平均がその影響を急速に減少させてしまう点がある。そこで低めの学習率を使用し、過去の勾配の影響をなだらかにする手法を提案。
648	DAWNBench: An End-to-End Deep Learning Benchmark and Competition	https://openreview.net/forum?id=ryQu7f-RZ	2.optimizerの影響	ある精度を達成するのに、どれくらい時間/コストがかかるかを検証した研究。このGPU、このライブラリ(TensorFlowなど)の実装でこれくらい時間がかかる、というように、モデルだけでなくそれを走らせるハードまで含めたEnd to Endのコストを検証している。
649	Synthesizing Audio with Generative Adversarial Networks	https://arxiv.org/abs/1802.04208	Chris Donahue, Julian McAuley, Miller Puckette	GANを音声に適用した研究。音声ベース(WaveGAN)と、スペクトログラムベース(SpecGAN)の2種類を提案している。音声は周期性があり特徴をとらえるには長い幅が必要なため、1次元のフィルタ(サイズ25)で、画像より大きい指数(4)をupsamplingに使用している。音質はWave、印象はSpecの方が良いという結果。
650	Fidelity-Weighted Learning	https://openreview.net/forum?id=B1X0mzZCW	Mostafa Dehghani, Arash Mehrjou, Stephan Gouws, Jaap Kamps, Bernhard Schlkopf	訓練データの中には、エキスパートがしっかり行ったものもあれば、クラウドソーシングにより、中には適当につけられたものもある。それらを同一の「学習データ」とくくるのでなく、適切な重みをつけて学習しようという提案。エキスパート以外の手によるデータで一度studentのネットワークを学習させる一方、エキスパートの手によるデータでデータの信頼度をteacherに学習させる。最終的に、teacherで重みを付けたデータでstudentを学習させるという寸法。
651	Efficient Neural Architecture Search via Parameter Sharing	https://arxiv.org/pdf/1802.03268.pdf	Hieu Pham/Google Brain, Language Technology Institute, Carnegie Mellon University	コントローラーは計算グラフの中の最も最適なサブグラフの探索方法を学ぶ。ヴァリデーションセットにおける報酬がもっとも最大になるように学習していく。これは選択したサブグラフがモデルと一致するようにキャノ二カルクロスエントロピーを最小化するように学習することを意味する。論文ではGPU(GTX1080Ti)を1枚で10時間程度で学習しPenn Treebank datasetで55.8パープレキシティ、Cifar10のテストデータで2.89%を達成
653	Bridging Long Time Lags by Weight Guessing and "Long Short Term Memory"	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.9964	Sepp Hochreiter , Jrgen Schmidhuber	LSTMのパラメーターを、勾配でなくランダムサーチで探したほうが速かったという研究。なんと1996年の論文。最適化方法の提案というより、既存の最適化手法、また解くべき問題が適切なのかという点への疑義としている。
654	Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization	https://arxiv.org/abs/1603.06560	Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar	ハイパーパラメーターの探索に、多腕バンディットを適用してより良いパラメーターほど学習時間を割り当てて慎重に選ぶという手法。ベースとなっているのは学習させる=>悪いもの半分を削る、を繰り返すSuccessiveHalvingという手法で、選別のための学習時間を効率的に割り振る
655	Deep contextualized word representations	https://arxiv.org/abs/1802.05365	Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer	文脈情報(単語系列の情報)を抽出する機構を転移学習する仕組み。具体的にはL層の双方向LSTMを事前学習して使用。単語情報を表す単語分散表現と各層の隠れ層をそれぞれマージしたものとを結合し利用するが、どの層をどれだけ重視するかの重みはタスク個別に設定する。これで質問回答等NLP6タスクでSOTA
656	Evolved Policy Gradients	https://arxiv.org/abs/1802.04821	Rein Houthooft, Richard Y. Chen, Phillip Isola, Bradly C. Stadie, Filip Wolski, Jonathan Ho, Pieter Abbeel	エージェントに戦略を学習させる際に、学習がはかどるよう目的関数を(進化戦略により)調整するという手法。環境からサンプリングした遷移からまずは普通に戦略を勾配により学習し、一方で戦略を通じ得られた報酬を元に目的関数を更新する。
657	Diversity is All You Need: Learning Diverse Skills without a Reward Function	https://sites.google.com/view/diayn	Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine	教師なし学習によりタスクの遂行に有効なスキル(=戦略)を獲得しようという試み。到達した状態からどのスキルが使われているか識別しようとするDiscriminatorと、識別性が上がるよう多様な状態に遷移しようとするスキルの相互学習の形態。一部の環境ではこれのみで攻略できた。
658	One Deep Music Representation to Rule Them All? : A comparative analysis of different representation learning strategies	https://arxiv.org/abs/1802.04051	Jaehun Kim, Julin Urbano, Cynthia C. S. Liem, Alan Hanjalic	音楽における転移可能な潜在表現を見つける試み。様々な構造/学習方法のモデルを使用し、画像における転移学習を達成するにはどのようにすればよいのかを検証している。画像におけるパッチのような局所特徴が音楽ではあまりみられないのと、マルチタスクが有効という示唆。
659	SSD: Single Shot MultiBox Detector	https://arxiv.org/abs/1512.02325	Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg	高速な物体検知を1ネットワークで行う研究。YOLOv2高速版のFast YOLOに速度は若干劣るものの、精度はFaster-RCNNと同等で上回る(素のYOLOよりは早い)。シンプルなEnd-to-Endのネットワークで、マルチスケールに対応するためネットワーク後方に複数サイズの特徴マップを持ち、異なるアスペクト比の物体に対応するためアスペクト比ごとに出力を分けて学習している。
660	Investigating Rumor News Using Agreement-Aware Search	https://arxiv.org/abs/1802.07398	Jingbo Shang, Tianhang Sun, Jiaming Shen, Xingbang Liu, Anja Gruenheid, Flip Korn, Adam Lelkes, Cong Yu, Jiawei Han	フェイクニュースを検知する試み。タスクとしてはある質問に対し、各記事のスタンス(賛成・反対など)を判定するというもの。実際にデータを参照することで関連があるか、関連がある場合そのスタンスは、という2つのタスクに分けたほうが有効と判断し、前者はRNN+Attention、後者は全結合で行っている。
661	Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari	https://arxiv.org/abs/1802.08842	2018/2/24	OpenAIが公表した強化学習への進化戦略の適応について、より簡単な実装で比肩する性能を出せたという話。OpenAIの手法ではoptimizerを経由してpolicyを学習するなどの工夫が取られているが、本研究では純粋にpolicyの勾配を重み付き平均で更新するというかなり単純な方法をとっている。
662	Can Neural Networks Understand Logical Entailment?	https://arxiv.org/abs/1802.08535	Richard Evans, David Saxton, David Amos, Pushmeet Kohli, Edward Grefenstette	文の論理構造を捉えるのに適したモデルを調査した研究。タスクは文間の関係推定で、関係性を捉えないと答えられないよう意図的に合成したデータセットを使用している。CNNやTransformerはあまり良好な結果でなくTreeLSTMと提案モデル(構造を表す?Worldと名付けられた潜在表現を経由して推定する)が良好
663	Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces	https://arxiv.org/abs/1802.09913	Isabelle Augenstein, Sebastian Ruder, Anders Sgaard	クラス分類のタスクについて、マルチタスク＋半教師ありを組み合わせた手法の提案。各タスクでラベル情報をEncodeした空間を共通で持ち、そこから各タスクに応じたラベルの予測を行う(主タスク)。同時にこのラベル空間を元にラベルなしデータへのラベルづけを行い、本体側の予測と比較する(補助タスク)
664	Efficient Neural Audio Synthesis	https://arxiv.org/abs/1802.08435	Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, Koray Kavukcuoglu	WaveNetをより高速にしたWaveRNNの提案。モデルはなんと1レイヤのRNN。16ビットの音声をhigh/lowの8ビットにわけて考え個別に予測する(dual softmax)、さらに重みを間引き96%削減、予測の将来部分に依存してさらに予測することでバッチ単位でのサンプリングを可能にしている。
665	Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation	https://arxiv.org/abs/1606.00776	Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula, Bowen Zhou, Yoshua Bengio, Aaron Courville	RNNの構造よりも入力データに着目した研究。入力として通常の文字列と、そこから「粗い(Coarse)」情報を抽出したものとを並列で与えている(=multi resolution)。「粗い」情報を作る方法として、名詞のみ抽出やActivity-Entity(dialog actと質問カテゴリのようなもの)を使用する方法が提案されている。Ubuntuのテクニカルサポートというタスク指向型対話、Twitterの非タスク指向型双方で既存のモデルを上回る。
666	Applied machine learning at facebook a datacenter infrastructure perspective  (HPCA18)	https://research.fb.com/publications/applied-machine-learning-at-facebook-a-datacenter-infrastructure-perspective/	Kim Hazelwood, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov,	Facebookで ML as a Service(MLaaS)をどのように提供しており、FB内の機械学習のパイプラインのデザインについて解説している。
667	Learning Longer-term Dependencies in RNNs with Auxiliary Losses	https://arxiv.org/abs/1803.00144	Trieu H. Trinh, Andrew M. Dai, Thang Luong, Quoc V. Le	RNNで長い系列を扱うために、ランダムに選択したある地点の過去or未来を予測させるという補助タスクを導入するという提案。これにより16.000という長い系列のタスクを既存手法より高い精度で行えた他、限定された範囲のBPTTのため計算コストが少ないという利点がある。
668	An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling	https://arxiv.org/abs/1803.01271	Shaojie Bai, J. Zico Kolter, Vladlen Koltun	WaveNetで利用されたCNNで系列データを扱う方法(causal + dilated +  residual)を、既存のRNN(LSTM/GRU)と比較した研究。画像ピクセル予測・音声・言語と一通りのタスクをそろえて検証したところ、CNNベースの方が良かったという結果。「系列ならRNN」を再考すべきとしている
669	Towards a Question Answering System over the Semantic Web	https://arxiv.org/abs/1803.00832	Dennis Diefenbach, Andreas Both, Kamal Singh, Pierre Maret	既存の知識ベースを使った研究は一つの知識ベースかつ英語に特化しているものがほとんどだった。そこで、自然言語を知識ベースを検索するための汎用クエリ(SPARQL)に変換することでどんな言語・知識ベースでも検索できるようにするというもの。
670	Annotation Artifacts in Natural Language Inference Data	https://arxiv.org/abs/1803.02324	Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, Noah A. Smith	Aという文があるとき文BがAとどういう関係か(強調、反対など)を推定するタスクのデータセットについて、Aだけで推定可能という話。代表的なSNLI/MultiNLIで、互いに半分以上は推定可能だった。これは、クラウドソーシングで文を作る際Aをあらかじめ強調しやすい文にするなどされていることによるという
671	Modeling Relational Data with Graph Convolutional Networks	https://arxiv.org/abs/1803.02324	Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, Max Welling	知識ベースのようなグラフを畳み込みリンクやノードのエンティティを推定する研究。リンクする周辺ノードをそれぞれ重みをかけて合計していくが、in/outの方向を区別し、また自己ループを考慮する。ただこのままだとリンクが多い場合パラメーターが増え普通に過学習するので、重み行列の数を絞っている(B個のVを用意して、それらにリンクごとのスカラー係数aを書けることでWを計算する)。
672	Meta-Learning for Semi-Supervised Few-Shot Classification	https://arxiv.org/abs/1803.00676	Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, Richard S. Zemel	Few-Shotでの分類を行う際、各クラスの代表ベクトル(プロトタイプ)を作りサンプルがどのクラスの代表と近いかを調べる方法がある。この代表ベクトルをラベルなしデータで改善する方法の提案。ラベルなしも代表への近さで分類するが、ノイズになるのを防ぐため除去用クラスタやマスクをかける手法を提案
673	Accelerated Methods for Deep Reinforcement Learning	https://arxiv.org/abs/1803.02811	Adam Stooke, Pieter Abbeel	強化学習について、CPU/GPUでの計算やバッチサイズを見直し、NVIDIA DGX-1を投入することでAtariをPolicyベースなら10分単位、DQNのようなQ-Valueベースなら2時間で学習できた話。CPUには環境でのstepを並列で、勾配はGPUで一気に計算する。勾配計算中CPUが空くので、2交代制で馬車馬のように働かせる
674	Anomaly Detection of Network Traffic Based on Prediction and  Self - Adaptive Threshold	https://pdfs.semanticscholar.org/8278/7c26985e270ea412de73db4ade4744564d89.pdf	Haiyan Wang/Binzhou University	ネットワークにおけるセキュリティにおける問題でネットワークの失敗やマルウェアによるアタックが挙げられる。ネットワークのモニタリングを行いトラフィックの予測と異常検知を組み合わせて行う手法を提案。Wavelet変換したデータの高周波をRVM、低周波をARMAで予測した結果を組み合わせた結果を最終結果として使用。閾値は自動調整する中心極限定理を導入。結果としてARMAを超える予測精度を達成
675	Deep Reinforcement Learning of Cell Movement in the Early Stage of C. elegans Embryogenesis	https://arxiv.org/pdf/1801.04600v2.pdf	2018/01/14	人間に近い微生物における、成長の初期段階での細胞の動きを強化学習で模倣してみると言う試み。
676	Zoneout: Regularizing RNNs By Randomly Preserving Hidden Activations	https://arxiv.org/pdf/1606.01305.pdf	David Krueger,	ゾーンアウトは、RNNを正則化するための新しい方法です。タイムステップごとに、
677	Reptile: a Scalable Metalearning Algorithm	https://arxiv.org/abs/1803.02999	Alex Nichol, John Schulman	Few-shotの学習に強いメタラーニングの手法の研究。少ないサンプルでも学習できる良い初期値を得るMAML(Model-Agnostic Meta-Learning)という手法をベースにしており、この初期値を現在の初期値vsサンプルを学習させた後の重みとの差分を使用し学習させていく(これはシンプルなSGDの仕組みと同様)。
678	Focal Loss for Dense Object Detection	https://arxiv.org/abs/1708.02002	Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollr	物体検出では、物体が映りうる箇所全部を調べるとなると大半の領域には何も映っていないので、物体のある/なしのラベルに大きな偏りが生まれる。それがゆえに(経験的なルールで絞り込み、分類する2ステージに比べ)1ステージがうまくいっていないと仮定。ラベル数が多い場合に誤差の重みを調整する手法(Focal Loss)によりこの点を克服した。
679	The Lottery Ticket Hypothesis: Training Pruned Neural Networks	https://arxiv.org/abs/1803.03635	Jonathan Frankle, Michael Carbin	DNNが最終的に学習される関数に比してパラメーター数が多いのは、多いほど最適な構造(=宝くじ)が現れる可能性が高いから、という仮説。枝刈りにより学習済みのモデルからこの「宝くじ」構造を抽出すると、その部分は高速に学習したとのこと。
680	A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music	https://arxiv.org/abs/1803.03635	Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck	Magentaプロジェクトから音楽を「混ぜる」ことができるMusicVAEが公開。名前の通りVAEがベースで、Encoderは双方向LSTM、Decoderは階層上の構成(バッチで生成を行うイメージで、現バッチ生成用の潜在表現を作るLSTMと、そこから音符を生成するLSTMの二段構造)になっている。名称はMusicVAE
681	Community Interaction and Conflict on the Web	https://snap.stanford.edu/conflict/	Srijan Kumar,  William L. Hamilton,   Jure Leskovec,   Dan Jurafsky	オンライン上での罵りあいを分析、予測した研究。闘争のきっかけはクロスリンク(掲示板Aに「Bという掲示板の奴らが・・・」的な投稿がされる)であることが多いという点に着目し、ユーザー、元/先の掲示板のベクトル(これはどのユーザーがどの掲示板に投稿したかというグラフから作られる)を使用したLSTMで予測している。
682	Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning	https://arxiv.org/abs/1803.05268	David Mascharka, Philip Tran, Ryan Soklaski, Arjun Majumdar	画像を見て質問に答えるVQAのモデルは、解釈性が高い場合精度が出ず、精度が高いと解釈性が低かった。そこで双方高いモデルを構築した研究。回答に必要な処理に応じたブロックを用意し、それらを連続的に適用することで回答を行う。四角の左は?ならQuery(Relate(Attention(四角),Left)))といった形
683	Impacts of Dirty Data: and Experimental Evaluation	https://arxiv.org/abs/1803.06071	Zhixin Qi, Hongzhi Wang, Jianzhong Li, Hong Gao	データの品質が機械学習アルゴリズムの性能に及ぼす影響について検証した研究。データの品質として代表的な3タイプ(欠損・矛盾・競合(=同じデータのはずだが値が異なる))を挙げ、データに対する欠損などの割合に対する精度変化を計測する指標を2つ提案し各手法を評価している
684	Controlling Decoding for More Abstractive Summaries with Copy-Based Networks	https://arxiv.org/abs/1803.07038	Noah Weber, Leena Shekhar, Niranjan Balasubramanian, Kyunghyun Cho	抽象型の要約では生成と元文からのコピーを併用するモデルが最近提唱されているが(いわゆるpointer-generatorネットワーク)、これはテスト時に元文をコピーする方に大きく偏る。そこで、コピーをする確率が高まるとペナルティを与える項を設けて調整を行ったという研究。
685	Word2Bits - Quantized Word Vectors	https://arxiv.org/abs/1803.05651	Maximilian Lam	単語分散表現をビット化してサイズを減らす試み。CBOWをベースに、量子化関数をかませ必要なサイズを減らしている。単語類似度と質問回答タスクで精度が向上したが、アナロジー(単語間の演算のようなタスク)では精度が低下。通常のWord2Vecが過学習気味になるのに対し、正則化効果が見られたとのこと。
686	PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning	https://arxiv.org/abs/1711.05769	Arun Mallya, Svetlana Lazebnik	ネットワークに複数の画像認識を行わせる試みで、最初のタスクを学習させた後にヒマをしているノードを次のタスクにあてて、ということを繰り返していく手法(選別はパラメーターの重みで行う)。これでVGG-16をベースに、3つの分類タスクをそれぞれ個別に学習させたモデル並みの精度で達成。
687	Adversarial Logit Pairing	https://arxiv.org/abs/1803.06373	Harini Kannan, Alexey Kurakin, Ian Goodfellow	敵対的サンプルに対する防衛方法として、学習時に2つのサンプルが類似していると判断する項(logit pair)を設ける手法を提案。ペアを学習データ中から選ぶclearn、本物と敵対的サンプルのペアにするadversarialの2つを使用。ホワイト/ブラックボックス何れでもImageNetのサイズで防衛効果を確認
688	Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models	https://arxiv.org/abs/1801.07704	Tal Baumel, Matan Eyal, Michael Elhadad	クエリに対し関連する複数の文書から回答する形の要約(クエリ型)と、抽象型要約を組み合わせる研究。クエリ型は抽出型要約の適用が多く冗長になることが多いが、抽象型を組み合わせることで解消しようとしている。クエリとの関連度を各文の各単語について計算し、このスコアと単語自身を学習済み抽出型要約への入力にしている。また、再帰的に生成を行うことで指定単語分の要約を生成。
689	On the importance of single directions for generalization	https://arxiv.org/abs/1803.06959	Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, Matthew Botvinick	DNNの挙動を理解するために、ネットワーク内のノードを削除して反応がどう変るかを検証した研究。猫や犬といった特定のクラスにのみ反応するノードはあまり重要でなく、よくわからない(confusing)ほうが重要(削除したときの影響が大きい)。また汎化性が高いほど削除耐性があるとのこと
690	World Models	https://arxiv.org/abs/1803.10122	David Ha, Jrgen Schmidhuber	強化学習において環境のシミュレーター(World Model)とエージェントの操作(Controller)を分けて考えたモデル。環境は画面の表現をVAEで、時系列の遷移をRNNで学習(次時刻におけるVAEの潜在表現zの分布を予測する)、操作側はVAEの潜在表現とRNNの隠れ層を結合して重みをかけるだけというシンプルさ。
691	Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs	https://arxiv.org/abs/1803.08035	Xiaolong Wang, Yufei Ye, Abhinav Gupta	知識グラフを使ってZero-shotの画像判定を行う試み。知らない動物でも知識グラフを辿ればわかるという発想で、各ノードが画像クラスに対応した知識グラフを使用し、ノードを表す単語からグラフ畳み込みを通じ画像特徴ベクトルを得る(ImageNetはWordNetベースなのでこれが可能)。SOTAを18.7%更新。
692	DCN+: Mixed Objective and Deep Residual Coattention for Question Answering	https://arxiv.org/abs/1711.00106	Caiming Xiong, Victor Zhong, Richard Socher	Question Answeringタスクのデータセット「SQuAD」を解くモデル。2017/11当時はSOTA
693	Deep Communicating Agents for Abstractive Summarization	https://arxiv.org/abs/1803.10357	Asli Celikyilmaz1 , Antoine Bosselut1,2 , Xiaodong He3 and Yejin Choi2	要約における課題の1つである長い入力文をどうEncodeするかに取り組んだ論文。
694	Group Normalization	https://arxiv.org/abs/1803.08494	Yuxin Wu, Kaiming He	Batch Normalizationはバッチのサイズが小さくなるとエラー率が高くなってしまうため、チャンネルをグループに分けてそれ毎に正規化を行おうという話(論文中の図の解説がわかりやすい)。チャンネルは相互に影響しあっているはずなので、グループでまとめてもいいはずとのこと
695	YOLOv3: An Incremental Improvement	https://pjreddie.com/media/files/papers/YOLOv3.pdf	Joseph Redmon Ali Farhadi	YOLOが進化しv3がリリース。バウンディングボックスの推定はYOLO9000を下地に各ボックスについてオブジェクトが収まっていそうかを計算し閾値で絞り込んでいる。クラスはsoftmaxでなく個別に線形回帰で予測、スケールごとのボックス設定、特徴抽出用ネットワークの改善といった工夫が盛り込まれている
696	Ranking Sentences for Extractive Summarization with Reinforcement Learning	https://arxiv.org/abs/1802.08636	Shashi Narayan, Shay B. Cohen, Mirella Lapata	文書中の各文に対する重要度のランク付けを強化学習で学習するという試み。文はCNNでエンコード、文書についてはこれをLSTMに逆順で入力し表現を作成。文書エンコードを初期値とし、文ベクトルを入力としたLSTMの出力を元にスコアを算出し、ROUGE-Lを報酬とし強化学習する。
697	Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings	https://arxiv.org/abs/1803.08495	Kevin Chen, Christopher B. Choy, Manolis Savva, Angel X. Chang, Thomas Funkhouser, Silvio Savarese	自然言語から3D画像の検索、また生成を試みた研究。3D画像とその説明のペアについて、画像はCNN・テキストはRNN(GRU)でエンコードし、言語=>3D=>言語の復元変換を学習する(言語・3Dを同じカテゴリの別のものと差し替えても学習)。また異なるカテゴリが遠くなるよう学習する。
698	Deformable Convolutional Networks	https://arxiv.org/abs/1703.06211	Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei	CNNで畳み込みを行う際はどの場所でも同じフィルターを使うが、高次の特徴になるにつれその範囲で区切れているとは限らなくなる。そこで畳み込みを行う際に畳み込むポイントを当初のフィルターからずらし、またずらしの幅も同時に学習する手法を提案。
699	Universal Sentence Encoder	https://arxiv.org/abs/1803.11175	Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil	転移学習用の文エンコーダーを開発。精度は高いがリソースも食うTransformerモデルと、精度は劣るが効率が良いDAN(単語とbigramのベクトルを平均して伝搬するだけのモデル)を提供。 SkipThought的な前後文予測、対話、文書分類でマルチタスク学習をすることで作成している。
700	Neural Text Generation: Past, Present and Beyond	https://arxiv.org/abs/1803.07133	Sidi Lu, Yaoming Zhu, Weinan Zhang, Jun Wang, Yong Yu	テキスト生成の手法についてのサーベイ。RNN言語モデル(教師あり)、強化学習、GANの3タイプが紹介されており、GANを中心に検証。教師ありの場合は訓練時は常に正解が与えられるが評価時はそうでない問題(exposure bias)があり、強化/GANはこれはないが学習が難しいという。
701	Actor-Critic based Training Framework for Abstractive Summarization	https://arxiv.org/abs/1803.11070	Piji Li† Lidong Bing‡ Wai Lam†	Actor-Criticを使った強化学習で生成型要約を行う
702	Synthesizing Programs for Images using Reinforced Adversarial Learning	https://deepmind.com/blog/learning-to-generate-images/	Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, Oriol Vinyals	画像を生成する際に、直接生成するのでなくレンダリングのプロセスを介することでより汎用性を挙げようという試み。画像=>描画コマンド列=>コマンドに従い描画=>画像を復元、という流れ。コマンドは描線の開始/終了点や線の引き方などで強化学習(エージェントはRNN)、生成全体はGANの仕組みで学習する
703	Learning Unsupervised Learning Rules	https://arxiv.org/abs/1804.00222	Luke Metz, Niru Maheswaranathan, Brian Cheung, Jascha Sohl-Dickstein	教師なし学習でネットワークの更新方法を学習させようという試み。分類タスクを少ないデータで学習できるようにするため、その背後で「学習方法」を教師なしで学習する。更新に使う重みは全結合ネットワークで推定するが、通常の誤差逆伝搬でなく出力と勾配を畳み込んで重みの更新を計算する
704	Training Tips for the Transformer Model	https://arxiv.org/abs/1804.00247	Martin Popel, Ondej Bojar	Attention Is All You Needに代表されるTransformer系のモデルを学習させるために必要なトリックをまとめたもの。ハイパーパラメーターだけでなく、前処理についても触れられている。
705	Learning to Navigate in Cities Without a Map	https://arxiv.org/abs/1804.00168	Piotr Mirowski, Matthew Koichi Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, Raia Hadsell	Google Street Viewという現実に近い環境で、強化学習を使用し目的地までのパスを学習したという研究(強化学習はActor-Criticベース)。画像をCNNで畳み込んでLSTMで予測というのがベースだが、(ゴールを含めた)ランドマークへの距離を予測する補助タスクを解かせる。また都市間の転移学習を行っている
706	Online Multi-Label Classification: A Label Compression Method	https://arxiv.org/abs/1804.01491	Zahra Ahmadi, Stefan Kramer	マルチラベル分類の学習を行う際に、ラベルを小さな潜在空間に圧縮して学習することで効率を上げるという研究。モデルの学習はバッチと圧縮したラベルのペアで行い、圧縮したラベルの復元は元のラベルとの誤差を最小化する形で行う。
707	The Kanerva Machine: A Generative Distributed Memory	https://arxiv.org/abs/1804.01756	Yan Wu, Greg Wayne, Alex Graves, Timothy Lillicrap	VAE的な潜在構造を学習する手法。系統としてはボルツマンマシンの流れをくみ、タイトルにもあるKanervaの記憶行列は1988年のもの。この記憶から読み出し用変数yで生成用潜在変数zをとり、zからサンプルxを生成する生成モデルとしてVAEのマナーで学習させる。精度はもちろんFew-shotに強いとのこと
708	Model-Based Planning with Discrete and Continuous Actions	https://arxiv.org/abs/1705.07177	Mikael Henaff, William F. Whitney, Yann LeCun	強化学習で戦略を勾配で学習するには時間がかかることが多い。そこで、環境をモデル化(遷移関数と報酬関数を推定)することで各状態における適切な行動を導出し、それで学習をするというもの。行動が離散の場合はsoftmaxで確率値化＋学習しやすくするためノイズを加えている
709	Image Generation from Scene Graphs	https://arxiv.org/abs/1804.01622	Justin Johnson, Agrim Gupta, Li Fei-Fei	自然言語からの画像生成に関する研究。文内の単語関係をグラフ構造化し(Scene Graph)、各オブジェクト表現についてGraph Convolutionで畳み込みそれでもってオブジェクト領域/セグメンテーションを予測する。そこから生成を行い、GANのようにDiscriminatorを使用し学習する。
710	SHAPED: Shared-Private Encoder-Decoder for Text Style Adaptation	https://arxiv.org/abs/1804.04093	Ye Zhang, Nan Ding, Radu Soricut	要約を行う際に、文面のスタイルを加味して生成する試み。Gigawordに含まれる7社のそれぞれに応じた生成に挑戦している。Encoder-Decoderがベースで、スタイルごとにE/Dを持ち出力を元にスタイル分類と生成を学習。実行時は、各Decoderの出力を分類確率を元にマージする。
711	Interpreting Deep Classifier by Visual Distillation of Dark Knowledge	https://arxiv.org/abs/1803.04042	Kai Xu, Dae Hoon Park, Chang Yi, Charles Sutton	学習済みのモデルを利用しデータの特徴ベクトルを推定するという手法。大枠としては学習済み教師モデルの予測をシンプルな生徒モデルで再現する形だが、この時生徒モデルの入力を特徴ベクトル(y)とし、このy自体も学習する。yは、モデルの判断の解釈にも役立つ。
712	Modeling Semantic Plausibility by Injecting World Knowledge	https://arxiv.org/abs/1804.00619	Su Wang, Greg Durrett, Katrin Erk	ある事象が常識的にあり得るかを判定する試み(飴を飲み込むはあり得るが、ボールを、はありえない的な)。関係を持つ2つの対象についてサイズや硬さ等の特性を定義し、それらのレベル(大小etc)と関係(どちらが大きいetc)を特徴として利用。レベル定義には代表例を利用している
713	An End-to-end Neural Natural Language Interface for Databases	https://arxiv.org/abs/1804.00401	Prasetya Utama, Nathaniel Weir, Fuat Basik, Carsten Binnig, Ugur Cetintemel, Benjamin Httasch, Amir Ilkhechi, Shekar Ramaswamy, Arif Usta	自然言語をSQLに変換する研究。実用を目指している感じが伝わってきて、モデルはseq2seqでシンプルな一方データベーススキーマの情報から学習データを自動で生成する(テンプレート文を穴埋めする形で行う)、JOINやEXISTSなどのネストクエリの扱いを丁寧に扱っている。
714	Generative adversarial network-based approach to signal reconstruction from magnitude spectrograms	https://arxiv.org/abs/1804.02181	Keisuke Oyamada, Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, Nobukatsu Hojo, Hiroyasu Ando	Magnitudeのスペクトログラム(画像)から、GANを用いて音声を復元する研究(実際は、音声復元に欠けているphase(位相)情報を復元する。また既存手法による位相推定を入力に使うため生成というよりリファインに近い)。Discriminatorは画像面だけでなく、STFTを使い音声面の評価も加味するようになっている
716	A Survey on Neural Network-Based Summarization Methods	https://arxiv.org/abs/1804.04589	Yue Dong	ニューラルネットベースの要約手法のサーベイ
717	Multimodal Unsupervised Image-to-Image Translation	https://arxiv.org/abs/1804.04732	Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz (Cornell University, NVIDIA)	GANを用いたドメイントランスレーションの研究。
718	Simple random search provides a competitive approach to reinforcement learning	https://arxiv.org/abs/1803.07055	Horia Mania, Aurelia Guy, Benjamin Recht	強化学習において、シンプル＋再現可能＋パラメーター変更に頑健なベースラインの構築を目指した研究。ランダムサーチをベースに、報酬の分散・トップNを使用した更新幅の安定化や、状態の正規化といった工夫を加えMuJoCoの人型モデル操作でSOTAの結果を出している。
719	Imagine This! Scripts to Compositions to Videos	https://arxiv.org/abs/1804.03608	Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, Aniruddha Kembhavi	動画の再構成を行う研究。与えられたテキストの中のエンティティ(人物/モノなど)の位置を推定し、そこに既存の動画から適切なパーツを切り取り配置するという形。背景についても、適切なパートを切り取る。パーツの検索はテキスト/生成済み画像をたたみ込んで作ったベクトルで動画DBを参照する形で行う
720	BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages	https://arxiv.org/abs/1710.02187	Benjamin Heinzerling, Michael Strube	サブワードレベルでの分散表現の有効性を検証した研究。これにより未知語への対応が容易になり、エンティティタイプの推定では一部言語では既存分散表現よりも高い精度。275言語の学習済みモデルも提供されている(日本語もあり)。
721	The unreasonable effectiveness of the forget gate	https://arxiv.org/abs/1804.04849	Jos van der Westhuizen, Joan Lasenby	LSTMにおいてforget gateが重要なことは知られているが、だったらforget gateだけでよくない？とした研究。時系列の長さを考慮した初期化(chrono initializer)を組み合わせ、同等どころか通常のLSTMを上回る結果を得る。
722	ListOps: A Diagnostic Dataset for Latent Tree Learning	https://arxiv.org/abs/1804.06028	Nikita Nangia, Samuel R. Bowman	あるモデルが、文法のようなツリー構造を正しくパースできるかを診断するためのデータセット(ListOps)を作成したという研究。検証では、ベースラインのLSTM/TreeLSTMがタスクをこなす一方(精度はLSTM < TreeLSTM)、LSTMにすら勝てないモデルも発見された(これには、SNLIでトップを取ったモデルも含む)
723	Evolved Policy Gradients	https://storage.googleapis.com/epg-blog-data/epg_2.pdf	Rein Houthooft, Richard Y. Chen, Phillip Isola, Bradly C. Stadie, Filip Wolski, Jonathan Ho, Pieter Abbee	強化学習の転移について、学習済み戦略の転移ではなく損失関数からアプローチしている研究。仕組み全体としては、戦略はSGDで学習する通常のものだが、その損失はエージェントの行動履歴を畳み込む形で計算され、損失関数の学習は進化戦略で行われる。転移は異なるドメインへはまだ難しいとのこと
724	Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play	https://arxiv.org/abs/1703.05407	Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, Rob Fergus	強化学習におけるカリキュラムラーニングを教師なしで行う手法。カリキュラムを出す側は学習側がタスクを解くのにステップがかかるほど報酬が高いが、制限ステップ内に解けなかった場合0(つまりギリギリ解けるが最良)。この枠組みで出す側・解く側をそれぞれ学習させる。
725	Towards a Neural Network Approach to Abstractive Multi-Document Summarization	https://arxiv.org/abs/1804.09010	Jianmin Zhang and Jiwei Tan and Xiaojun Wan	複数ドキュメントに対する生成型要約モデル
726	Cascade R-CNN: Delving into High Quality Object Detection	https://arxiv.org/abs/1712.00726	Zhaowei Cai, Nuno Vasconcelos	Faster R-CNNのRPN以降の部分を多段にしたCascade R-CNNを提案。段階的にIoU閾値を上げながらBounding Boxをリファインしていくことで高いRecallとIoUを両立。
727	NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations	https://arxiv.org/abs/1804.07209	Marco Ciccone, Marco Gallieri, Jonathan Masci, Christian Osendorfer, Faustino Gomez	入力に応じて適応的に学習を行うためのアーキテクチャの提案。通常の伝搬では隣のレイヤから入力をもらうしかないが、提案構造では単一の層(x)を、任意の回数(K)、skip-connection＋毎回入力を適用することで伝搬していく。こうして作成されるブロックを連結してネットワークを構成する。
728	Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer	https://arxiv.org/abs/1804.06437	Juncen Li, Robin Jia, He He, Percy Liang	文のスタイルトランスファーを行う研究。具体的には、ネガティブな文を意味を維持したままポジティブな文にするなど。属性と関連が深いワード(n-gram)を頻度から特定し、関連語とそれ以外に分けた後、それ以外/転移先の関連語をそれぞれRNNでエンコードした結果を結合しデコーダーから生成を行う。
729	MADE: Masked Autoencoder for Distribution Estimation	https://arxiv.org/abs/1804.06437	Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle	AutoEncoderと自己回帰モデルを組み合わせて、生成モデルを作成する研究。自己回帰モデルは当然予測する先のデータに依存してはならないが、AutoEncoderの入力には全データが入ってくる。このため、マスクをかけて先のデータが伝搬しないようにする。シンプルな構成で扱いやすい。
730	Reinforced Co-Training	https://arxiv.org/abs/1804.06035	Jiawei Wu, Lei Li, William Yang Wang	教師なしデータに対する、協調学習を行う試み。片方の分類機がつけたラベル(と少量の教師データ)でもう片方が学習し、というのを交互に繰り返していく。分類機に与える教師なしデータに対するサンプリングバイアスを防ぐ＋学習に効果的なデータを与えるために、強化学習を使用している。
731	QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension	https://arxiv.org/abs/1804.09541	Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, Quoc V. Le	質問回答のモデルを、CNN+Attentionのみで作成したという研究。これにより、学習/推論速度が向上し精度も高まった。入力ベクトルには単語以外に文字も使用し、畳み込みはCNN+Self Attention+全結合で行っている。また、データの水増しとして翻訳モデルを使用している(英語=>仏語=>英語に戻して水増し)
732	Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies	https://arxiv.org/abs/1804.09541	Max Grusky	テキスト要約向けの新しいデータセット「NEWSROOM」
733	Style Transfer Through Back-Translation	https://arxiv.org/abs/1804.09000	Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, Alan W Black	文の意図や意味の改変を行わずに言い回しを変えるテキストにおけるStyle Transferの研究
734	Realistic Evaluation of Deep Semi-Supervised Learning Algorithms	https://arxiv.org/abs/1804.09170	Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, Ian J. Goodfellow	半教師あり学習について、効果を測定するための実験条件をそろえようという提言。ベースラインとしての純粋な教師あり、転移学習の必要性や、教師ありのデータに対する教師なしデータの分布や量の違いをきちんと述べることなどが上げられている。検証された半教師ありの全てが転移学習に負ける結果。
735	Measuring the Intrinsic Dimension of Objective Landscapes	https://arxiv.org/abs/1804.08838	Chunyuan Li, Heerad Farkhoor, Rosanne Liu, Jason Yosinski	DNNにおいて、実際問題を解くのに必要なパラメーターの次元はどの程度なのかを計測することを試みた研究。更新可能なパラメーターの次元を制約して最適化を行う、というプロセスを徐々に制約を上げながら繰り返すことで、本当に必要な次元数を洗い出す。以下は、ネットワークのパラメーター空間が3次元(D=3)の場合、まず二次元に制約し更新する様子を表している(θ^dは二次元で、Pをかけてd=>Dとする(Pは値固定の純粋に転写のために使われる行列))。
736	Word Embedding Perturbation for Sentence Classification	https://arxiv.org/abs/1804.08166	Dongxu Zhang, Zhichao Yang	自然言語におけるData Augmentationに挑戦した研究。具体的には、単語ベクトルにノイズを加えたり、意図的に単語を落とす等の処理を行い、その効果を見ている。結果としては、単語単位でノイズをかけたり落としたりする処理(重みを0にしたり単語を欠損させたり)よりも、ガウシアンノイズなどの連続的なノイズの方が良かったとのこと。
737	Multi-Mention Learning for Reading Comprehension with Neural Cascades	https://arxiv.org/abs/1711.00894	Swabha Swayamdipta, Ankur P. Parikh, Tom Kwiatkowski	一文書の中に答えの箇所がある、という単純な形式でなく複数文書に解答箇所が散在するようなタスク(TriviaQA)に挑戦した研究。単純な全結合のネットワークを階層上に積み上げ最上位で全体を見て判断する構成で、特徴量も単語頻度とAttention(質問文との関連で重みをつける)というシンプルなもの。
738	GAGAN: Geometry-Aware Generative Adversarial Networks	https://arxiv.org/abs/1712.00684	Jean Kossaifi, Linh Tran, Yannis Panagakis, Maja Pantic	オブジェクトの形や回転・移動などを明示的に扱うことのできるGAGANを提案。オブジェクトの形状データをベクトルに埋め込み、それと分布からサンプリングされたベクトルをconcatしたzから画像を生成する。生成画像や元画像は形状データを使って標準的な向き・位置に揃えられたのちDiscriminatorに入力される。
739	Extreme Adaptation for Personalized Neural Machine Translation	https://arxiv.org/abs/1805.01817	Paul Michel	話者の属性に合わせた翻訳文を生成する
740	Sentence-State LSTM for Text Representation	https://arxiv.org/abs/1805.02474	Yue Zhang , Qi Liu and Linfeng Song	BiLSTMでは1つのdirectionの計算に1つの隠れ層の状態しか使用できず、局所情報を明示的にモデリングできていない
741	Stack-Pointer Networks for Dependency Parsing	https://arxiv.org/abs/1805.01087	Xuezhe Ma	Pointer-NetにStackを取り付けたDependency parseを行うモデル
742	Frustratingly Easy Meta-Embedding -- Computing Meta-Embeddings by Averaging Source Word Embeddings	https://arxiv.org/abs/1804.05262	Joshua Coates, Danushka Bollegala	分散表現について、異なる手法で作られた分散表現を単純に平均/結合するだけで改善することができるとした研究。結合の方が良好だが次元数が多くなってしまう一方、平均でも改善が可能なことを確認(1~2pt落ちる)。
743	Sentence-State LSTM for Text Representation	https://arxiv.org/abs/1805.02474	Yue Zhang, Qi Liu, Linfeng Song	双方向LSTMの弱点(並列計算が難しい、局所特徴が明示的にモデル化されてない、長文に弱いなど)を克服するための構造の提案。局所特徴(自身含む前後の単語の隠れ層を結合したもの)とグローバルな特徴(系列全体の平均から算出)を順次更新していく形のネットワークになっている
744	The Statistical Recurrent Unit	https://arxiv.org/abs/1805.02474	Junier B. Oliva, Barnabas Poczos, Jeff Schneider	移動平均で隠れ層を更新していくというシンプルな方法で、LSTM/GRUと同等/それ以上の性能を獲得できたという研究。
745	Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens	https://arxiv.org/abs/1805.02214	Marek Rei, Anders Sgaard	Zero-shotで系列ラベリングを行うという研究。Bi-directional+Attentionで文分類を行う分類機のAttentionを組み合わせてトークンのラベルを予測させる。文単位の分類機があれば、学習なし(Zero-shot)でトークン単位の予測が可能としている。
746	Sim-to-Real: Learning Agile Locomotion For Quadruped Robots	https://arxiv.org/abs/1804.10332	Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke	シミュレーターで学習(deep RL)した歩行コントローラーをfine tuningなしで実ロボットに適用できた。学習時にノイズを加えるだけでなく、丁寧にシステム同定をしたり、Bulletを改良してモデルを現実に近づけたり、観測空間の次元を小さくしたりしている。4脚ロボットMinitaurで実験。
747	Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification	https://arxiv.org/abs/1805.02220	Yizhong Wang, Kai Liu, Jing Liu, Wei He, Yajuan Lyu, Hua Wu, Sujian Li, Haifeng Wang	質問に対し、複数の文書の中から正しい回答範囲を抽出する研究。回答の始点/終点推定に加えさらに各単語が回答範囲に含まれるか否かを予測し、最後は各文書の回答候補をAttentionの仕組みでスコア付けして選択する。MS-MARCOで、抽出型とはいえROUGE-Lは人手に匹敵する値
748	Training Classifiers with Natural Language Explanations	https://arxiv.org/abs/1805.03818	Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, Christopher R	アノテーションを行う際ラベルだけでなく理由を入れてもらい、その理由をラベル付け関数に変換してアノテーションを効率化するという研究。ラベル付け関数への変換はルールベースで行われている(xxの時はyyだから=> if xx then yy、のような)。作成した関数は真のラベルとの相関で重みづけする
749	Real-Time Data-Driven Interactive Rough Sketch Inking	http://hi.cs.waseda.ac.jp/~esimo/ja/research/inking/	EDGAR SIMO-SERRA, SATOSHI IIZUKA, HIROSHI ISHIKAWA	ラフスケッチに対するペン入れをサポートする技術の研究。これにより切れた線を繋ぐツール(インカーペン/インカーブラシ)、主要な線以外を消すツール(スマートイレイサー)を開発している。構成はEnd2Endだが、学習データが多く用意できないため前処理(線の均質化)/Augmentationなどを工夫している。
750	Universal Language Model Fine-tuning for Text Classification	https://arxiv.org/abs/1801.06146	Jeremy Howard, Sebastian Ruder	テキスト分類における転移学習を行った研究。言語モデルで基礎となるモデルを作り、その後ターゲットドメインの文書で学習した後分類を学習する。転移を行う際にレイヤ毎の学習率を適用する、学習率を一気に上げ徐々に下げる、後ろの層から徐々に解凍していくといった工夫がとられている。
751	From Word to Sense Embeddings: A Survey on Vector Representations of Meaning	https://arxiv.org/abs/1805.04032	Jose Camacho-Collados, Mohammad Taher Pilehvar	単語分散表現の場合一つの単語は一つのベクトルで表現されるが、本来単語は多様な意味を持つ。そうした「意味」の表現の獲得を目指した研究のサーベイ。教師なし/知識ベースの大きく2つに分けて解説されており、その比較についても記載されている。
752	Faithfully Explaining Rankings in a News Recommender System	https://arxiv.org/abs/1805.05447	Maartje ter Hoeve, Anne Schuth, Daan Odijk, Maarten de Rijke	ニュースなどをユーザーにレコメンドする際に、なぜその順序で出てきているのかの説明を出力するという研究。ランキング関数の特徴のうち、特徴を変化させることでランキングに大きな変化が起こるものを見つけ、その特徴の内容でもって説明を行うというアプローチ。
753	Hyperspherical Variational Auto-Encoders	https://arxiv.org/abs/1804.00891	Tim R. Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, Jakub M. Tomczak	VAEで使用しているガウス分布をvon Mises-Fisher(vMF)分布に変更することで、超球上の構造を持つデータにも対応できるようにしたという研究。
754	Connecting Generative Adversarial Networks and Actor-Critic Methods	https://arxiv.org/abs/1610.01945	David Pfau, Oriol Vinyals	GANとActor Criticは構成が似ているので、双方の学習安定化の試みが互いに応用できるのではないかという提案。GANとActor Criticの違いは、Actor/Generatorが状態(s_t/x)を受け取るかの違いであるとしている(Generatorはxを受け取らず、zから生成する)。
755	A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors	https://arxiv.org/abs/1805.05388	Mikhail Khodak, Nikunj Saunshi, Yingyu Liang, Tengyu Ma, Brandon Stewart, Sanjeev Arora	単語ベクトルを周辺語のベクトルから予測させる学習を行うことで、未知語や低頻度語が登場しても周辺のベクトルから逆算できるようにする試み。予測は周辺単語ベクトルの平均に重みをかける(線形変換)というシンプルなもの。性能を測るための新しいデータセット(CRW)も提案。
756	SmoothOut: Smoothing Out Sharp Minima for Generalization in Large-Batch Deep Learning	https://arxiv.org/abs/1805.07898	Wei Wen, Yandan Wang, Feng Yan, Cong Xu, Yiran Chen, Hai Li	パラメタにノイズを加えてから勾配を求めることでシャープな解に陥りにくくするSmoothOutという手法を提案。以下の図がわかりやすい。
757	Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context	https://arxiv.org/abs/1805.04623	Urvashi Khandelwal, He He, Peng Qi, Dan Jurafsky	RNNではコンテキストの情報が活かされるとよく言われるが、実際どれくらい過去の単語がどう加味されているのかを調査した研究。過去200語くらいまでを見ていて、直近50が特に意識されるとのこと。直近50は語順が重要だが、それ以降はあまり重要でない(アバウトに記録されている)という結果。
759	Self-Supervised Representation Learning for Continuous Control	http://www.cs.unm.edu/amprg/Workshops/MLPC18/submissions/paper_10.pdf	Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet	強化学習において補助タスクにより精度を上げた研究。具体的には、同じ時間にとられた異なる視点の画像ペアはPositive、異なる時間にとられた画像ペアはNegativeとして学習を行う(比較に使うベクトルは時系列フレームを畳み込む)。これを既存アルゴリズム(PPO)の特徴として使うことで性能向上を確認。
760	Self-Attention Generative Adversarial Networks	https://arxiv.org/abs/1805.08318	Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena	既存のGANはCNNベースのため局所特徴に依存しており、離れた場所の情報を参照することができない。そのため、Attentionの仕組みを導入して離れた局所特徴を重みをかけて参照できるようにする手法。局所特徴とAttention情報の利用の度合いは、係数でもって調整を行う。
761	A Universal Music Translation Network	https://arxiv.org/abs/1805.07848	Noam Mor, Lior Wolf, Adam Polyak, Yaniv Taigman	音楽のスタイル変換を試みた研究。モデルはWaveNet形式のEncoder/Decoderで、Encoderは共用でDecoderが楽器ごとに分かれる(正則化として、Encode結果からどの楽器か推定できない=楽器独立の特徴を捉えるよう矯正する項を入れている。Domain Confusionと呼ばれる手法)。実験では変換した音に対する専門家の評価等を行っている。
762	FollowNet: Robot Navigation by Following Natural Language Directions with Deep Reinforcement Learning	https://arxiv.org/abs/1805.06150	Pararth Shah, Marek Fiser, Aleksandra Faust, J. Chase Kew, Dilek Hakkani Tur	ロボットを自然言語でナビゲートする。
763	Global-and-local attention networks for visual recognition	https://arxiv.org/abs/1805.08819	Drew Linsley, Dan Scheibler, Sven Eberhardt, Thomas Serre	画像認識のネットワーク内にAttentionを導入し認識精度を上げた研究。低次元への圧縮(shrink)/復元(expand)が基本処理で、これを特徴マップのチャンネル単位(グローバル)/領域単位(ローカル)双方で行い結合してAttentionマップを作る。これを人の認識と近づけるための作業も提案し顕著な精度向上を確認
764	AutoAugment: Learning Augmentation Policies from Data	https://arxiv.org/abs/1805.09501	Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, Quoc V. Le	最適なData Augmentationを探索する研究。画像の切断や反転・回転といった16の操作について、操作のパラメーター(回転の度合いや輝度など)、適用確率を離散化(それぞれ10、11)。2操作がワンセットで、それを5つ束ねたものが最終的な処理になり、これを強化学習で探索する(探索空間は3溝ほどにも及ぶ)。
765	Hyperbolic Attention Networks	https://arxiv.org/abs/1805.09786	Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, Nando de Freitas	双曲空間内でAttentionを行うことで構造を上手く表現できるようにする試み。グラフ構造における推論とTransformerにおけるSelf-Attentionが本質的に等価であるとし(ノードの重み=Attentionの重み)これが(階層構造の表現力の高い)双曲空間上でも実現できるとした。性能は微増
766	Do Better ImageNet Models Transfer Better?	https://arxiv.org/abs/1805.08974	Simon Kornblith, Jonathon Shlens, Quoc V. Le	ImageNetで強いモデルは他のタスクでも強いのか？という点を調査した論文。転移学習した場合は強い相関があるが、重み固定の特徴抽出機として使う場合はResNetが(他の精度の高いモデルよりも)優秀という評価。ImageNetで強い構造は他のタスクでも優秀という。
767	Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition	https://arxiv.org/abs/1805.11686	Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, Sergey Levine	完全なデモンストレーションが無い場合の逆強化学習としてイベントというタスク上のゴールを推定する手法(VICE)の提案。イベントという概念を用いることで従来の逆強化学習を一般化した手法となっており、エキスパートとエージェントによる行動を識別する識別器を用いたイベント発生確率の学習と、イベント発生確率を報酬に用いたエージェントの方策の学習とを交互に行う。
768	You Only Look Twice: Rapid Multi-Scale Object Detection In Satellite Imagery	https://arxiv.org/abs/1805.09512	Adam Van Etten	物体検知を衛星写真のようなピクセル数が多く検出対象が極めて小さい画像に適用する手法の提案。YOLOをベースに、中間のレイヤで処理した結果を最終畳み込みの前に結合するパスを追加している。また画像をパッチに分けて検出し、結果を結合することで領域を特定する。
769	Theory and Experiments on Vector Quantized Autoencoders	https://arxiv.org/abs/1805.11063	Aurko Roy, Ashish Vaswani, Arvind Neelakantan, Niki Parmar	VQ-VAE(
770	Playing hard exploration games by watching YouTube	https://arxiv.org/abs/1805.11063	Yusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang, Nando de Freitas	強化学習を行う際に、YouTubeのプレイ動画をお手本にするという研究。実環境と動画はフレームレートなどが異なるため、動画内の画面のペアをとりその離れ具合を教師なしで学習し、抽象的な位置表現を獲得(音でも実施)。学習時はエージェントとプレイ動画の位置表現が近い場合に追加報酬を与える。
771	How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)	https://arxiv.org/abs/1805.11604	Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry	Batch Normalizationは有効だがそれは共変量シフトを抑えるからではないという話。BNを導入したネットワークに意図的に共変量シフト(レイヤ出力の平均/分散を変動)させても性能に変化がないことを確認(=そもそも共変量シフトは性能に影響ない)。真の効果は(正規化により)出力に対する勾配の変動を抑制し滑らかにする点という
772	AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference	https://arxiv.org/abs/1805.08941	Jian-Hao Luo, Jianxin Wu	モデルのダウンサイズを、ダウンサイズしてから(精度を再度上げるため)再学習という2段階でなく同時に行う研究。具体的には、CNN内のフィルタ(=チャンネル)にマスクをかけつつ学習を行う。マスクのかけ具合(=圧縮率r)と、圧縮or精度の優先度(λ)を指定しつつ圧縮が可能。
773	MolGAN: An implicit generative model for small molecular graphs	https://arxiv.org/abs/1805.11973	Nicola De Cao, Thomas Kipf	化学物質のグラフ構造をGANを使って生成する研究。単純な生成だけでなく、強化学習を使い生成された化学構造が特定の性質を持つ場合に報酬を与え(化学構造の特性自体は外部のシステムで判定する)、有効な構造が生成されるようにしている。
774	Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models	https://arxiv.org/abs/1805.12114	Kurtland Chua, Roberto Calandra, Rowan McAllister, Sergey Levine	強化学習でモデルベースでDNNのモデルフリーと同等の性能が出せたとする研究。状態をサンプリングして軌跡を生成するのを確率的NN(パラメーターが重みで表現された分布)のアンサンブルで行い(PE)、行動は状態を見ず行動系列を生成する(CEM)。獲得報酬でCEM、最適系列の行動/状態からPEを学習していく。
775	Learning a Prior over Intent via Meta-Inverse Reinforcement Learning	https://arxiv.org/abs/1805.12573	Kelvin Xu, Ellis Ratner, Anca Dragan, Sergey Levine, Chelsea Finn	逆強化学習では、少ないサンプルから報酬を推定しないといけない点がネックになっている。そこで様々なタスクから「報酬の推定の仕方」を学んでおき、学習速度を上げるというメタラーニングと組み合わせた手法の提案。該当タスク学習のための勾配(Inner)を、メタ側(Outer)から提供する形で更新する。
776	DeepProbLog: Neural Probabilistic Logic Programming	https://arxiv.org/abs/1805.10872	Robin Manhaeve, Sebastijan Dumani, Angelika Kimmig, Thomas Demeester, Luc De Raedt	DNNの認識モデルを使い論理演算を行うことを試みた研究。1・4という数字画像を加算する場合digit(1)+ digit(4)と言う風に書けるようにする(digitが数字画像認識モデル)。モデルも含め式と演算結果から学習する(勾配を確率に沿い伝搬する)。ProbLogという言語(Pythonから使える)をベースに開発
777	InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets	https://arxiv.org/abs/1606.03657	Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel	GAN の潜在空間の解釈可能性を上げる研究。潜在変数の部分集合 c と生成画像との間の相互情報量を最大化することでベクトル c が画像の重要な特徴に対応することを期待する。
778	BAGAN: Data Augmentation with Balancing GAN	https://arxiv.org/abs/1803.09655	Giovanni Mariani, Florian Scheidegger, Roxana Istrate, Costas Bekas, and Cristiano Malossi	GANを訓練・テストデータセットのデータ拡張に利用する研究。データセット数の少ないラベルの訓練時に他のラベルの特徴量を訓練に用いる点が特徴。
779	Strategic Object Oriented Reinforcement Learning	https://arxiv.org/abs/1806.00175	Ramtin Keramati, Jay Whang, Patrick Cho, Emma Brunskill	強化学習のモデルベースの手法で、ピクセルベースで状態を学習するのはきついため物体検知と組み合わせて、物体の遷移を予測する形でモデルを構築するアプローチ。物体検知=>遷移モデルの選択=>ツリー探索を行い(モデル内で)最大報酬の行動選択をとる、という形で学習を行う。
780	To Trust Or Not To Trust A Classifier	https://arxiv.org/abs/1805.11783	Heinrich Jiang, Been Kim, Maya Gupta	機械学習で、予測結果をどれぐらい信じるべきかについて新しい信頼度のスコアを作成したという研究。イメージ的にはモデルの予測結果とKNNの分類結果を比較するような形で、予測結果がデータから見てかけ離れたクラスを予測していないかをチェックする。
781	Unsupervised Learning of Style-sensitive Word Vectors	https://arxiv.org/abs/1805.05581	Reina Akama, Kento Watanabe, Sho Yokoi, Sosuke Kobayashi, Kentaro Inui	単語の分散表現を作成する際に、意味的なもの(What)と言い方的なもの(How)を分けて学習する試み。意味的なものは近傍単語、言い方は(近傍より)全体的に含まれるとの仮説から周辺「以外」から学習する方法と、周辺から意味・周辺より遠くから言い方を学習する手法を提案(学習はCBOWと同じ枠組み)。
782	Do CIFAR-10 Classifiers Generalize to CIFAR-10?	https://arxiv.org/abs/1806.00451	Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar	CIFAR-10を使って「良いモデル」を選択しているうちに、CIFAR-10のテストセット(上の精度)に過適合したモデルが生まれてきてしまっているのではないかという提言。CIFAR-10のデータに近い新しいテストセットを作って検証したところ、4%~10%ほどの精度低下がみられたという(ResNet/VGGで8%近く落ちる)。
783	Relational inductive biases, deep learning, and graph networks	https://arxiv.org/abs/1806.01261	Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu	今後より人間に近しいタスクを行っていくには、推論方法をより一般化したものにしていく必要があるという提言。CNNは局所的な情報から、RNNは系列的な情報からしか推論できないため、グラフ型が適しているとしている。そこでグラフネットワークを新しい構造単位として使うための定義を行っている。
784	Relational recurrent neural networks	https://arxiv.org/abs/1806.01822	Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap	メモリネットワークを改良し、LSTMに組み込んだ研究。メモリは行列で表現され、入力を含めた各行に対しクエリ(Q)とキー(K)・値(V)を算出し、クエリとキーの近さで重み(=Attention)を計算し値に乗じて更新を行う(softmax(QK^T)V)。言語モデルなどの教師ありと、強化学習で効果を確認。
785	Neural Relational Inference for Interacting Systems	https://arxiv.org/abs/1802.04687	Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, Richard Zemel	グラフニューラルネット(GNN)でVAEを行ったという研究。ノードをたどるパスを入力とし、GNNで畳み込み、潜在表現の分布からサンプルした結果＋前回のパスからNステップのパスをGNNで予測する。これにより教師なしで物理的な動き(関節のモーションなど)の予測と表現の獲得を行っている。
786	Playing Atari with Six Neurons	https://arxiv.org/abs/1806.01363	Giuseppe Cuccu, Julian Togelius, Philippe Cudre-Mauroux	強化学習でたった6~18ノードでAtariを攻略した研究。辞書のような構造で状態を離散表現に変換し、数ニューロンの素のRNNで行動を選択する。学習は進化戦略で行う。辞書は新規の状態が既存の辞書の値と大きく異なる場合追加されるようになっており、そのためRNNへの入力サイズも学習中に変動する。
787	Adversarial Training Methods for Semi-Supervised Text Classification	https://arxiv.org/abs/1605.07725	Takeru Miyato, Andrew M. Dai, Ian Goodfellow	テキスト分類を行う際に、単語のone-hotではなく埋め込み表現に摂動を挿入し正則化効果を得る手法。加える摂動は分類を邪魔できたかと、元のデータとどれだけ離せたかの2種で検証(前者は教師ラベルが必要だが、後者は不要になる)。教師なしの後者でも十分な効果があることを確認。また、併用も可能。
788	Perturbative Neural Networks	https://arxiv.org/abs/1806.01817	Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides	従来の畳み込み層の代替えになりうる摂動層を提案。摂動層は畳み込み層よりも少ないパラメータで学習が可能。また、MNIST、CIFAR-10、PASCAL VOCおよびImageNetにおいてCNNと同等の性能を発揮する。
789	Deep Variational Reinforcement Learning for POMDPs	https://arxiv.org/abs/1806.02426	Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, Shimon Whiteson	POMDPで環境をモデル化し、POMDPの信念状態をもとにA2Cを行う強化学習。POMDPは状態観測が不完全な場合(たまにセンサー途切れるなど)に使われるモデル化の手法で、そのため状態は「そう思われる状態=信念状態(Belief State)と呼ばれる」。A2Cのn-stepを複数まとめてbackpropすることで性能を上げている
790	Improving Language Understanding by Generative Pre-Training	https://blog.openai.com/language-unsupervised/	Alec Radford. Karthik Narasimhan, Tim Salimans, Ilya Sutskever	Transformerベースの言語モデルを事前学習し、文関係や文類似度・質問回答といったタスクに対し転移学習でSOTAを更新できたという研究。トークンは単語でなくbytepairを使用し、事前学習にはBooksCorpusを使用している(本のデータで、コンテキストが長い)。転移学習時には言語モデルの損失を加えている
791	A Simple Method for Commonsense Reasoning	https://blog.openai.com/language-unsupervised/	Trieu H. Trinh, Quoc V. Le	事前学習した言語モデルを使用し代名詞の解決問題に答える研究。パソコンを鞄に入れようとしたが「それ」が大きすぎて入らなかった、という時「それ」がパソコンか鞄かを回答する形で、言語モデルを使用し「それ」を回答候補に置き換えた場合の文全体/置き換えた以後の単語の出現確率の変動を見る
792	Graph Convolutional Neural Networks for Web-Scale Recommender Systems	https://arxiv.org/abs/1806.01973	Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec	Graph Convolutionをプロダクションレベル(30億ノード！)で使用したという金字塔的な論文(Pinterestで使われている)。グラフサイズが膨大なので近傍ノードをサンプリング(ランダムウォーク)で収集しており、収集はCPU・畳み込みはGPUで役割分担しMapReduceで分散処理して高速化している。
793	Classifier-agnostic saliency map extraction	https://arxiv.org/abs/1805.08249	Konrad Zolna, Krzysztof J. Geras, Kyunghyun Cho	通常のSaliency Map(モデルの着目点を示したマップ)は分類機とペアで作られるが、それはその分類機にとってのマップでしかない、という問題提起から分類機独立のマップを作る試み。ストレートに複数の分類機共通のマップを作るのは現実的でないため、学習過程の分類機を使用しモデルと同時に学習させる
794	Attention Solves Your TSP	https://arxiv.org/abs/1803.08475	W.W.M. Kool, M. Welling	巡回セールスマン問題をAttention+強化学習で解く手法。Encoderは周辺ノードの情報を畳み込み、DecoderはEncoder出力/前回ノード/初期ノードの3つを元に次のノードを選択する(Policyとして動作する)。Encoderで畳み込む際に、TransformerベースのAttentionを使用する。学習はPolicy Gradientの形。
795	QuaterNet: A Quaternion-based Recurrent Model for Human Motion	https://arxiv.org/abs/1805.06485	Dario Pavllo, David Grangier, Michael Auli	RNNを使用して3次元のボーンの動きを学習させる研究。動きの角度表現(腕を30度傾けるなど)には3次元のオイラー角を使用するのが一般的だが、オイラー角は一周(2π)すると値が元にもどるため不連続であるという扱いにくさがあった。そこで4次元で3次元空間の回転を表現するQuaternionを使用している。
796	Improved Regularization of Convolutional Neural Networks with Cutout	https://arxiv.org/abs/1708.04552	Terrance DeVries, Graham W. Taylor	画像認識におけるData Augmentationの手法で、画像の一部(矩形領域)をマスクする(=Cutout)というとても簡単な方法(を既存のAugmentationに組み込むこと)でCIFAR10・100、SVHNでSOTAを更新したという話。組み合わせているのはzero-pad/random crop/水平のフリップ+今回のCutouのみ。
797	A Probabilistic U-Net for Segmentation of Ambiguous Images	https://arxiv.org/abs/1806.05034	Simon A. A. Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R. Ledsam, Klaus H. Maier-Hein, S. M. Ali Eslami, Danilo Jimenez Rezende, Olaf Ronneberger	画像のセグメンテーションでは「このピクセルはxx」と決定的なのが一般的だが、確率的な場合もある(医療画像における腫瘍かも？など)。そこでVAEの仕組みを用い、U-Netで特徴抽出＋潜在分布からのサンプリング結果で領域の推定を行う。潜在分布を入力/実領域から作成した事後分布とのKL距離で学習する
798	RTSeg: Real-time Semantic Segmentation Comparative Study	https://arxiv.org/abs/1803.02758	Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand	セグメンテーションについては精度が注目されがちだが、速度の方に注目して各手法の比較を行ったサーベイ
799	Neural scene representation and rendering	https://deepmind.com/blog/neural-scene-representation-and-rendering/	S. M. Ali Eslami, Danilo J. Rezende, Frederic Besse, Fabio Viola, Ari S. Morcos, Marta	シーンを認識で、A地点/B地点からの画像から、C地点からの画像を推定するというタスクを解かせた研究。各地点からの画像をネットワークで潜在表現に圧縮し、クエリ(C地点)と潜在表現から画像を生成する。これにより2Dから3Dへのレンダリングや、強化学習での学習効率の向上が行えたとのこと。
800	Hierarchical Imitation and Reinforcement Learning	https://arxiv.org/abs/1803.00590	Hoang M. Le, Nan Jiang, Alekh Agarwal, Miroslav Dudk, Yisong Yue, Hal Daum III	模倣学習を行う際に、タスクを階層的に考える手法の提案。例えば「学校まで行く」というのが高次の行動で「右に一歩進む」が低次の行動(通常のアクション)になる。各ゴールへの到達が失敗した場合のみエキスパートの行動を取り入れることで、模倣対象であるエキスパートの負担を下げることができる。
801	Deep Code Comment Generation	https://drive.google.com/file/d/14ta2a_ZNApQC1nFb5mPwTa6hlZ3xkm1E/view?usp=sharing	Xing Hu, Ge Li, Xin Xia, David Lo, Zhi Jin	Javaメソッドを対象とするコードコメントの自動生成する手法、DeepComの提案。プログラミング言語で書かれたソースコードを自然言語であるコメントに機械翻訳をするというアプローチをとっている。ソースコードから抽象木(AST)を生成し、それをSeq2Seqモデルの学習に用いることで従来の技術よりも良い結果を得ている。
802	Adversarial Contrastive Estimation	https://arxiv.org/abs/1805.03642	Avishek Joey Bose (1,2), Huan Ling (1,2), Yanshuai Cao (1)	単語埋め込みなどのパラメータ推定に用いられるNoise Contrastive Estimationの負例サンプリングにGANの仕組みを取り入れた。Conditional GANの一種と見ることができる。実験によってNCEと比較して早く収束することと応用タスクでの複数のメトリックが改善することが確認された。
803	Neural Translation of Musical Style	https://arxiv.org/abs/1708.03535	Iman Malik, Carl Henrik Ek	音楽のスタイル変換を試みた研究。Encoder-Decoderが基本モデルで、Encoderは音楽のジャンルによらない表現を学習し、Decoderは各ジャンルごとの音楽の生成を学習させるというスタイル。
804	Mixed Precision Training	https://arxiv.org/abs/1710.03740	Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu	計算速度の向上ために半精度(FP16)、学習精度維持のために単精度(FP32)をうまく組み合わせて学習する手法の提案。具体的には、重みはFP32で管理するが(勾配の値は小さいため、FP16だと消えてしまう可能性がある)、FP32をFP16に圧縮し、Forward/Backwardを行う。この時、lossの値が小さくFP16では消えてしまうことが多いため、スケーリングをした上でBackwardし、あとで戻す処理を行っている。
805	Taskonomy: Disentangling Task Transfer Learning	https://arxiv.org/abs/1804.08328	Amir Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik, Silvio Savarese	様々な画像関連のタスクが、どれくらい相互に転移しやすいかを調べた研究。全26の画像関連タスク(セグメンテーションやエッジ推定など)をそれぞれ個別に学習した後、他のどのタスクに転移しやすいか、また他のどの特徴と組み合わせた場合精度が高まるかを検証。これにより、タスク関連マップを作成した
806	RUDDER: Return Decomposition for Delayed Rewards	https://arxiv.org/abs/1804.08328	Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Sepp Hochreiter	強化学習では通常報酬を予測する形で学習を行うが、最終報酬から各ステップの即時報酬を逆算する(Return Decomposition)という、逆方向の手法を取った研究。
807	The Natural Language Decathlon: Multitask Learning as Question Answering	https://arxiv.org/abs/1806.08730	Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher	自然言語処理のタスク全10個を単一のモデルで解く新しい問題設定と評価手法の提案。タスクは要約・翻訳・質問回答など様々なものを含むが、フォーマットは質問回答の形に還元されている(要約なら、「この文の要約は？」になるなど)。マルチタスクで学習を行い、転移性能が確認できたとのこと。
808	Probabilistic FastText for Multi-Sense Word Embeddings	https://arxiv.org/abs/1806.02901	Ben Athiwaratkun, Andrew Gordon Wilson, Anima Anandkumar	サブワードを分布で表現し、単語をその集合(混合ガウス分布)で表現するという手法(サブワードは文字n-gramで作成する)。式はサブワードの組み合わせのみでなく、単語ベクトルを補完する形で定義されている。単語類似度の精度が上がったほか、未知語対応の効果が確認できた。
809	A Simple but Tough-to-Beat Baseline for Sentence Embeddings	https://openreview.net/forum?id=SyK00v5xx	Sanjeev Arora, Yingyu Liang, Tengyu Ma	単語ベクトルから文のベクトル表現を得る、シンプルだが強力な手法。単語ベクトルの平均を取るのが基本だが、TF-IDFライクに頻度が高い語については重みを下げる。そして、各文のベクトルが出そろった後全体(文数Xベクトルサイズ)の第一主成分要素をマイナスする。
810	Evaluation of sentence embeddings in downstream and linguistic probing tasks	https://arxiv.org/abs/1806.06259	Christian S. Perone, Roberto Silveira, Thomas S. Paula	ELMo(
811	Self-Imitation Learning	https://arxiv.org/abs/1806.05635	Junhyuk Oh, Yijie Guo, Satinder Singh, Honglak Lee	強化学習で、過去の良かった行動を模倣することで精度を上げる研究。通常の学習に加え、推定よりも高い報酬が得られた経験(R > V_theta)をサンプルしての学習を追加する(R - V_thetaでActor/Criticの学習を行う)。ただ探索が足りないと局所最適な行動に陥ることがありバランス設定が重要になるとのこと
812	GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations	https://arxiv.org/abs/1806.05662	Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen, Ruslan Salakhutdinov, Yann LeCun	自然言語において、Attentionの転移を試みた研究。これにより単語分散表現＋Attentionの転移と、単語＋その「組み合わせ方」についても転移を行うという併用が可能になる。Attentionの機構はCNNを使ったSelf-Attentionを多段に組んだような形。性能はいまいちだが、併用によるブーストは確認。
813	Insights on representational similarity in neural networks with canonical correlation	https://arxiv.org/abs/1806.05759	Ari S. Morcos, Maithra Raghu, Samy Bengio	ネットワーク間の類似性を調べるための手法の提案。類似度の手法は正準相関分析を基準としている(A, Bについて、相関(共分散)を最大化しつつA.Bそれぞれもよく説明するベクトルを発見する手法)。CNNでは汎化性能が高いほど似ている、RNNでは層が薄いほど入力=>出力の層が近づく速度が速いと言う結果
814	cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information	http://www.statnlp.org/wp-content/uploads/papers/2018/cw2vec/cw2vec.pdf	Shaosheng Cao, Wei Lu, Jun Zhou, Xiaolong Li	cw2vecは新しい単語分散表現方法。漢字は英語ワードと違う、漢字の形自体は情報がふくまれている。word2vec, gloveなどの手法は漢字の画（漢字を構成する点や線）を考えしていない。cw2vecは漢字を字画で表示し、n-gramの情報を学習する。cw2vecは今までの手法より、漢字の形態と構造情報（word morphological information）をよりよい学習した。
815	One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning	https://arxiv.org/abs/1802.01557	Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, Sergey Levine	One-shotで模倣学習を、しかもドメインを変えて行う研究。具体的には(ロボットのでなく)人の動作を一回見て模倣する。事前学習として人とロボットの動作のペア画像を用意し、戦略の初期値を人の画像から、初期値からの学習をロボットの画像から行う。これで人の画像から良い初期値を得る関数を学習する
816	Design Challenges and Misconceptions in Neural Sequence Labeling	https://arxiv.org/abs/1806.04470	Jie Yang, Shuailong Liang, Yue Zhang	系列ラベリング(固有表現認識やPOSTaggingなど)で良好な精度を出している12のモデルについて、前処理やハイパーパラメーターなどの条件をそろえ同じデータセットで比較した研究。タスクによって効果のある手法(LSTM or CNN、文字特徴、CRF層etc)にばらつきがあるが、効果がある場合は1ptほど上がる
817	DARTS: Differentiable Architecture Search	https://arxiv.org/abs/1806.09055	Hanxiao Liu, Karen Simonyan, Yiming Yang	ニューラルネットの構造探索を勾配法で学習する手法(実質的には構造全体でなくセル構造の探索)。ノードをつなぐ処理の選択確率と(ノード数は事前に決める)、処理に使用する重みを交互に学習していく。「処理を選択する」というのは微分不可能なので、実質的には各処理の結果にかける重みになっている。
818	A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress	https://arxiv.org/abs/1806.06877	Saurabh Arora, Prashant Doshi	逆強化学習のサーベイ論文。基本的な仕組みから活用用途までが書かれている。逆強化学習の代表的な手法(Max Margin/Max Entropy/Bayesian)をきちんと整理して書いている。
819	Why do deep convolutional networks generalize so poorly to small image transformations?	https://arxiv.org/abs/1805.12177	Aharon Azulay, Yair Weiss	CNNが微細な画像の変更に弱いことを検証した研究。畳み込みによる画像領域の「サンプリング」はサンプリングの基本原則を無視しており(端的にはアンチエイリアスをかけろ原則)、それが原因ではないかとしている(=Poolingを入れることを推奨している)。また、学習画像の偏りについても言及している。
820	Learning to Search in Long Documents Using Document Structure	https://arxiv.org/abs/1806.03529	Mor Geva, Jonathan Berant	長い文書からの質問回答を、強化学習(DQNベース)で行う研究。ドキュメントの見出し(タイトル、セクションetc)をノード、その中の文をリーフに見立てツリー構造にして、回答がある文までの探索方法を学習させる。回答は文章前半にあることが多いため初期位置のノードは分布からサンプルしてスタートする
821	ResNet with one-neuron hidden layers is a Universal Approximator	https://arxiv.org/abs/1806.10909	Hongzhou Lin, Stefanie Jegelka	ResNetにおいて、ノードは一つだけで十分ではないかという研究。実験により全結合より単一ノードの方が決定境界を上手く学習することを確認。Dropoutがなぜ有効なのか？の証左にもなっているという。
822	Generating Natural Adversarial Examples	https://arxiv.org/abs/1710.11342	Zhengli Zhao, Dheeru Dua, Sameer Singh	「自然な」Adversarial Exampleを生成する研究。自然なというのは単なるノイズではなく実際のデータとしてあり得そうな変動を乗せるという意。一般的にはデータに直接ノイズを入れるが、こちらではGANの潜在空間上で入力に近い＋誤認識を誘う表現を探し生成する。これで自然言語でのExample生成も可能
823	Dynamic Integration of Background Knowledge in Neural NLU Systems	https://arxiv.org/abs/1706.02596	Dirk Weissenborn, Tom Koisk, Chris Dyer	自然言語処理において外部知識を獲得する機構を組み込む研究。外部知識は知識グラフのように構築が大変なものでなく、普通の文書であることを想定している。最初は単なる単語分散表現として格納するが、文章のEncode結果を使い徐々にコンテキスト依存の表現に更新していく。
824	Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting	https://arxiv.org/abs/1706.02596	Yen-Chun Chen, Mohit Bansal	要約を作成する際文を抽出してから書き換える形で作成する手法。文選択は単語分散表現をCNNで畳み込みBi-directionalにかけたものをRNNに入れ、ステップごと選択を行う。これをEncoder-Decoderに入れ書き換えを行うが、文選択が微分不可能なため強化学習で最適化を行っている
825	Sequential Copying Networks	https://arxiv.org/abs/1807.02301	2018/07/09	従来のコピーメカニズムは、ソースから１単語をコピーするかどうかの制御だったが、単語系列をコピーする拡張をおこなった。系列をコピーできると、フレーズや固有表現をコピーできるようになる
826	CartoonGAN: Generative Adversarial Networks for Photo Cartoonization	http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf	Yang Chen/Tsinghua University,	写真を高品質マンガに置き換えるGANの提案。
827	Differentiable Learning-to-Normalize via Switchable Normalization	https://arxiv.org/abs/1806.10779	Ping Luo, Jiamin Ren, Zhanglin Peng	正規化の手法で、Batch・Instance・Layerという3種のNormalizationをミックスする手法の提案。平均を引いて分散で割るという形式は同じだが、平均・分散を各手法における平均/分散の重みつき平均で算出する。この重みは学習可能であり、グループ数を決める必要があるGroup Normaliztionより優位性がある
828	Glow: Generative Flow with Invertible 1x1 Convolutions	https://arxiv.org/abs/1807.03039	Diederik P. Kingma, Prafulla Dhariwal	画像生成を可逆変換の関数を組み合わせ行う手法。1. データ各点に対する正確な尤度評価が可能、2. 潜在表現が得られる、3. 並列計算が行いやすいの3点を満たす(VAEだと1、GANだと2、自己回帰だと3がNGになる。可逆変換の場合逆変換で1、各関数(encoder)により2の獲得が可能)。高解像度の画像生成に成功
829	Is Q-learning Provably Efficient?	https://arxiv.org/abs/1807.03765	Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, Michael I. Jordan	モデルフリーとモデルベースで、どれぐらいサンプル効率に差があるのかを分析した研究。モデルフリーにおいても、探索をUCB(未探索のところほど探索する手法)で行い、学習率を直近の経験を重く評価するよう調整することでモデルベースの下限と同等の性能が出せることを証明。
830	Semantically Equivalent Adversarial Rules for Debugging NLP Models	http://sameersingh.org/files/papers/sears-acl18.pdf	Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin	自然言語処理におけるAdversarialな変換の研究。単純な言いかえ(ルールで変換)で文書読解やQA、感情分類のモデルをだませることを確認。例えばmovieを同じ意味のfilmに変えるだけでセンチメントが反転したり、What isをWhat'sに変えるだけでQAが間違えるなどかなり衝撃的な結果。
831	Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning	http://openaccess.thecvf.com/content_cvpr_2018/papers/Ramanishka_Toward_Driving_Scene_CVPR_2018_paper.pdf	Vasili Ramanishka/vram@bu.edu	運転者の行動理解のためのホンダリサーチインスティチュートデータセット (HDD）の紹介。（行動４分類、１）目標指向、２）刺激、３）原因、４）気づきへと注釈する方法の提案）
832	How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments	https://arxiv.org/abs/1806.08295	Cdric Colas, Olivier Sigaud, Pierre-Yves Oudeyer	強化学習でランダムシードによって挙動が変わってしまう問題について、どれくらいシードを取れば十分なのか調査した研究。結論としては、AとB、2つのアルゴリズムを比較する際はブートストラップ法で信頼区間を求めた上で、有意水準0.05未満のウェルチt検定を行う。またシード数は20はほしいという
833	Direct Uncertainty Prediction with Applications to Healthcare	https://arxiv.org/abs/1807.01771	Maithra Raghu, Katy Blumer, Rory Sayres, Ziad Obermeyer, Sendhil Mullainathan, Jon Kleinberg	ラベル付けに際して意見が割れるようなケースがある場合、通常は意見の分布(複数人のアノテーターによるラベル付けの結果分布)を予測させる。そうではなく、通常のラベルに加えて意見が割れる/割れないを直接予測させる方が効果的だったという研究。
834	Representation Learning with Contrastive Predictive Coding	https://arxiv.org/abs/1807.01771	Aaron van den Oord, Yazhe Li, Oriol Vinyals	Encodeした潜在表現からさらに自己回帰モデルでコンテキストを抜き、そこから複数ステップ先の潜在表現を予測させることでより抽象化された、長期の推定に有用な表現を学習させる手法。学習では、コンテキスト/データの量的偏りを改善するためコンテキストの有り無しにおける確率比率を使用している。
835	An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution	https://arxiv.org/abs/1807.03247	Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, Jason Yosinski	CNNが位置情報の推定タスクに弱いことを突き止め、その解決方法を提案した研究。CNNでは座標をDeconvolutionして2次元マップ上の位置を復元するというタスク難しいことを確認。これはフィルターが位置情報を知らないためで、x/yの座標情報を追加すると上手くいくことを確認
836	LiDAR-Video Driving Dataset: Learning Driving Policies Effectively	http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf	Yiping Chen	これまで自動運転に向けた実験データセットというとビデオしかなかったため、レーザーによる奥行き情報データセット（LiDAR-Video Driving Dataset）の紹介する。片目しかないドライバーが駐車および車線変更作業が上手にできないように、奥行き情報の利用は予測性能の大幅な向上をもたらすことが分かった。
837	AtDelfi: Automatically Designing Legible, Full Instructions For Games	https://arxiv.org/abs/1807.04375	Michael Cerny Green, Ahmed Khalifa, Gabriella A.B. Barros, Tiago Machado, Andy Nealen, Julian Togelius	ゲーム内のルールをグラフで表現し、そこからゲームのチュートリアルを自動生成するという研究。
838	Window Opening Model using Deep Learning Methods	https://arxiv.org/pdf/1807.03610.pdf	Romana Markovica, Eva Grintal, Daniel Wo lki, Je rome Frisch, Christoph van Treeck	商業ビルの暖房換気および空調（HVAC）の室内気候、エネルギー消費を、初めて窓の解放を考慮（多層パーセプトロンとハイパーパラメーターを使用）したモデル化をした研究。現実応用の可能性として、Modelicaベースの熱ビルディングテストを行い、オフィスの建物の評価精度とF1得点はそれぞれ86-89％と0.53-0.65、スパース入力データでは高いF1スコアを出せた。
839	Context Encoding for Semantic Segmentation	http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Context_Encoding_for_CVPR_2018_paper.pdf	Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal	セマンティックエンコーディングロスを組み込んだコンテキストエンコーディングモジュールの紹介と、セマンティックセグメンテーションフレームワークContext Encoding Network（EncNet）の設計と実装。これにより、映像シーンの理解の精度をあげた。PASCAL VOC 2012で85.9％、PASCALで51.7％の結果。（Amazon.com関連の研究）
840	Outfit Generation and Style Extraction via Bidirectional LSTM and Autoencoder	https://arxiv.org/abs/1807.03133	Takuma Nakamura/ZOZO Research	似合う服装スタイルの提案。人の服装やスタイルを、カジュアル、フォーマルの割合でラベリングすることを提案しており、そのラベルから服装コーディネートを生成した。これにより様々な好みに応じることができる。（ZOZOの研究）（KDD2018採択）
841	Making Neural QA as Simple as Possible but not Simpler	https://arxiv.org/abs/1703.04816	Dirk Weissenborn, Georg Wiese, Laura Seiffe	近年提案されている抽出型QAタスク(SQuAD等)のモデルは、シンプルなニューラルベースのモデルと比較されることなく、様々なコンポーネントが提案され複雑化している。本論文では現状のモデルと同等精度のシンプルな構造のモデルを提案している。
842	Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms	https://arxiv.org/abs/1805.09843	Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang Su, Yizhe Zhang, Chunyuan Li, Ricardo Henao, Lawrence Carin	自然言語処理において文章の構造をとらえるためにCNNやLSTMが利用されているが、それらがどれくらい役に立っているのかを検証した論文。分散表現＋重み無しのPooling処理のみというシンプルなモデル(SWEMs)と比較したが、分類・文選択といったタスクでCNN/LSTMを上回る結果
843	A Multi-Horizon Quantile Recurrent Forecaster	https://arxiv.org/pdf/1711.11053.pdf	Ruofeng Wen/Amazon.com	MQ-RNN による確率的時系列回帰のフレームワークの提示。Amazon.comで販売商品の需要予測、電気料金負荷を予測し、ベストの予測結果(論文公表時点)。特に時系列データの大きなスパイクやディップの原因となる、季節性や既知のイベントの問題を考慮するようにした。(NIPS2017採用論文)
844	Residual Dense Network for Image Super-Resolution	http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Residual_Dense_Network_CVPR_2018_paper.pdf	Yulun Zhang/Department of Electrical and Computer Engineering, Northeastern University	超解像（SR）のためのRDNを提案。RDNは階層的特徴を全て考慮できる方法。CNNベースのSRモデルが、オリジナルの低解像度（LR）画像から階層的特徴を全て使用するわけではないため、階層的特徴を考慮した。結果、最先端の方法に対して競争力のある性能を発揮した。
845	Language to Action: Towards Interactive Task Learning with Physical Agents	https://www.ijcai.org/proceedings/2018/0001.pdf	Joyce Y. Chai/Michigan State University	事前プログラムなしで人工エージェント（ロボット）に新タスクを教える、対話型タスク学習(ITL) の具体例の紹介。スムージーの作り方タスクでは、YOLOを用いて周囲を観察し、どのように意味的・コミュニケーション上の共通認識を持てるようにしたが紹介されている。
846	The price of debiasing automatic metrics in natural language evaluation	https://arxiv.org/abs/1807.02202	Arun Tejasvi Chaganty, Stephen Mussman, Percy Liang	BLEUやROUGEといった自動評価指標が、人間の評価と相関がないことは知られていた。この研究では単に相関がないだけでなく人手評価との差異がタスクやモデル(手法)によっても変わってしまうことを示しており、その変動を除去するには完全な人手評価と同等のコストがかかるとしている。
847	When deep learning meets security	https://arxiv.org/abs/1807.04739	Majd Latah	DNNをセキュリティに応用した研究のサーベイ。検知系を主体にまとめており、マルウェア・BotNet(乗っ取られたネットワーク)・悪意あるコードそれぞれの検知手法についてまとめている。
848	Probabilistic Model-Agnostic Meta-Learning	https://arxiv.org/abs/1806.02817	Chelsea Finn, Kelvin Xu, Sergey Levine	Few-shot learningにおけるMAMLという手法は、少ないサンプルで学習できる「ベストな初期値」を探す手法となっている。この研究では「ベストな初期値」の分布を推定することで、(少ないサンプルしかない)タスクにとってベストな初期値を複数サンプリングできるようにしている。
849	How transferable are the datasets collected by active learners?	https://arxiv.org/abs/1807.04801	David Lowell, Zachary C. Lipton, Byron C. Wallace	学習効果が高いサンプルを優先してラベル付けするアクティブラーニングの手法では、実際学習させるモデルで学習効果を予測する。この「学習効果が高い」とされたサンプルが他のモデルの学習でも有用かを検証した論文。同一のモデルでなければ、他のモデルにとってはランダムに選んだ方がましという結果
850	Motivating the Rules of the Game for Adversarial Example Research	https://arxiv.org/abs/1807.06732	Justin Gilmer, Ryan P. Adams, Ian Goodfellow, David Andersen, George E. Dahl	Adversarial Exampleについて、実際のセキュリティ上の脅威を想定して問題設定を行おうという提言。攻撃者と防衛者それぞれについて、何がゴールで(分類ミスが起こればいいのか、特定の間違った分類にさせないといけないのか)どういう行動がとれるのか(どの程度の摂動までOKか等)を定義している。
851	Flood-Filling Networks	https://arxiv.org/abs/1611.00421	Micha Januszewski, Jeremy Maitin-Shepard, Peter Li, Jrgen Kornfeld, Winfried Denk, Viren Jain	3Dのセグメンテーションを行う際に、3D空間の一部を切り取りマスクを推定＋マージ、という処理を繰り返すことで全体のセグメンテーションを行う研究。推定されたマスクは再度ネットワークへの入力となり、再帰的に処理される(Jordan型のRNNに近い)。
852	UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction	https://arxiv.org/abs/1802.03426	Leland McInnes, John Healy	t-SNE/PCAのような、次元圧縮した表現を得る手法。論文だけでなく実装が公開されている。データをローカルな多様体の複合として表現し、データ(X)が次元圧縮後の表現(Y)とこの複合多様体上で一致するかを学習させることで圧縮を行う。
853	On the Robustness of Interpretability Methods	https://arxiv.org/abs/1806.08049	2018/06/21 (presented at 2018 ICML Workshop on Human Interpretability in Machine Learning)	LIME をはじめとした摂動に基づいた説明手法が，勾配に基づいたそれと比べ，入力が僅かに異なるだけで説明も大きく異なる，即ち，頑健でないことを示した研究．局所リプシッツの定義から，説明の頑健性を測る指標を導入した．いかにして既存の説明手法に頑健性を施すかについても議論している．
854	Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies	https://arxiv.org/pdf/1801.01615.pdf	Hanbyul Joo/Carnegie Mellon University	顔の表情、身体の動き、手のジェスチャーなどをマークなしでキャプチャーできるモデルの提案。複数人も可能。
855	3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children with Autism	http://openaccess.thecvf.com/content_cvpr_2018/html/Marinoiu_3D_Human_Sensing_CVPR_2018_paper.html	Elisabeta Marinoiu	ヒューマノイドロボット用自閉症児の感情認識タスクの紹介。自閉症児は、他人の表情を読み取り行動することが苦手と言われる。一方ルールベースで動くコンピュータなどにはうまく対処しているため、ロボットを自閉症児の他人とのインタラクションを高めるために使用。感情認識タスクは認識率40〜50％の性能範囲であり、工業レベルで使われているRGB-D Kinectシステムと同等レベルの競争力となった。
856	Instance-Level Explanations for Fraud Detection: A Case Study	https://arxiv.org/abs/1806.07129	2018/06/19 (presented at 2018 ICML Workshop on Human Interpretability in Machine Learning)	Achmea 社の保険金詐欺検出に関する事例研究．ドメインエキスパートらの予測結果に対する理解を促すため，3 種類のモデル説明技術を用いて，2 つのダッシュボードを設計，実務に適用した．その際に得た学びと，新たに生じた課題について議論している．
857	VizWiz Grand Challenge: Answering Visual Questions from Blind People	http://openaccess.thecvf.com/content_cvpr_2018/html/Gurari_VizWiz_Grand_Challenge_CVPR_2018_paper.html	Danna Gurari/University of Texas at Austin	世界中のエンジニアに、視覚障害者支援をするためのVizWizと呼ばれるデータセットを公開する。VizWizは、視覚障害者によって撮影された31,000以上の写真と「この写真に写っているものはなに？」等の質問で構成されている。障害者視点で撮影されたデータはこれまでなかったので直接視覚障害者のニーズに答えられる。１）視覚的な質問に対する答えを予測し、２）視覚的な質問に答えることができないかどうかを予測したい。
858	Contrastive Explanations with Local Foil Trees	https://arxiv.org/abs/1806.07470	2018/06/19 (presented at 2018 ICML Workshop on Human Interpretability in Machine Learning)	既存の説明手法の多くは，説明時，予測に寄与する特徴量を提示するが，次元が大きくなるにつれ，その数は多くなってゆき，解釈が難しくなる．本研究は，「なぜ B でなく A と予測したのか？」という問いに対して一対他方式の決定木を学習することで，提示する特徴量の数を抑えることに成功した．
859	Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems	https://arxiv.org/abs/1806.07552	2018/06/20 (presented at 2018 ICML Workshop on Human Interpretability in Machine Learning)	既存の解釈可能性に関する研究は，解釈可能性の定義や研究動機に一貫性がなかった．本草案は，解釈可能性の形式化に貢献するため，機械学習システムは「誰にとって」解釈可能であるべきかを探った．そして，機械学習システムと何らかの目的を持つ 6 つの役割からなるエコシステムモデルを提案した．
860	A survey on policy search algorithms for learning robot controllers in a handful of trials	https://arxiv.org/abs/1807.02303	Konstantinos Chatzilygeroudis, Vassilis Vassiliades, Freek Stulp, Sylvain Calinon, Jean-Baptiste Mouret	ロボットのコトンロールを、少ないサンプルから学習する手法のサーベイ。シミュレーションやデモ、環境情報から何らか(モデル/戦略/報酬)の事前分布を改定するのが基本で、多くの手法はこれらの組み合わせを行っている。
861	A Survey on Open Information Extraction	https://arxiv.org/abs/1806.05599	Christina Niklaus, Matthias Cetto, Andr Freitas, Siegfried Handschuh	大量の文書から物事の関係性を抽出するOpen Information Extractionのサーベイ論文。学習データを(あまり)必要としない/マルチドメインのデータに適用可能/効率的に処理できる、という3要件が必要としている。過去に発表されたシステムについて、どのような評価が行われたかについてもまとめられている
862	Meta-Learning with Latent Embedding Optimization	https://arxiv.org/abs/1807.05960	Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, Raia Hadsell	各タスクに共通する良い初期値を学習するMAMLは、パラメーター数が多い場合推定が困難という問題があった。そこで、パラメーターを低次元の潜在表現から復元することで対応したという研究(学習データから潜在表現の分布を推定し、サンプルする過程はVAEと同様)。
863	Deep Clustering for Unsupervised Learning of Visual Features	https://arxiv.org/abs/1807.05520	Mathilde Caron, Piotr Bojanowski, Armand Joulin, Matthijs Douze	End-to-Endで、CNNを利用したクラスタリングを行う研究。CNNで特徴抽出=>k-meansでクラスタリング=>そのラベルで学習、というステップを繰り返すことでクラスタリングを行う。ただ、無策で行うと巨大な1クラスタに全画像を収めて精度100%になる可能性があるため、空クラスタを防止したりラベルの偏りがないように工夫したりしている。
864	UNet++: A Nested U-Net Architecture for Medical Image Segmentation	https://arxiv.org/abs/1807.10165	Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, Jianming Liang	画像の特徴抽出によく利用されるUNetを改良したUNet++の提案。UNetでは同階層のEncoder/Decoderを直接接続するが、UNet++ではEncoderの各階層から復元した結果を使いDecoderまでDense CNNで接続する。これによりEncoder/Decoderの表現差異が小さくなるようにしている。また、各階層から復元した結果にそれぞれLossを適用する。
865	Defining Locality for Surrogates in Post-hoc Interpretablity	https://arxiv.org/abs/1806.07498	2018/06/19 (presented at 2018 ICML Workshop on Human Interpretability in Machine Learning)	局所性の定義，即ち，説明器を学習する際に利用する標本の生成方法が，予測の説明の精度に大きく影響することを示した研究．説明対象の予測の標本から最も近く，かつ，ブラックボックス分類器の決定境界付近に位置する標本を見つけ出し，その近傍で標本を生成，説明器の学習に利用する方法を提案した．
866	Visual Explanations From Deep 3D Convolutional Neural Networks for Alzheimer's Disease Classification	https://arxiv.org/abs/1803.02544	Chengliang Yang/Dept. of Computer & Information Science & Engineering University of Florida	アルツハイマー病早期診断のための、ディープラーニング手法（3次元畳み込みニューラルネットワーク）の透明性を高められた手法の紹介。特に視覚的説明を生成するための手法（階層的な3D画像セグメンテーションの感度分析、空間マップ上のネットワークアクティベーションを視覚化）。
867	Using deep learning for comprehensive, personalized forecasting of Alzheimer’s Disease progression	https://arxiv.org/abs/1807.03876	Charles K. Fisher/Unlearn.AI, Inc.	病気の進行は人によって違うことがあるが、機械学習手法で、個人別に予測してくれるものはなかった。アルツハイマー病等のデータ（18ヶ月間の1908人の患者、42の臨床的特徴量）を用いて、個人別疾患進行予測モデルを作った。認知試験、臨床検査、臨床特性との関連性をシミュレートし、予測と信頼区間の両方を生成している。
868	A brain signature highly predictive of future progression to Alzheimer’s dementia	https://arxiv.org/abs/1712.08058	Christian Dansereau	軽度認知障害（MCI）からアルツハイマー型痴呆（AD）への進行を高精度（CN：認知機能正常者と、AD：アルツハイマー型痴呆者を９０％の精度で分けられるときに、１００％の精度で）で予測できるようにした機械学習の手法。
869	All-Optical Machine Learning Using Diffractive Deep Neural Networks	https://arxiv.org/abs/1804.08711	Xing Lin, Yair Rivenson, Nezih T. Yardimci, Muhammed Veli, Mona Jarrahi, Aydogan Ozcan	ニューラルネットワークの伝搬処理を波動(光)の伝搬とみなし、光を利用した演算を試みた研究。実際にネットワークの重みを表現する伝播/反射を行う板を3Dプリントで作成し(5層の全結合NN)、MNISTの手書き文字の分類に成功。なお実際の入力は光なので、学習時も入力する数字を光の振幅に変換する。
870	When Recurrent Models Don't Need To Be Recurrent	https://arxiv.org/abs/1805.10369	John Miller, Moritz Hardt	重みに制約を与えたRNNは、長さを切った全結合のネットワークでほぼ近似できるとした研究。理論の前提となる、重みについての制約と勾配についての制約がネットワークのパフォーマンスに影響を与えないことを実験で確認し、理論が成立するとしている。
871	Breaking NLI Systems with Sentences that Require Simple Lexical Inferences	https://arxiv.org/abs/1805.02266	Max Glockner, Vered Shwartz, Yoav Goldberg	文関係の推論を行うタスクで、語彙と文法の識別能力を個別に検証した論文。文構造はそのままに同じ意味の単語に置き換えた(sadとunhappyなど)データセットを作成し検証。置き換え後の単語が未知語にならないようにしても、既存のSOTAモデルの精度がガタ落ちしたという結果。
872	A Survey of the Usages of Deep Learning in Natural Language Processing	https://arxiv.org/abs/1807.10854	Daniel W. Otter, Julian R. Medina, Jugal K. Kalita	自然言語処理におけるDNNの適用事例についてまとめたサーベイ。DNNだけでなく、SVMや決定木も含めた昔ながらのモデルについてもきちんと言及されている。
873	TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing	https://arxiv.org/abs/1807.10875	Augustus Odena, Ian Goodfellow	ニューラルネットに対しカバレッジテストを行う手法の提案。ツール(TensorFuzz)も公開されるとのこと。数件の実サンプルを元にノイズを加えたデータを生成して入力、アクティベーションのパターンをクラスタリングし既存のクラスタから遠い場合検証データに加えるという形。
874	Learning Visual Question Answering by Bootstrapping Hard Attention	https://arxiv.org/abs/1808.00300	Mateusz Malinowski, Carl Doersch, Adam Santoro, Peter Battaglia	重要な箇所に重みを掛けるSoft-Attentionでなく、生体における本来の"Attention"に近いHard-Attention(重要な特徴のみサンプルする)を改良した研究。VQAのタスクで画像/質問それぞれをEncodeした結果を合算し、そのベクトルのL2距離から関連度を計測、閾値以上のパート以外にマスクをかける。
875	Neural Baby Talk	https://arxiv.org/abs/1803.09845	Jiasen Lu(Georgia Institute of Technology)	キャプション中のSlotと物体検出情報を紐付けた(Slotの種類は、単語のEntity等から事前に定義しておく)物体検出(Faster-RCNN)情報から紐づけた自然言語により、キャプションを生成できる新しいフレームワワークを提案。
876	Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning	http://aclweb.org/anthology/P18-1199	Pengda Qin	関係抽出タスクに強化学習を用いる。
877	Neural Argument Generation Augmented with Externally Retrieved Evidence	https://arxiv.org/abs/1805.10254v1	Xinyu Hua and Lu Wang	与えられたstatementをもとに、議論の文章を生成する。
878	The Devil of Face Recognition is in the Noise	https://arxiv.org/abs/1807.11649	Fei Wang, Liren Chen, Cheng Li, Shiyao Huang, Yanjie Chen, Chen Qian, Chen Change Loy	顔認識におけるラベルのノイズの影響について。MegaFaceやMS-Celeb-1Mなどの大規模なデータセットは、量が多いため検索エンジン等を利用し収集とアノテーションをしている。そのため誤ったラベルが多く、これを修正した場合実データの20~30%程度で同等の精度が出せるという。また、大規模ながら人手でアノテーションしたデータセット
879	Neural Arithmetic Logic Units	https://arxiv.org/abs/1808.00508	Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, Phil Blunsom	ニューラルネットでは数値スケール(1と20が20倍違うなど)が上手く学習できない。この改善としてスケールシフト(数値演算)が表現できるユニットを提案(加減算のNACと、掛け算のNALU)。実験から非線形変換が数値表現の獲得に影響を及ぼしていたので線形変換で作成。画像/言語の数値変換などで効果を確認
880	Unsupervised Aspect Term Extraction with B-LSTM & CRF using Automatically Labelled Datasets	https://arxiv.org/pdf/1709.05094	Athanasios Giannakopoulos | Claudiu Musat | Andreea Hossmann | Michael Baeriswyl	この論文はSupervised aspect term extractionというタスクにたいして、２つの問題を解決したい。一つ目はラベル付きデータの数が少ない。二つ目は人工的にアノテーションする時間がかかる、尚且お金がかかる。解決策としては、まずラベルなしデータを使うこと。このようなデータにたいして、自動的にアノテーションする。この論文はrule-based手法を基ついて、自動的にアノテーションするプロセスを提出した。
881	MnasNet: Platform-Aware Neural Architecture Search for Mobile	https://arxiv.org/abs/1807.11626	Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Quoc V. Le	強化学習によるネットワーク構造探索を、モバイル用モデルの探索に活用した研究。精度/実行速度をバランスさせる指標を開発し、それを報酬として学習させる。探索空間についてはブロックの構成を決めてしまう一方、ブロック内のセル構造は位置により変えられるようにしている
882	Dataset Construction via Attention for Aspect Term Extraction with Distant Supervision	https://arxiv.org/pdf/1709.09220	Athanasios Giannakopoulos, Diego Antognini, Claudiu Musat, Andreea Hossmann, Michael Baeriswyl	Aspect term extractionというタスクにおいて、一つの文はaspect termが含まない場合もある。このような文はaspect term extractionのノイズになる。この論文はattentionモデルを使って、一つのレビューの各文にattention scoreにつける。文のattention scoreは閾値より低かったら、この文はaspect termが含まない文として削除される。文をフィルタリングして、新しいデータセットを作る。実験によって、新しいデータセットはaspect term extraction taskによい効果がある。
883	Generating Fine-Grained Open Vocabulary Entity Type Descriptions	https://arxiv.org/abs/1805.10564v1	Rajarshi Bhowmik and Gerard de Melo	Entityに対する事実情報をMemory機構に保持しておき、それを使ってDescriptionを生成するモデル。
884	Automated rule selection for aspect extraction in opinion mining	https://www.ijcai.org/Proceedings/15/Papers/186.pdf	Qian Liu, Zhiqiang Gao, Bing Liu and Yuanlin Zhang	文法の依頼関係でopinion termとaspect termの抽出方法はよく使われている。これは教師なし、且つドメインに依存しない方法だから、文法のルールをきちんと選択する必要がある。本研究はgreed searchに基づいて、最適な文法ルールのサブセットを見つけることを狙う。
885	A Study of Different Approaches to Aspect-based Opinion Mining	https://pdfs.semanticscholar.org/5eff/37f092946731e79e6d014197df0479889f3a.pdf?_ga=2.256548488.269970705.1534222090-863377252.1528950518	Pratima More, Archana Ghotkar	ニューラルネットワーク以外のaspect extraction手法をまとめて、最後はfrequency-basedとrelation-basedの手法だけを実験した。結果として、relation-basedの手法は良かった。
886	Think Visually: Question Answering through Virtual Imagery	https://arxiv.org/abs/1805.11025v1	Ankit Goyal Jian Wang Jia Deng	状態の画像と説明文を読み込んで、質問に回答するタスクの提案。
887	Improving Aspect Term Extraction with Bidirectional Dependency Tree Representation	https://arxiv.org/abs/1805.07889	Luo, Huaishao	文中のaspect termを抽出するため、文の構文木を利用するend-to-endモデルを提案した。
888	Efficient Low-rank Multimodal Fusion with Modality-Specific Factors	https://arxiv.org/abs/1806.00064v1	Zhun Liu	マルチモーダルデータを使った予測などに使うテンソルを低ランクへの分解を行うことで計算量の削減と並列化を実現
889	Neural Processes	https://arxiv.org/abs/1807.01622	Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S.M. Ali Eslami, Yee Whye Teh	確率過程をニューラルネットで実装する試み。問題設定としては、ある入力と出力のペア(x, y)が与えられたときに、その背後にある相関関係を推定し別途与えられるx'からy'を予測するというもの。全体としては、与えられたペアを潜在表現にするencoder、それを集計するaggregator、集計されたものを基に復元を行うdecoderというシンプルな構成。
890	Universal Transformers	https://arxiv.org/abs/1807.03819	Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, ukasz Kaiser	Transformerは閉じた系列の予測には強いものの、RNNのように連続的な系列の予測には弱いという弱点があった(学習時にないサイズの系列の場合パフォーマンスが落ちる)。そこでEncoder/Decoderブロックの処理を再帰的に何回か繰り返す処理を提案(回数を動的に決める手法も提案)
891	Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++	https://arxiv.org/abs/1803.09693	David Acuna, Huan Ling, Amlan Kar, Sanja Fidler	セグメンテーションにおけるアノテーションを支援する手法の提案。セグメンテーションとその頂点を提案、人が頂点を修正できるようにする。CNNで画像特長、RNNで頂点予測という構成で頂点予測に強化学習(厳密一致でなくても報酬を与える)、頂点のリファインにGNNと凝った構成
892	Exploring the Limits of Weakly Supervised Pretraining	https://research.fb.com/publications/exploring-the-limits-of-weakly-supervised-pretraining/	Dhruv Mahajan Ross Girshick Vignesh Ramanathan Kaiming He Manohar Paluri Yixuan Li Ashwin Bharambe Laurens van der Maaten	転移学習に関する研究で、どう事前学習するといいのかを検証した研究(Instagramのデータを使用)。事前学習の規模は転移性能への貢献がある、ラベル空間は近いほうがいい、また視覚的多様性を増やしたほうが良いとしている。また、現在のネットワークがImageNetのタスク/サイズに適合しすぎているとも。
893	Skill Rating for Generative Models	https://arxiv.org/abs/1808.04888	Catherine Olsson, Surya Bhupatiraju, Tom Brown, Augustus Odena, Ian Goodfellow	GeneratorとDeiscriminatorでトーナメント戦を行うことで、GANの評価を行う試み。各GとDは、異なる学習過程、また異なるハイパーパラメーター設定のものを選出する。これにより、どの段階でGがDに勝てるようになったのかなどを計測できる。勝率以外にレーティング(Glicko2)を導入など本物さながら
894	Large-Scale Study of Curiosity-Driven Learning	https://pathak22.github.io/large-scale-curiosity/	Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, Alexei A. Efros	内発的報酬のみでどこまでプレイできるかを検証した研究。内発的報酬は行動による状態変化に対して与えられており、本研究では「状態」の表現方法についてピクセル/固定CNN/VAE/IDFの4つを使用している。タスクが進むか、どれが有効かはかなりタスクに依存している。
895	Hierarchical Attention: What Really Counts in Various NLP Tasks	https://arxiv.org/abs/1808.03728	Zehao Dou, Zhihua Zhang	Attentionを再帰的に複数回かけるMulti-Level Attentionの、各段階における重み付き和をとるHierarchical Attentionを提案。5段階程度の階層で、ベースラインのモデルをブーストできることを確認。
896	Neural Architecture Search: A Survey	https://arxiv.org/abs/1808.05377	Thomas Elsken, Jan Hendrik Metzen, Frank Hutter	機械学習によるネットワークの構造探索手法(NAS)に関するサーベイ。探索空間・探索方法・アーキテクチャの評価方法と3つの観点に分けて手法をまとめている。探索以外の変形(mutation)については弱めだがきちんと載っている。
897	Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction	https://arxiv.org/abs/1808.03867	Maha Elbayad, Laurent Besacier, Jakob Verbeek	DenseNetで翻訳を行うという斬新な手法。入力マップは、ターゲット長Ｘソース長Ｘ(ターゲットembeddingサイズ＋ソースembeddingサイズ)となる。畳み込みは、未来の情報に依存しないようマスクをかけて実施。最終的にソース長部分を集約する際はAverage(+Attention)で実施。
898	AVID: Adversarial Visual Irregularity Detection	https://arxiv.org/abs/1805.09521	Mohammad Sabokrou, Masoud Pourreza, Mohsen Fayyaz, Rahim Entezari, Mahmood Fathy, Jrgen Gall, Ehsan Adeli	異常を消した画像を生成するGeneratorと、異常を検知するDiscriminatorを戦わせる形で異常検知を行う手法。実際検知を行う際は、実画像とGの生成結果の差(異常があれば消されるので差が大きくなる)と領域単位のDの判定結果を合わせて行う。
899	Customized Regression Model for Airbnb Dynamic Pricing	http://www.kdd.org/kdd2018/accepted-papers/view/customized-regression-model-for-airbnb-dynamic-pricing	Peng Ye, Julian Qian, Jieying Chen,Chen-Hung Wu,Yitong Zhou,Spencer De Mars,Frank Yang,Li Zhang	Airbnbで使用している価格決定モデルについての論文。予約される確率と適正価格の予測を分けており、予約確率により提案価格を上下させている。価格の予測は、予約されなかった場合に提案モデルが引く価格を予測していた割合(=価格を下げれば予約されたケース)と、予約された場合に提案モデルが低い価格を予測していた割合(=もっと上げていればよかったケース)の2つをバランスさせる形で行う。
900	Everybody Dance Now	https://arxiv.org/abs/1808.07371	Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros	Everybody Dance Nowというすごいタイトルの論文。動画を与えることで、画像を同じように動かすという内容。仕組み的には、フレームレベルでのイメージ変換として動いている。ポーズ推定のネットワークと、ポーズを取らせるGAN(ポーズと画像のペアについて、画像がGによる生成か判断)の2つで構成される
901	Aspect Term Extraction with History Attention and Selective Transformation	https://arxiv.org/abs/1805.00760	Xin Li, Lidong Bing, Piji Li, Wai Lam, Zhimou Yang	系列レベリングの手法でaspect termを抽出する時、opinion termを考慮していない。opinion termの情報を利用するため、attentionモデルで抽出したaspect履歴を学習しながら、文ごとにopinion要約を生成する。aspect履歴とopinion要約をあわせて、aspect termを予測のする仕組みを提案した。
902	Count-Based Exploration with the Successor Representation	https://arxiv.org/abs/1807.11622	Marlos C. Machado, Marc G. Bellemare, Michael Bowling	強化学習で、難しい探査問題に対してドメイン固有の知識に依存してるため、一様ランダムな探索ばかり行われていた。表現学習に密接な関係にある劣確率的代替表現(substochastic successor representation)を疑似カウントとするカウントベース探索を行うことでより大きいドメインの問題にアプローチできる研究。SSRは代替状態の類似性によって状態の一般化をする代替表現に少し変更を加えたもの。
903	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	https://arxiv.org/abs/1808.04926	Divyansh Kaushik, Zachary C. Lipton	読解問題を機械学習で解くためのデータセットについて、その正当性を調査した論文。具体的には、文書だけ、質問だけでどれくらい回答できるのか調べている。SQuADではどちらかだけだとかなり落ちるが、bAbIでは落差が少なく、CBTに至っては質問だけ使うほうが精度が高いという結果。
904	Top-Down Tree Structured Text Generation	https://arxiv.org/abs/1808.04865	Qipeng Guo, Xipeng Qiu, Xiangyang Xue, Zheng Zhang	テキスト生成にて、直接文生成ではなく構文木を予測して生成を行う手法。構文木はBi-directional RNNで表現し、上位レイヤから親、同レイヤから隣接ノードの潜在表現を取ってノードの潜在表現を作成する。そこからの隣接ノード生成は別のRNNで行う。学習は階層が浅いものから深いものへ切り替えていく
905	Graph-to-Sequence Learning using Gated Graph Neural Networks	https://arxiv.org/abs/1806.09835	Daniel Beck† Gholamreza Haffari‡ Trevor Cohn†	Gated Graph Neural Net(GGNN)を使いグラフから直接文章を生成する
907	Hybrid optical-electronic convolutional neural networks with optimized diffractive optics for image classification	http://www.computationalimaging.org/publications/hybrid-optical-electronic-convolutional-neural-networks/	Julie Chang, Vincent Sitzmann, Xiong Dun, Wolfgang Heidrich, Gordon Wetzstein	光学的にニューラルネットを実装する手法の研究。今までできていなかったCNNを、しかも自然光(インコヒーレント)で実装している。位相マスクを使い一画像内に複数の変換結果を並べることで、畳み込みによるチャンネル分の画像を表現している。これでCNNの一層目を実装し、以後は通常のCNNへ入力する
908	Improving Abstraction in Text Summarization	https://arxiv.org/abs/1808.07913	Wojciech Kryciski, Romain Paulus, Caiming Xiong, Richard Socher	抽象型要約の抽象度をさらに高める試み。BiLSTMがベースだが、生成に学習済み言語モデルも活用している。EncoderへのAttention、DecoderへのAttention、Decoder State、そして言語モデルの状態を結合して出力単語を予測する。原文にないフレーズを増やしつつ、ROUGE/人手評価の結果を維持できたという
909	What Makes Reading Comprehension Questions Easier?	https://arxiv.org/abs/1808.09384	Saku Sugawara, Kentaro Inui, Satoshi Sekine, Akiko Aizawa	文書読解で使用されているデータセットについて、その難易度を調査した研究。質問文と最も距離が近い文に回答があるなどの簡単な質問と、複数文にまたがるなどの難しい質問とを分け、その比率を調査している。難しい質問ではモデルの精度が低下し、比率によっては既存モデルの過剰評価に繋がっていると
910	SOLAR: Deep Structured Latent Representations for Model-Based Reinforcement Learning	https://arxiv.org/abs/1808.09105	Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J. Johnson, Sergey Levine	Model Baseの手法で学習を行う際に、環境全体をモデル化するのでなく、局所的なパートだけモデル化して(このとき戦略も線形化する)、戦略の勾配を推定するという手法。これにより環境全体をモデル化する必要なしにModel Baseによる効率的な学習が可能になる。
911	Pros and Cons of GAN Evaluation Measures	https://arxiv.org/abs/1802.03446	Ali Borji	GAN(生成モデル)の評価方法をまとめたサーベイ。指標は定量的・定性的の2つに分けられている。定量的な指標はサンプル/生成結果からそれぞれ画像特長を抽出し距離を測るのがスタンダード。定性的な指標は人手評価を行うが、過学習が高く評価される(サンプルに似ていればいいとなると、過学習していたほうが有利)のを防ぐ必要があると。
912	Question Answering by Reasoning Across Documents with Graph Convolutional Networks	https://arxiv.org/abs/1808.09920	Nicola De Cao, Wilker Aziz, Ivan Titov	質問回答システムについて、参照文書をグラフで表現して、Graph Convolutionにより回答のエンティティを推定するという研究。複数文書の参照を想定しており、同文書内は共起・外文書へはメンションでリンクを構成し、回答候補のエンティティについてGCNでノード表現を作成し質問表現と合わせ回答する
913	Phrase-Based & Neural Unsupervised Machine Translation	https://arxiv.org/abs/1808.09920	Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc'Aurelio Ranzato	Facebookが教師なしで翻訳を行う手法を公開。単語ベースの翻訳の学習(先行研究 
914	Bottom-Up Abstractive Summarization	https://arxiv.org/abs/1808.10792	Sebastian Gehrmann, Yuntian Deng, Alexander M. Rush	要約で使われるPointer-Generatorモデルは生成とコピーを併用するモデルだが、コピー幅が長くなることが多いという弱点があった。そこで、ソース文において重要な単語(要約と一致する箇所＋既出でない)を抽出して、そこ以外にAttention Maskをかけるという手法で改善を行う試み。
915	Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction	https://arxiv.org/abs/1805.04601	Hu Xu, Bing Liu, Lei Shu and Philip S. Yu	Aspect抽出は複雑なタスク。普通のgeneral-purpose embedding (GloVe)は足りないので、domain-specific embeddingも必要。提案手法は２つのembeddingをembedding layerとして、アレンジしたCNNを使っている。シンプルなモデルだが、いい効果が出た。
916	LUCSS: Language-based User-customized Colourization of Scene Sketches	https://arxiv.org/abs/1805.04601	Changqing Zou, Haoran Mo, Ruofei Du, Xing Wu, Chengying Gao, Hongbo Fu	自然言語でイラストの色塗りを行う研究。「オレンジの車〜」「水色の空〜」と入力すると、イラスト中の該当箇所の色が変わる。イラストに対するインスタンスセグメンテーション＋Image Caption＋GANを組み合わせている。
917	Learning Conditioned Graph Structures for Interpretable Visual Question Answering	https://arxiv.org/abs/1806.07243	Will Norcliffe-Brown, Efstathios Vafeias, Sarah Parisot	画像を見て質問に答えるタスクに対し、Graph Convolutionを使う手法。検知したオブジェクトの画像特長と質問文の特徴を結合したものをノードとし、ノードの接続は画像特長の位置を基に行う。これにより、物体間の位置関係を質問文のコンテキストで把握することを狙っている。VQA-v2でSOTA。
918	Simpler but More Accurate Semantic Dependency Parsing	https://arxiv.org/abs/1807.01396	Timothy Dozat, Christopher D. Manning	シンプルなDependency Parserの提案。単語と品詞の情報を結合してBi-directionalに入力し、その潜在表現からエッジの有無とエッジのラベルを同時に予測させる(最終的には、単語間のエッジ情報を表すグラフ表現を出力することになる)。これで既存の複雑なモデルを超える精度
919	Bilinear Attention Networks	https://arxiv.org/abs/1805.07932	Jin-Hwa Kim, Jaehyun Jun, Byoung-Tak Zhang	VQAにおいて画像と質問にCo-attentionを貼るのは有効だが、全ペアに対して計算を行うのは非常に高コストになる。そこで、行列の低ランク近似(low-rank bilinear model)を用いてAttentionを近似し、計算効率を高めたという研究。これにより重みの数も減るためCo-attentionで起こりがちな過学習も防げた
920	Recurrent Relational Networks	https://arxiv.org/abs/1711.08028	Rasmus Berg Palm, Ulrich Paquet, Ole Winther	オブジェクト間の関係を推論するシンプルなRelation Network( 
921	Detect-and-Track: Efficient Pose Estimation in Videos	https://arxiv.org/abs/1712.09184	Rohit Girdhar/The Robotics Institute, Carnegie Mellon University	多人数ビデオ映像での複雑な人体keypoint推定とトラッキングの問題で、非常に単純かつ、高効果な手法を提案。
923	Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction	https://arxiv.org/abs/1808.09602	Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi	AI関連研究論文のAbstractから知識を抽出する手法。各文のエンティティとエンティティ間の関係、先行文への参照を同時に解く。文内から適当な長さのスパンを抽出し双方向LSTMで共通特徴を作成、その後は被参照/関係あり、また各タスクごとの予測をそれぞれ全結合で予測
924	Spherical Latent Spaces for Stable Variational Autoencoders	https://arxiv.org/abs/1808.09602	Jiacheng Xu, Greg Durrett	VAEを学習するとき、潜在表現zが無視され事前分布と事後分布を一致させる方向に学習が行われてしまうことがある(KL collapse)。これを解消するために、正規分布の代わりにVon MisesFisher分布(球面上の分布)を導入。KL距離からパラメーター要素(μ,σ)を除去することで、学習による一致を防いでいる
925	Deep Learning for Generic Object Detection: A Survey	https://arxiv.org/abs/1809.02165	Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, Matti Pietikinen	物体検知の手法のまとめで、手法だけでなく物体検知というタスク自体についてもその歴史をたどり系統図にまとめている。図解も豊富で分かりやすく、まさに"Survey"の名を冠するにふさわしい出来。
926	Mapping Natural Language Commands to Web Elements	https://arxiv.org/abs/1808.09132	Panupong Pasupat, Tian-Shun Jiang, Evan Liu, Kelvin Guu, Percy Liang	自然言語でWebサイトを操作することを目指した研究。「最初の記事をクリック」や「表示言語を切り替えて」といった自然言語の指示を実現できるかというタスクを提案している(具体的には、ページのDOM/自然言語指示を基に正答エレメントを選択する形になる)。データとベースラインを提供している
927	Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning	https://arxiv.org/abs/1807.03146	Supasorn Suwajanakorn, Noah Snavely, Jonathan Tompson, Mohammad Norouzi	3D空間上のキーポイントを、ラベルからではなく視点の異なる2つの2D画像から推定する手法の提案。推定したキーポイントが他方の2D画像上でも同じ点となるか(Multi-view consistency)、それぞれ推定したキーポイントの位置関係から視点差(回転)を復元できるか、の2点を目的関数に組み込み学習する。
928	Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning	https://arxiv.org/abs/1708.02596	Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, Sergey Levine	モデルフリーの学習とモデルベースの学習を併用する、Dynaという手法に連なる手法。モデルとしてニューラルネットを使用し、単純に次の遷移だけでなく次の次の・・・というMulti-stepの予測をさせるようにすることで、既存の(ニューラルネットを利用した)モデルベースの研究を上回る精度を出している。またモデルベースで学習したモデルをエキスパートとすることで、モデルフリーのモデルを模倣学習するという興味深い手法も提示。
929	Speaker Recognition from Raw Waveform with SincNet	https://arxiv.org/abs/1808.00158	Mirco Ravanelli, Yoshua Bengio	音声を処理するCNNで、生の音声を処理する1層目を意図的にバンドパスフィルタを模すことで(フィルタする周波数領域は学習させるようにする)話者特定の精度と速度を上げた研究。具体的にはHamming windowをかけてFFT=>フィルタ適用=>逆FFTで元に戻す(窓関数は他のでもあまり大差ないらしい)。
930	Graph Convolution over Pruned Dependency Trees Improves Relation Extraction	https://nlp.stanford.edu/pubs/zhang2018graph.pdf	Yuhao Zhang,Peng Qi,Christopher D. Manning	Graph Convolutionを使い、文中から主語と目的語の関係を正しく推論するという研究。単純にGCNを使うだけでなく、activation前の正規化やGCNに投入する前にBi-directionalで文脈情報を得ておく、不要パスの枝刈りといった多くの工夫を行っている。
931	Multi-task Deep Reinforcement Learning with PopArt	https://arxiv.org/abs/1809.04474	Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, Hado van Hasselt	報酬のクリッピングを見直して適応的な正規化(PopArt)を導入したという話。例えばパックマンでは幽霊を食べる、ペレットを取得する、という様々な行動があるがクリッピングするとすべて「+1」になってしまう。このため報酬(実際は価値)を正規化すること対応した。複数ゲームをまとめても効果を確認。
932	Combined Reinforcement Learning via Abstract Representations	https://arxiv.org/abs/1809.04506	Vincent Franois-Lavet, Yoshua Bengio, Doina Precup, Joelle Pineau	強化学習で、モデルフリーの学習とモデルベースの学習を同時に行う手法の提案。この時環境の表現を共有することで双方に有用な潜在表現を獲得できるほか、モデルベースでモデルフリーの学習効率を上げることができる。
933	Textual Analogy Parsing: What's Shared and What's Compared among Analogous Facts	https://arxiv.org/abs/1809.02700	Matthew Lamm, Arun Tejasvi Chaganty, Christopher D. Manning, Dan Jurafsky, Percy Liang	数値に関するニュース文からグラフを生成することを目指した研究。文中のフレーズをフレームにあてはめて情報を抽出するのが基本だが、比較などの情報が落ちないよう、入れ子になったフレーム(TAP frame)にあてはめることを提案。データセットも併せて提供している。
934	Pose Proposal Networks	http://taikisekii.com/PDF/Sekii_ECCV18.pdf	Taiki Sekii	物体検出の枠組みで、素の画像から任意数のPoseを検出する研究。物体検出部分はYOLOと同様画像をグリッドに分割して検出を行う。これで人のパーツとその接続(limb=四肢)を検出した後、これらをマージして各人体のパーツにする。これでまさにYOLOのように十分な精度での高速検知を可能に
935	Unsupervised Sentence Compression using Denoising Auto-Encoders	https://arxiv.org/abs/1809.02669	Thibault Fvry, Jason Phang	文にノイズを加えて復元する、というAuto-Encoderの枠組みで文圧縮を行う試み。文へのノイズ挿入は、他の文からサンプリングした単語を対象文に加えてシャッフルすることで行う(シャッフルは単語/n-gram単位で行う)。この他、辞書型未知語対応などいくつかの工夫を行っている
936	Model-Based Reinforcement Learning via Meta-Policy Optimization	https://arxiv.org/abs/1809.05214	Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, Pieter Abbeel	モデルベースを使用しメタラーニングを行う研究。メタラーニングでは複数のタスクにおける学習結果を統合することで転移しやすいモデルを作成するが、この研究では複数のタスクの代わりに複数のモデルベース環境/そこでの戦略を用いてメタラーニングを行っている。
937	Conditional Neural Processes	https://arxiv.org/abs/1807.01613	Marta Garnelo, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo J. Rezende, S. M. Ali Eslami	観測データをモデル化する確率過程とニューラルネットを組み合わせた手法。確率過程の代表格であるガウス過程では観測点から分散共分散を計算しモデル化を行うが、分散共分散の更新を行うには既存のデータ点を確保しておく必要がある。そこでEncoder/Decoderの枠組みで、データ点=>ベクトル表現=>ベクトル表現を集約=>Decoderで復元、という形でNN化している。
938	Generative Adversarial Imitation Learning	https://arxiv.org/abs/1606.03476	Jonathan Ho, Stefano Ermon	模倣学習をGANの枠組みで行う試み。Generatorは戦略を学習し、Discriminatorは生成された行動軌跡がエキスパートのものかGのものかを識別するよう訓練する。戦略はTRPOの枠組みで学習を行う。
939	A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors	https://arxiv.org/abs/1805.05388	Mikhail Khodak, Nikunj Saunshi, Yingyu Liang, Tengyu Ma, Brandon Stewart, Sanjeev Arora	未知語や低頻度語の分散表現を得る手法の提案。周辺単語(コンテキスト)の平均でそこそこ良い精度が出ることが知られているが、単純平均から単語ベクトルを復元するような重みを学習することで、より精度の高い推定が可能なことを示した。Bi-LSTMには劣るものの単純な方法ながら各タスクで良い精度
940	Graph Convolutional Networks for Text Classification	https://arxiv.org/abs/1809.05679	Liang Yao, Chengsheng Mao, Yuan Luo	文書分類にGraph Convolutionを適用した研究。ノードとして文書・単語双方をとっており、文書=>単語の重みはTF-IDF、単語=>単語の重みはコーパス全体の共起(PMI)をとっている。単語分散表現なしにCNNやLSTMのモデルを上回る効果を出せている。ただ、テストセットのノードが既知であるTransductive設定
941	Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora	https://arxiv.org/abs/1806.03191	Stephen Roller, Douwe Kiela, Maximilian Nickel	文章から上位概念を抽出する(「笛」は「楽器」など)方法について、パターンマッチと統計的手法(分布ベースの手法)の比較を行った研究。結果としては、パターンの方が優秀で、パターンで検知されたものと似ている(=共起する)ものも評価することでさらに改善できるとのこと。
942	Understanding Batch Normalization	https://arxiv.org/abs/1806.02375	Johan Bjorck, Carla Gomes, Bart Selman, Kilian Q. Weinberger	Batch Normalizationの効果について検証した研究。発表当初提案されていた共変量シフトの解消はあまり効果がなく、それ以外の効果で精度の改善が行えているとしている。具体的には、BNなしでは層が深くなるにつれてチャンネルの分散が大きくなり、勾配が入力に非依存になってくる。BNはこの問題を解決し、結果として大きな学習率の設定が可能になり、正則化/精度が向上する。
943	Identifying Generalization Properties in Neural Networks	https://arxiv.org/abs/1809.07402	Huan Wang, Nitish Shirish Keskar, Caiming Xiong, Richard Socher	DNNの汎化性能を向上させるための、適切なノイズについての研究。解はflatな方が汎化性能が高いことが知られているが(lossの高いsharpな箇所よりflatなlocal minimumの方がシンプルな決定境界を持つことも論文中で示されている)、flatに誘導するためのノイズとしてPAC-Bayesで上限を設定している。
944	Semi-Supervised Sequence Modeling with Cross-View Training	https://arxiv.org/abs/1809.08370	Kevin Clark, Minh-Thang Luong, Christopher D. Manning, Quoc V. Le	半教師あり学習で、教師なしのデータからも有効な学習を行う方法の提案。ラベルありについては普通に学習するが、ラベルなしの場合はデータの一部を欠損させた状態で欠損のない場合と同じ確率分布が予測できるかを学習する。これにより教師なしラベルからもEncode精度を上げるための学習を行う。
945	Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension	https://arxiv.org/abs/1804.07726	Minjoon Seo, Tom Kwiatkowski, Ankur P. Parikh, Ali Farhadi, Hannaneh Hajishirzi	既存のQAモデルは「質問」「回答が含まれる文書」間にAttentionを張って回答を見つけるのが一般的だが、通常候補となる文書は膨大で、質問ごとに文書ベクトルを再計算するのは現実的でない。そこで、質問と文書は個別にベクトル化されることを前提としたタスクを提案している
946	Training a Ranking Function for Open-Domain Question Answering	https://arxiv.org/abs/1804.04264	Phu Mon Htut, Samuel R. Bowman, Kyunghyun Cho	一般的なQAでは、回答を含む文書が既にあることは前提とできない。そこで質問に関連する文書の検索、回答が含まれる可能性に応じて再ランキング、最後に回答抽出、という3段階のプロセスを提案している。2番目のステップ(文ベクトルによる再ランキング)で精度の向上を確認。
947	Learning to Summarize Radiology Findings	https://arxiv.org/abs/1809.04698	Yuhao Zhang, Daisy Yi Ding, Tianpei Qian, Christopher D. Manning, Curtis P. Langlotz	放射線診断について、患者の症状と検査結果から所見を生成する研究。具体的には、Background(xxが痛い、など)とFindings(胸にyyがあり、など)から、所見(zzの症状が疑われる、など)を生成する。基本はFindingからの要約(Pointer-Generator)だが、Backgroundを別途EncodeしDecode時参照している。
948	How to train your MAML	https://openreview.net/forum?id=HJGven05Y7	?	各タスクに共通する良い初期値をみつけるMAMLの改善提案。勾配からさらに学習する形になるため二階微分が必要で計算コストが高い点と、安定性が低い点が課題だった。計算については序盤では一階の微分による近似で済ます、安定性では複数ステップの重み付平均を取るなどの対策をとっている。
949	Recurrent Experience Replay in Distributed Reinforcement Learning	https://openreview.net/forum?id=r1lyTjAqYX	?	Rainbowの亜種であるAPE-X(
950	Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow	https://xbpeng.github.io/projects/VDB/index.html	Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, Sergey Levine	GANの枠組みで模倣学習/逆強化学習を行うGAILの安定性を向上させるための研究。VAEのように入力を基に分布のパラメーターを推定し潜在表現をサンプリングするが、この際入力と潜在表現の相関が一定値以下になるよう制約をかける(入力=潜在表現にならないようにする)。
951	How good is my GAN?	https://arxiv.org/abs/1807.09499	Konstantin Shmelkov, Cordelia Schmid, Karteek Alahari	GANを評価する指標として、GANで生成された画像を使って学習しオリジナルの画像で評価するGAN-train、オリジナル画像で学習しGAN画像で評価するGAN-testの2つを提案。前者で画像品質/クラスラベルの多様性、後者は高すぎる場合元画像を記憶しているだけと推察できる。
952	Regularizing and Optimizing LSTM Language Models	https://arxiv.org/abs/1708.02182	Stephen Merity, Nitish Shirish Keskar, Richard Socher	言語モデルについての正則化手法をまとめた論文。DropConnectの適用、Averaged SGDの使用、embeddingへのDropout適用、sequence lengthの動的変更といった工夫が紹介されており、各手法のインパクトについても分析されている。
953	A Span Selection Model for Semantic Role Labeling	https://arxiv.org/abs/1810.02245	Hiroki Ouchi, Hiroyuki Shindo, Yuji Matsumoto	系列ラベリングをスパンベースで行う研究。これにより単語(token)単位でなく「ここからここまで」という形でラベルを振ることができる。Bidirectionalが基本で、A=>Bのスパンの場合スパン特徴はをA+BとA-Bをコンカチしたもので、各候補スパンについてスコアを算出した後greedy searchで最適化する。
954	Generative Neural Machine Translation	https://arxiv.org/abs/1806.05138	Harshil Shah, David Barber	生成モデルで翻訳を行う研究。ソースとターゲットで潜在表現を共有することで、同じ意味の文は同じ意味の潜在表現になるようにしている。潜在表現の生成はガウス分布を使っており、潜在表現からソースの生成はLSTM、ターゲットの生成はソースのEncode結果+潜在表現を利用する。
955	Decoupling Strategy and Generation in Negotiation Dialogues	https://arxiv.org/abs/1808.09637	He He, Derek Chen, Anusha Balakrishnan, Percy Liang	価格交渉を行う対話データセットと、それを解くための基礎的なモデルの紹介。1402のアイテムについて(アイテムには画像がついているものもある)、価格(値下げ)交渉を行っている6682対話が収録されている。モデルとしては、seq2seqと対話行為タイプを推定するモジュール構成タイプの2つが提供されている
956	Reinforcement Learning for Improving Agent Design	https://arxiv.org/abs/1810.03779	David Ha	強化学習において、頭脳である戦略だけでなく体(エージェントの形態: 足の角度や長さなど)も学習させるという試み。体の調整具合に応じて、報酬も変動させる。戦略と体の調整は重みを共有し、学習は素のPolicy Gradientがベースでパラメーターをサンプリングする方式を組み合わせている。
957	Meta-Learning: A Survey	https://arxiv.org/abs/1810.03548	Joaquin Vanschoren	メタラーニングについてのサーベイ資料。学習する「メタ」の種類として、ハイパーパラメーターなどの設定、タスクの性質(類似性)、モデルの学習結果(重み)に内在する共通特徴の3点を挙げ、このカテゴリで既存研究をまとめている。
958	Ask the Right Questions: Active Question Reformulation with Reinforcement Learning	https://arxiv.org/abs/1705.07830	Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby, Wei Wang	QAシステムを構築する際に、目的のAを得るためQを変える手法(いわば、質問の仕方を変える)。目的のAが得られたら報酬、という強化学習の枠組みで学習している。言いかえはSeq2Seqがベースだが強化学習のみでは厳しいため、事前に翻訳のコーパス、さらに単言語の言い換えコーパスで学習している
959	BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding	https://arxiv.org/abs/1810.04805	Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova	Bi-directionalのTransformerを事前学習し、QAや文関係推論などのタスクに転移した研究。ELMo(
960	Unconventional Wisdom: A New Transfer Learning Approach Applied to Bengali Numeral Classification	https://www.researchgate.net/publication/326989744_Unconventional_Wisdom_A_New_Transfer_Learning_Approach_Applied_to_Bengali_Numeral_Classification	Hasib Zunair, Nabeel Mohammed, Sifat Momen	CNNを転移学習する際、最初と最後の層「以外」をfreezeするという通常とは変わった手法の提案。最終の分類層をランダムな重みで固定してもなかなかの精度が出るという。Kaggleで開催されたベンガル語手書き数字を認識するコンペティションで6位をとっている。なお上位はより大きいモデル＋アンサンブル
961	Multi-Task Learning as Multi-Objective Optimization	https://arxiv.org/abs/1810.04650	Ozan Sener, Vladlen Koltun	マルチタスク学習を、複数目的関数のパレート最適を求める形で解く手法。タスクAを偏重したらタスクBにダメージが行くという、いわば予算制約がある形の制約を課してこれをFrankWolfe法で解く。タスク共通部分の勾配については、本来タスクごとに求める必要があるが上界で近似し計算を効率化している
962	Unsupervised Neural Multi-document Abstractive Summarization	https://arxiv.org/abs/1810.05739	Eric Chu, Peter J. Liu	教師なしで要約を学習する手法の提案。逆翻訳に近い構成で、元文書をモデルに入力し要約を得て、今度は要約をモデルに入れて元文書を復元する(モデルはEncoder-Decoder)。元文書と要約は潜在表現が近しいはず(=Encode結果のベクトル距離)＋元文書の再構成lossの2つで学習を行う。
963	Trellis Networks for Sequence Modeling	https://arxiv.org/abs/1810.06682	Shaojie Bai, J. Zico Kolter, Vladlen Koltun	CNNとRNN、双方の性質を兼ね備えたTrellis Networkの提案。WaveNetに近い構成だが、各層のActivationを行う際に都度元の入力を加えると共に、畳み込みの重みを各層で共有している。これにより、展開されたRNNと同等の処理が行われるようになっている。
964	Deep Imitative Models for Flexible Inference, Planning, and Control	https://arxiv.org/abs/1810.06682	Nicholas Rhinehart, Rowan McAllister, Sergey Levine	模倣学習とモデルベースの学習を組み合わせるという、ありそうでなかった手法の提案。「ある目的地が与えられた場合にエキスパートが取る軌跡」(事後確率)を、エキスパートの行動から学習した状態遷移モデル(事前確率)＋状態/軌跡がゴールへとつながる確度(尤度)の2つから学習する。
965	Supervising strong learners by amplifying weak experts	https://arxiv.org/abs/1810.08575	Paul Christiano, Buck Shlegeris, Dario Amodei	複雑なタスクについて、人間のサポートを受けながら、対象のタスクをブレークダウンして学習する手法の提案。人の役割はタスクをばらすことと実際に解くことで、タスクの分解方法/回答を学習した教示役(Amplifier)を使って教師ありで学習を行う。実験自体はまだ簡単なタスクでしか行っていない。
966	Do Deep Generative Models Know What They Don't Know?	https://arxiv.org/abs/1810.09136	Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, Balaji Lakshminarayanan	生成モデルは入力データの分布をモデル化するため、外れ値(out-of-distribution)への対応にも強いと考えられていた。ところが、実験してみると学習したデータよりも学習していないデータに対し高い尤度を割り当てる現象が見られた(CIFAR-10/SVHNで確認)。ただ、これが一般的な現象なのかは要検証。
967	Knows When it Doesn’t Know: Deep Abstaining Classifiers	https://openreview.net/forum?id=rJxF73R9tX	?	学習データに含まれるノイズの影響を軽減する手法。分類クラスに「不要クラス」を一つ追加し、イメージ的にはモデルがそこにデータを「捨てる(Abstain)」ことを許容する形で学習を行う。既存のsoftmaxに不要クラス確率を組み込んだ式＋捨てすぎ抑制の項というlossで学習する
968	Learning sparse transformations through backpropagation	https://arxiv.org/abs/1810.09184	Peter Bloem	疎な重みを学習する手法の提案。密(Dense)より疎の方が、学習が早く解釈性も高くなる。疎な行列はインデックスと値で表現するのが効率が良いが、インデックスは離散値なので勾配法での学習は困難。そのためインデックスを正規分布で表現し(分散=0になると単一値に収束)連続値化することで学習する。
969	Learned Cardinalities: Estimating Correlated Joins with Deep Learning	https://arxiv.org/abs/1809.00677	Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, Alfons Kemper	SQLを最適化する際はカーディナリティ(戻される行数)を予測して実行計画が立てられるが、その予測をNNで行う手法の提案。SQL内の各要素は離散値で表現し、2層NNで処理したのち種別(テーブル、JOIN方式、条件式)ごとに集計し予測を行う。学習は教師ありだが学習用クエリを生成しデータ数を稼いでいる
970	Intrinsic Social Motivation via Causal Influence in Multi-Agent RL	https://arxiv.org/abs/1810.08647	Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro A. Ortega, DJ Strouse, Joel Z. Leibo, Nando de Freitas	マルチエージェントの強化学習で、他エージェントへの影響力を考慮するモデルの提案。影響力は行動とメッセージの2つで与える(行動とメッセージ発信2つのpolicyを持つ)。影響力は他エージェントの影響がない場合の行動vs実際の行動(影響あり)の差で計算する(行動間の相互情報量に近い形になる)。
971	graph2vec: Learning Distributed Representations of Graphs	https://arxiv.org/abs/1707.05005	Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, Shantanu Jaiswal	doc2vecの手法を、グラフに適用した手法。文書の長さが異なってもdoc2vecが使えるように、グラフサイズが異なっても表現が得られる。グラフ全体を文書・グラフからrootをもつサブグラフをサンプリングしたものを単語とみなし表現の更新を行う。コードの依存グラフからマルウェア検知を行っている。
972	Graph Convolutional Reinforcement Learning for Multi-Agent Cooperation	https://arxiv.org/abs/1810.09202	Jiechuan Jiang, Chen Dun, Zongqing Lu	Graph Convolutionを利用してマルチエージェントの強化学習を解く手法の提案。各エージェントの観測結果をノードとし、Graph Convolutionをかけた結果を観測情報と併せて各エージェントのQ-Networkに入力する。ただ、接続関係(Edge)をどう定義するかは環境に依存する。
973	Towards Efficient Large-Scale Graph Neural Network Computing	https://arxiv.org/abs/1810.08403	Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong Zhou, Yafei Dai	大規模なグラフでGraph Neural Networkの計算を行うための手法の提案。ノードからの伝播計算(Scatter)、エッジの特徴計算(ApplyEdge)、エッジ計算の集約によるノード特徴の計算(Gather)という3プロセスで行う。GPU/Multi GPU上で動くように最適化しており、数万〜数百万ノードのグラフで検証している
974	Sequence classification with human attention	http://aclweb.org/anthology/K18-1030	Maria Barrett, Joachim Bingel, Nora Hollenstein, Marek Rei, Anders Sgaard	文分類や文の誤りチェック(系列ラベリング)において、Attentionを人の教示により誘導する手法。実タスク(分類/ラベリング)とAttention予測2つのマルチタスクの形式で学習する(実際は交互に学習する)。教示は人の視線データからとっているが、文分類の文に対応する視線データは必要ない(交互学習により)
975	Quality Diversity Through Surprise	https://arxiv.org/abs/1807.02397	Daniele Gravina, Antonios Liapis, Georgios N. Yannakakis	進化戦略を改良する手法の提案。行動の質(Quality)、行動の新規性(Novelty)、行動の予想外性(Surprise)をミックスする。新規性と予想外性については、前者は平均からのズレ、後者は予測からのズレで評価する。評価に際しては、ベクトル値ではなく実際の行動(軌跡など)から行う。
976	One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks	https://arxiv.org/abs/1810.11043	Tianhe Yu, Pieter Abbeel, Sergey Levine, Chelsea Finn	複合タスクを、人のデモからOne-shotで学習するという研究。学習には人とロボットの個別タスクのデモが必要だが、同期している必要はない。学習時は、フェーズ(タスクの場面認識)とタスクの解き方の2点を学ぶ。予測時は、複合タスクがタスクのどの場面か認識し、それにより戦略を使い分ける。
977	Exploration by Random Network Distillation	https://arxiv.org/abs/1810.12894	Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov	強化学習で、未知の状態を探索するよう促す内発的報酬によりMontezumaを人間より高いスコアで攻略したという研究。手法はシンプルで、状態を入力とする予測問題の二乗誤差を報酬にする。問題自体はランダムでよく、状態への到達回数が多い=学習回数が多くなるほど誤差が少なくなる=報酬が少なくなる。
978	GraphIE: A Graph-Based Framework for Information Extraction	https://arxiv.org/abs/1810.13083	Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, Regina Barzilay	情報抽出にGraph Convolutionを使う手法。ノードは単語と文双方を想定しており、単語の場合は言及や共参照、文の場合には(SNSにおける)ユーザー間の関係を使っている。全体は系列ラベリングの問題として解いており、近傍の情報をBi-LSTMでとり(Encoder)、離れた情報をGCNを通じて得てDecoderで予測する
979	Importance of a Search Strategy in Neural Dialogue Modelling	https://uralik.github.io/beamdream/	Ilya Kulikov, Alexander H. Miller, Kyunghyun Cho, Jason Weston	対話システムにおいて、モデル本体だけでなく単語選択のSearchアルゴリズムも重要だという研究。長さを変えてBeam Searchを複数回行うiterativeな方法+対話データを基にしたスコア関数を定義(複数回は、並列で実行)。結果を見ると、どちらかというとスコア関数の方が効いている印象。
980	WaveGlow: A Flow-based Generative Network for Speech Synthesis	https://arxiv.org/abs/1811.00002v1	Ryan Prenger, Rafael Valle, Bryan Catanzaro	Glow+WaveNetの手法により高速な音声生成を行う手法。ネットワークを可逆変換にする(Glow)ことで、ノイズ=>音声の尤度と音声=>ノイズの尤度を等価にし、計算しやすい後者から前者を計算する。(可逆な)アフィン変換的な処理を行うが、このシフト量をWaveNet on メルケプストラムで計算する。
981	Multi-Hop Knowledge Graph Reasoning with Reward Shaping	https://arxiv.org/abs/1808.10568	Xi Victoria Lin, Richard Socher, Caiming Xiong	不完全な知識グラフ上での関係推定を、強化学習で行う手法。グラフが不完全でありグラフ上の関係=正しい保証がなため、別途事前学習した知識グラフの分散表現で到達できなかった場合でも蓋然性に応じ報酬を与える。また、冗長な関係パスを防ぐため行動を正則化(Dropout)するようにしている。
982	Efficient Metropolitan Traffic Prediction Based on Graph Recurrent Neural Network	https://arxiv.org/abs/1811.00740	Xiaoyu Wang, Cailian Chen, Yang Min, Jianping He, Bo Yang, Yang Zhang	Graph Neural Networkで交通量の予測を行うという手法。普通は道路をEdge、交差点をNodeにするが、これだとNodeに情報がない。そのため、道路をNodeにしてEdgeを道路間の接続(左折etc)にするモデル化を行っている。また時系列データからのGraph更新を行うため、ノードの更新にRNN(GRU)を適用している
983	A Stable and Effective Learning Strategy for Trainable Greedy Decoding	https://arxiv.org/abs/1804.07915	Yun Chen, Victor O.K. Li, Kyunghyun Cho, Samuel R. Bowman	翻訳においてSearch方法を学習する手法の提案。構成はシンプルでEncoderとDecoderの情報を元に、RNN的に系列を予測する。学習については、既存の翻訳モデルのSearch結果を各種指標(BLEU以外にMETEOR、TERなど)で評価し、最も高い系列を教師として使用する。効果は1pt前後。
984	Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation	https://arxiv.org/abs/1806.02473	Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, Jure Leskovec	Graph Convolutionと強化学習を組み合わせて、化合物を生成する研究。Graph Convolutionにより各ノードの特徴を計算し(結合方式を加味している)、行動としてLink Predictionの要領で2つのノードと結合方式を決定する。本物との識別性(敵対的loss)と結合ルール(原子価)、ドメイン固有の物性を報酬とする
985	Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control	https://arxiv.org/abs/1811.01848	2018/11/5	強化学習で、モデルベースの計画と価値関数を組み合わせた手法。行動は、モデルによる一定期間のシミュレーション結果から最良のパスの最初の行動を選択(MPC)。この時、状態価値は複数の価値関数をマージしたものを使う。行動結果はreplayに蓄積され、そこからノイズを加味した価値関数の更新を行う。
986	Language GANs Falling Short	https://arxiv.org/abs/1811.02549	Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, Laurent Charlin	テキスト生成において品質と多様性という2面で評価を行う方法と、その評価において基本的な最尤推定のモデル(MLE)がGANベースよりも優位であることを示した研究。softmaxのtemperatureは高いと確率が等しい=多様性に寄与、低いと決定的=品質に寄与するので、これを変化させた時のカーブで比較する。
987	Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?	https://arxiv.org/abs/1811.02553	Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry	PPO/TRPOといった戦略系の強化学習を検証した研究。勾配は多くの状態/行動ペアから算出した理論値から程遠く、Advantageによる勾配の安定も微少で(ただ獲得報酬には大きな影響がある)、双方の手法がアルゴリズム的に狙った目標は全く達成していない結果。報酬が獲得できる理由は別にあるのではという
988	Measuring the Effects of Data Parallelism on Neural Network Training	https://arxiv.org/abs/1811.03600	Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, George E. Dahl	バッチサイズと学習ステップ数の関係を調べた研究。画像/テキスト双方のデータセットとモデル、また最適化方法など様々な要因を変え検証を行なっている。バッチサイズは学習ステップ数の削減に寄与するが、その効果はある点から漸減する。この点は256~1024ぐらいだが、先の要因により変動する。
989	Applying Deep Learning To Airbnb Search	https://arxiv.org/abs/1810.09591	Malay Haldar, Mustafa Abdool, Prashant Ramanathan, Tao Xu, Shulin Yang, Huizhong Duan, Qing Zhang, Nick Barrow-Williams, Bradley C. Turnbull, Brendan M. Collins, Thomas Legrand	Airbnbにおける検索ランキングの改善に、ニューラルネットを適用するプロジェクトの記録資料(タイトルは"Deep"が付いているが、実際は2層のNN)。検証の始め方、また効果が出なかったモデル、特徴エンジニアリングとの組み合わせ方など、実際にプロジェクトを進める上で有用な情報が詰められている。
990	Differentiable Monte Carlo Ray Tracing through Edge Sampling	https://people.csail.mit.edu/tzumao/diffrt/	Tzu-Mao Li, Miika Aittala, Frdo Durand. Jaakko Lehtinen	画像のレンダリングを微分可能にするという研究。これにより、影や鏡面への写り方が変わる様をニューラルネットで学習、シミュレートできるようになる。今まではオブジェクト等の境界面が微分不可能になるため適用が難しかったが、境界(エッジ)周辺のサンプリングから微分値を近似する手法を使っている
991	Pathologies of Neural Models Make Interpretations Difficult	https://people.csail.mit.edu/tzumao/diffrt/	Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber	モデルが自然言語を理解しているか検証した研究。「重要でない単語」を落として何が残るかをみており、結果として意味不明な単語が残る一方、モデルの確信度は高いままだった。欠損した入力に対してはエントロピーを高くする正則化を行なうことで、精度を維持したまま解釈性を上げる手法を提案している
992	DeepMasterPrints: Generating MasterPrints for Dictionary Attacks via Latent Variable Evolution	https://arxiv.org/abs/1705.07386	2017/5/21	指紋認証を突破する(騙す)画像をGANで生成するという研究。より騙しやすい画像を生成する潜在表現を、進化戦略で探索するという手法をとっている。
993	Blindfold Baselines for Embodied QA	https://arxiv.org/abs/1811.05013	Ankesh Anand, Eugene Belilovsky, Kyle Kastner, Hugo Larochelle, Aaron Courville	質問に対し1人称視点での探索から回答を行うEmbodiedQAについて(例: テーブルの下にある箱の色は?と聞かれたらテーブルの近くまで行って下を見て回答、など)、探索しなくても回答できることを示した研究(しかも精度はSOTA)。回答の内容に大きな偏りがあり、BOWからの線形予測で単純に正答できる。
994	Pitfalls of Graph Neural Network Evaluation	https://arxiv.org/abs/1811.05868v1	Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, Stephan Gnnemann	様々なグラフ系のネットワークについて同じ基準で評価を行った研究。具体的には、代表的なデータセットについて学習/評価の分割を固定でなくランダムに、学習プロセス(Optimizerや重みの初期化)を統一して評価している。オリジナルのGCNが優秀とのこと。
995	Understanding Back-Translation at Scale	https://arxiv.org/abs/1808.09381	Sergey Edunov, Myle Ott, Michael Auli, David Grangier	翻訳における、逆翻訳の有効性について検証した研究。逆翻訳では、学習する翻訳が日=>英の場合、英のみのコーパスを英=>日に戻す(逆翻訳)ことでデータの水増しを行う。逆翻訳時に、greedyでなくノイズを加えた方が良好な結果。これにより商用の翻訳サービスを上回る結果
996	Reward learning from human preferences and demonstrations in Atari	https://arxiv.org/abs/1811.06521	Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, Dario Amodei	エキスパートの教示のみ、報酬なしで強化学習を行う手法。エキスパートは短いプレイ(1.7秒ほど)のうちA/Bどちらが好ましいかを選択する。選択を予測するモデルを学習することでReplay Buffer内のランキングが可能になり、この擬似的な報酬とエキスパートと通常の行動との距離、正則化項から学習する
997	Robust Domain Adaptation By Augmented Cyclic Adversarial Learning	https://openreview.net/forum?id=HJxjSR5so7	2018/10/23	CycleGANにおける再構成制約を緩和した研究。CycleGANではS=>T=>S'と変換した際S/S'の同一性を見るが、これは「同ドメイン(スタイル)」かつ「同コンテンツ」である必要があり、データが少ない場合学習が難しかった。そこで「同コンテンツ」であればOKにする緩和を行なっている
998	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	https://arxiv.org/abs/1811.06031	Victor Sanh, Thomas Wolf, Sebastian Ruder	自然言語処理において関連のあるタスクを階層型に積んだマルチタスクモデルで、各タスクのSOTAを更新した研究。具体的には固有表現認識、固有表現の参照認識、その上に共参照/関係抽出の2つを積んでいる。モデル自体はGlove/ELMo/Character CNNを入力に多層のBi-LSTMを積んでいくというシンプルなもの
999	Curiosity Driven Exploration of Learned Disentangled Goal Spaces	https://arxiv.org/abs/1807.01521	Adrien Laversanne-Finot, Alexandre Pr, Pierre-Yves Oudeyer	環境の情報をβ-VAEで学習し、得られた潜在表現を元にCuriosity-drivenな探索を行うことで状態間の関係(状態Aの時行動xをしたらBになる、など)、また適切な中間ゴールをサンプリングするという研究。Colabによるデモが提供されている。
1000	How agents see things: On visual representations in an emergent language game	https://research.fb.com/publications/how-agents-see-things-on-visual-representations-in-an-emergent-language-game/	Diane Bouchacourt, Marco Baroni	機械学習で、画像と自然言語の関係認識を検証した研究。画像を見てそれが何か(猫など)を判定するSenderと、Senderの判定結果と画像を受け取りSenderの判定と一致しているか当てるReceiverで実験。全然違う画像同士でも、ノイズ画像同士でも攻略可能で、低レベルな画像特徴しかみていないという結果
1001	Unsupervised Transfer Learning for Spoken Language Understanding in Intelligent Agents	https://arxiv.org/abs/1811.05370	Aditya Siddhant, Anuj Goyal, Angeliki Metallinou	対話システムで、ラベルなしデータを有効に使う研究。ラベルなしの発話テキスト(音声認識システムの認識結果から収集)でELMoを学習してブーストするが、発話は短いため構成を簡素にしたELMoLを使用。学習後は独立して使用ではなく、発話意図/エンティティ推定のモデルに組み込み転移学習させる(ULMFiT)
1002	Natural Environment Benchmarks for Reinforcement Learning	https://arxiv.org/abs/1811.06032	Amy Zhang, Yuxin Wu, Joelle Pineau	強化学習において、学習は2Dのゲーム・現実は実カメラからの学習、というギャップをなくすための新しいベンチマークの提案。画像の理解を検証する、画像のマスクを外していく環境と、既存タスクの背景を実画像に置き換える2種類のタスクを提案。後者では既存手法がパフォーマンスを下げることを確認
1003	Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search	https://arxiv.org/abs/1811.06272	Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste Lespiau, Nicolas Heess	強化学習において因果推論の反事実モデルを利用することで、戦略の評価を行う研究。通常の戦略評価は当然エージェントの経験を基に行われるが、この場合「経験できたこと」でしか評価できずバイアスがかかる可能性がある。そのため、経験の可否を左右する因子を変数として織り込んだモデルで評価を行う
1004	Rethinking ImageNet Pre-training	https://arxiv.org/abs/1811.08883	Kaiming He, Ross Girshick, Piotr Dollr	ImageNetによる事前学習は学習速度の向上に寄与するが、精度/正則化の面ではスクラッチからの学習に比べ優位になるわけではない、という実験結果。また元が分類問題なので、セグメンテーションなど境界識別の転移にはあまり寄与しないとしている。
1005	Graph Refinement based Tree Extraction using Mean-Field Networks and Graph Neural Networks	https://arxiv.org/abs/1811.08674	Raghavendra Selvan, Thomas Kipf, Max Welling, Jesper H Pedersen, Jens Petersen, Marleen de Bruijne	グラフ構造の中から、中核的なサブグラフを抽出する研究。与えられた隣接行列の接続が、サブグラフに所属するかどうかをノード/エッジの特徴から判定する形になる。モデルとしてサブグラフの観測確率を最大化するNNであるMFAと、Graph Convolutionを使用している。肺や航空路の推定に使用している
1006	Learning from Demonstration in the Wild	https://arxiv.org/abs/1811.03516	Feryal Behbahani, Kyriacos Shiarlis, Xi Chen, Vitaly Kurin, Sudhanshu Kasewa, Ciprian Stirbu, Joo Gomes, Supratik Paul, Frans A. Oliehoek, Joo Messias, Shimon Whiteson	模倣学習による運転操作の学習で、定点カメラの映像からエキスパートの軌跡を収集する研究。定点カメラの映像と衛星写真の映像を比較しカメラのキャリブレーションを行い、物体検知(Mask R-CNN)結果をもとにトラッキングし軌跡を作成する。この軌跡を再現できるシミュレーター上で模倣学習(GAIL)を行う
1007	CubeNet: Equivariance to 3D Rotation and Translation	https://arxiv.org/abs/1804.04458	Daniel Worrall, Gabriel Brostow	3Dオブジェクトの畳み込みで、回転などをしても同じ物体だと認識させることを目的とした研究。手法としては回転の特徴を得るためにフィルタ自体を回転させ畳み込みを行なっている。回転は24が定義されているが、等価な回転が存在するためそれについては計算を省略している。
1008	Unsupervised Word Discovery with Segmental Neural Language Models	https://arxiv.org/abs/1804.04458	Kazuya Kawakami, Chris Dyer, Phil Blunsom	教師なしで文字列から単語境界(セグメント)を学習する手法。単語内の文字同士は関連するが単語同士は関連しないと仮定を置き(semi-Markov)、文字ベースのLSTMとKey-Valueのメモリ機構を用いて境界の予測を行う。ただメモリに依存し学習データを覚えきるのを抑制するため単語長による正則化を入れている
1009	Training Compact Neural Networks with Binary Weights and Low Precision Activations	https://arxiv.org/abs/1804.04458	Bohan Zhuang, Chunhua Shen, Ian Reid	ニューラルネットの軽量化についての研究。低精度/低ビット幅の活性化演算ではサイズが小さくなる代わりに精度が低くなるのが常だったが、既存のレイヤー単位の演算を、演算ブロックを複数組み合わせた計算で代替することで精度を維持している。
1010	Learning Actionable Representations with Goal-Conditioned Policies	https://arxiv.org/abs/1811.07819	Dibya Ghosh, Abhishek Gupta, Sergey Levine	強化学習における状態の表現について、状態の近さよりも状態に到るまでの軌跡の近さに着目した研究。これにより見た目は非常に近い一方、到達経路が異なるケースを区別できる。状態到達への不確実性(確率)をうまくモデル化できるMaxEntropyの戦略から状態に到る行動分布を算出し、分布の差異で学習する
1011	State Representation Learning for Control: An Overview	https://arxiv.org/abs/1802.04181	Timothe Lesort, Natalia Daz-Rodrguez, Jean-Franois Goudou, David Filliat	強化学習で用いられる、状態表現の作成について手法をまとめたサーベイ。単純な状態圧縮としてAutoEncoder、状態/行動から次の状態を予測させる(forward)、状態/遷移先から行動を逆算する(inverse)、なんらかの分布の仮定(prior)、といった分類+その他でまとめられている。
1012	GAN Dissection: Visualizing and Understanding Generative Adversarial Networks	https://arxiv.org/abs/1811.10597	David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba	GANによる生成過程において、どのユニットがどのクラス(ドアや木)の生成に寄与しているかを解析する手法を提案。ユニットの生成結果(特徴マップ)のヒートマップと画像のセグメンテーションのオーバーラップ(IoU)を比較し特定を行う。これを応用した生成(対象クラスを出し入れする)も行なっている
1013	Collaging on Internal Representations: An Intuitive Approach for Semantic Transfiguration	https://arxiv.org/abs/1811.10153	Ryohei Suzuki, Masanori Koyama, Takeru Miyato, Taizan Yonetsuji	画像の、任意領域をあるクラスに寄せられるようにするという研究)犬の目を猫っぽくするなど)。GANによる生成で、Batch Normalizationの要領で正規化をしつつ対象クラスの平均/分散によせる、ストレートに空間特徴をミックスするという2つの手法が提案されている。
1014	Hierarchical visuomotor control of humanoids	https://arxiv.org/abs/1811.09656	Josh Merel, Arun Ahuja, Vu Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva Tirumala, Nicolas Heess, Greg Wayne	人型モデルのコントロールについて、一定時間の短い行動を学習した戦略を、メタ的な戦略で組み合わせることで解く研究。組み合わせについて、単にどの戦略かだけでなく、戦略のどのパート(一定時間のうちどこの部分か)を選択できるようにしている。
1015	Understanding the impact of entropy in policy learning	https://arxiv.org/abs/1811.11214	Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, Dale Schuurmans	Policy Gradientの学習が難しいのは、勾配の見積り精度の問題ではなく、解に至る勾配局面がフラットになっているからだという指摘。目的関数へのentropyの導入は、行動を多様化させるという当初の目的ではなく、局所解(最適とは限らない)を発生させ勾配をつけるという点で効果があるとしている。
1016	Partial Convolution based Padding	https://arxiv.org/abs/1811.11718	Guilin Liu, Kevin J. Shih, Ting-Chun Wang, Fitsum A. Reda, Karan Sapra, Zhiding Yu, Andrew Tao, Bryan Catanzaro	画像タスクにおけるPaddingの改善。通常は周辺を0で埋めて畳み込むことが多いが、畳み込み領域における0でない領域の割合を重みとして適用するという手法を取っている(これは、穴あきの画像処理で用いられる手法を参考にしている)。VGG/ResNetといったモデルで精度が改善することを確認。
1017	Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects	https://arxiv.org/abs/1811.11553	Michael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, Anh Nguyen	画像認識モデルが、データセット内の特定ポーズに過適合しているのではという話。画像内の物体を回転させると、認識精度が低下することを確認。xyzそれぞれの方向で10度以下の回転でも誤識別させることができる。回転画像を学習させることで対応できるが学習していないポーズへの汎化は難しいとのこと
1018	Graph R-CNN for Scene Graph Generation	https://arxiv.org/abs/1808.00191	Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, Devi Parikh	画像がどういうシーンか理解するために、画像における物体間の関係を捉えるモデルを提案。物体検知・関係の枝刈り・Node/Edgeのクラス推定という3ステップで処理を行なっており、最終ステップにAttention付きGraph Convolutionを使用している。データセットとしては、Visual Genomeを使用している。
1019	Dataset Distillation	https://arxiv.org/abs/1811.10959	Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, Alexei A. Efros	モデル側ではなくデータセット側を蒸留するという試み。MNISTでは10件のデータで94%、CIFAR-10では100件で54%の精度を達成している。蒸留後のデータによるパラメーター更新で、実データにおける目的関数の値が下がるよう、蒸留後データとパラメーター更新の学習率を学習する
1020	IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis	https://arxiv.org/abs/1807.06358	Huaibo Huang, Zhihang Li, Ran He, Zhenan Sun, Tieniu Tan	VAEに、GANにおけるDiscriminatorの批評機能を組み込んだ研究。実データを再構成できるようにする(Auto Encoder学習)と共に、生成結果が事前分布に近くなるように学習する。実データに従うようにするvs生成結果に従うようにする、という形で内部にmin-maxの仕組みを持っている。
1021	A Retrieve-and-Edit Framework for Predicting Structured Outputs	http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs	Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, Percy S. Liang	ソースコードのような構造を持つ文を生成する際は、ゼロから生成するよりも既存のものをコピーしてきて編集したほうが楽、ということで抽出を行うRetrieverと編集を行うEditorの2つで生成を行っている研究。前者はEncoder/Decoder(Encode結果で抽出を行う)、後者はseq2seqで構築している。
1022	Video-to-Video Synthesis	https://arxiv.org/abs/1808.06601	Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, Bryan Catanzaro	セグメンテーションを行った動画から、実動画を生成する研究。条件付けからの生成(Conditional GAN)を、時系列に沿い連続的に、しかも高解像度で行うという点に挑戦している。前タイムステップの画像を入力に取る、時系列の真贋を判定するDの追加などの工夫が取られている。
1023	Randomized Prior Functions for Deep Reinforcement Learning	https://arxiv.org/abs/1806.03335	Ian Osband, John Aslanides, Albin Cassirer	強化学習において、固有のノイズが加えられた複数エージェントのアンサンブルを取る手法の提案。イメージ的には各エージェントがスロットで、優秀と見込まれるエージェントを引く(=採用する)形になる(Thompson sampling)。
1024	Sanity Checks for Saliency Maps	http://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps	Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim	画像認識モデルの判断根拠を知るための手法であるSaliency Mapについて、説明がどう変化するかを調べた研究。Saliency Mapはモデルの重みに対する影響を受けるはずだが、一部のWeightを初期化しても結果は変わらなかった。また一部のデータに対しラベルをランダムにしても説明が変わらないという結果。
1025	Compositional Imitation Learning: Explaining and executing one task at a time	https://arxiv.org/abs/1812.01483	Thomas Kipf, Yujia Li, Hanjun Dai, Vinicius Zambaldi, Edward Grefenstette, Pushmeet Kohli, Peter Battaglia	模倣学習においてエキスパートの行動を潜在表現にする際、行動をパートに分解するという手法(物をつかむ/持ち上げる/移動する、など)。基本はVAEで、Encodeを行う際にEncodeパートの境界も予測させる(境界数は指定する)。学習させる際は、予測対象外(境界外)にMaskをかけてBackpropを行う。
1026	Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog	https://research.fb.com/publications/cross-lingual-transfer-learning-for-multilingual-task-oriented-dialog/	Sebastian Schuster, Sonal Gupta, Rushin Shah, Mike Lewis	対話において、少ないデータしかない言語への効率的な転移方法を調べた研究。学習データを翻訳、多言語対応の分散表現の使用、翻訳タスクにAuto Encoderタスクを追加(元文を復元させる)の3つを検証。対話以外のコーパスが一定量ある場合は分散表現、そもそもない場合＋Auto Encoderが有効そうとの結果
1027	Compositional Attention Networks for Machine Reasoning	https://arxiv.org/abs/1803.03067	Drew A. Hudson, Christopher D. Manning	論理的推論を行うCLEVRのタスクで、98.9%というほぼ限界の精度を達成した研究。RNNのセルを工夫しており(MACと命名)、Read/Write/Controlという3種の機構を持つ。Controlが質問を受け取りRead/Writeを制御する。Readは質問対象の画像からの読出しを行い、Writeは読出しを行うための内部情報を更新する
1028	Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures	https://arxiv.org/abs/1812.01647	Jonathan Uesato, Ananya Kumar, Csaba Szepesvari, Tom Erez, Avraham Ruderman, Keith Anderson, Krishmamurthy (Dj)Dvijotham, Nicolas Heess, Pushmeet Kohli	自動運転など失敗が許されないシーンにおける、強化学習のテスト方法についての提案。通常の状態ではほとんど成功してしまうため、敵対的サンプル生成のように失敗しやすい初期状態を探索することでアルゴリズムの安定性を評価する。
1029	CornerNet: Detecting Objects as Paired Keypoints	https://arxiv.org/abs/1808.01244	Hei Law, Jia Deng	物体検知において、左上と右下だけ予測するCornerNetの提案。左上と右下は別々に予測を行い(ネットワークを分岐させる)、左上/右下の潜在表現が近しい場合同じ物体のもの(=ペア)とする。MS COCOにて、One-Stageの手法の中ではSOTAを達成。
1030	ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst	https://arxiv.org/abs/1812.03079	Mayank Bansal, Alex Krizhevsky, Abhijit Ogale	自動運転車であるWaymoで使われているアルゴリズムの紹介。RNNがベースとなっており、自身の軌跡(将来10点)を予測する。入力はセンサー情報直ではなく、他のネットワークなどで加工した中間表現が使用される(この中に、他の物体の動きを予測するPerceptionRNNが含まれる)。
1031	Grasp2Vec: Learning Object Representations from Self-Supervised Grasping	https://arxiv.org/abs/1811.06964	Eric Jang, Coline Devin, Vincent Vanhoucke, Sergey Levine	強化学習において、演算可能な表現を学習・活用した研究。「シーンA-物体X=シーンB」というように、物体の移動をベクトル(表現)の演算で表現できるよう学習する。学習した表現を使用し実行した行動を表現することができるため、表現演算による補助報酬を与えている。
1032	A Style-Based Generator Architecture for Generative Adversarial Networks	https://arxiv.org/abs/1812.04948	Tero Karras, Samuli Laine, Timo Aila	Style Transferを利用した画像生成の提案。潜在表現から直接生成せずに、一旦学習済みのネットワークでスタイルベクトルに変換する。その後、スタイルベクトルとノイズを畳み込み処理のブロックに都度入力することで生成を行う。
1033	Learning Latent Dynamics for Planning from Pixels	https://arxiv.org/abs/1811.04551	Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson	モデルベースの学習で、POMDP的な仕組みを考慮した手法。(観測が不十分なため)状態遷移、また報酬は確率的であるとし、同じ状態でもばらつくようにしている。具体的には、状態遷移/報酬の出力を(隠れ層から)行う際、分布を経由している。学習時は複数ステップの予測をさせることで汎化性能を上げている
1034	An Empirical Model of Large-Batch Training	https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/science-of-ai/An+Empirical+Model+of+Large-Batch+Training.pdf	Sam McCandlish, Jared Kaplan, Dario Amodei	勾配のノイズ(異なるバッチ間での分散)と、バッチサイズの関係について調査した研究。ノイズが大きくなるほど、大きいバッチサイズが利用できる傾向がある。直感的に難しいタスクほどノイズが大きくなる傾向がある。これは学習が進むにつれ(=難しいタスクを解こうとするにつれ?)ても同様の傾向がある
1035	Contextual String Embeddings for Sequence Labeling	https://aclanthology.coli.uni-saarland.de/papers/C18-1139/c18-1139	Alan Akbik, Duncan Blythe, Roland Vollgraf	文字ベースの言語モデルを使い、固有表現認識のSOTAを達成した研究。Bi-directionalの文字ベース言語モデルを使用し、文頭=>単語終了までのforward、文末=>単語始点までのbackwardを結合して単語表現を作成している。作成した単語表現をBi-directionalのCRFに入れて予測を行う
1036	Linguistically-Informed Self-Attention for Semantic Role Labeling	https://arxiv.org/abs/1804.08199	Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, Andrew McCallum	Multi-Headのモデルに文法的な知識を組み込んだ研究。述語に対する主語/目的語などの特定(Semantic Role)でSOTAを大幅に更新。係り受け/POS/述語予測を同時に解かせるマルチタスク、Attention headの一つを係り受け先の重みが高くなるよう誘導するといった工夫を行なっている
1037	Bayesian Optimization in AlphaGo	https://arxiv.org/abs/1812.06855	Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, Nando de Freitas	AlphaGoで使われたハイパーパラメーターサーチの手法。最適化前後では勝率が50%から66.5%へと向上したという。手法はベイズ最適化を使用しており、Gaussian processを使い過去のモデルより勝率が高くなりそうなパラメーターを提案する。
1039	Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks	https://arxiv.org/abs/1812.07252	Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, Konstantinos Bousmalis	シミュレーターを本物に近づけるのでなく、本物の画像を(Generatorで)シミュレーターに近づける形で強化学習を行った研究。Generatorの学習には本物の画像ではなくテクスチャーや色、光の具合などをランダムに変更した画像を使用。これで物をつかむタスクで70%の成功率、本物の画像を使えば91%となった
1040	Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering	https://openreview.net/forum?id=HkfPSh05K7	Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Andrew McCallum	複数文書にまたがるQA(文書読解)を、スケーラブルに行う手法の提案。質問をベクトル化=>関連文書(パラグラフ)の検索=>質問と文書のマッチを行い、関連個所の抽出とクエリの更新を行う=>(検索に戻る)、という形で読解を行う。周回を行うことで、最初の検索が間違っていた場合でも修正が行えるとしている
1041	Deep reinforcement learning with relational inductive biases	https://openreview.net/forum?id=HkxaFoC9KQ	Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, Peter Battaglia	強化学習において、環境における局所特徴の関係を捉えることで学習効率を上げる試み(敵と自機の位置関係などが捉えられることを期待している)。CNNで畳み込んだ結果のチャネル方向のベクトルをエンティティとして、Multi Head Attentionで関係をとっている。そこから戦略/価値双方の計算を行なっている
1042	Learning to Understand Goal Specifications by Modelling Reward	https://openreview.net/forum?id=H1xsSjC9Ym&noteId=BkeNfekT27	Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, Edward Grefenstette	強化学習で、エキスパートの行動への近さで報酬を与える手法の提案。「エキスパートの行動への近さ」はGANの枠組みで判定しており、模倣学習のGAILと近い。GAILとの差分としては、戦略の学習にエキスパートの行動を使う点、軌跡ではなく状態で報酬を与える点、報酬の離散化の3点が挙げられている
1043	How Powerful are Graph Neural Networks?	https://openreview.net/forum?id=ryGs6iA5Km	Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka	Graph Neural Netについて、その処理の意味と効果をきちんと測ろうという研究。GNNの各処理を、周辺ノードの集約(AGGREGATE)、集約結果の伝搬方法(COMBINE)、結果の出力方法(READOUT)などの形で抽象化している。さらに、識別性能の程度とそれが発揮できる/できないケースを検証している。
1044	Supervised Community Detection with Line Graph Neural Networks	https://openreview.net/forum?id=H1g0Z3A9Fm&noteId=rklhGf2ggN	Zhengdao Chen, Lisha Li, Joan Bruna	Graph Neural Netでコミュニティの検知(ノードの分類)を行った研究。接続によるノードの更新と、ノードによるエッジの更新(エッジ同士の接続は、後戻りなし=non-backtrackingとする)を交互に繰り返す形でノード特徴を作成していく(多分)。
1045	Learning Context-Sensitive Convolutional Filters for Text Processing	https://arxiv.org/abs/1709.08294	Dinghan Shen, Martin Renqiang Min, Yitong Li, Lawrence Carin	自然言語処理において、CNNのフィルタも学習させる試み(感覚的には、positive/negativeに反応するフィルタを作るなど)。手法自体はストレートで、フィルタの生成機構とフィルタを使った分類機を同時に学習させる。質問回答への応用も行っている。
1046	Graph Neural Networks: A Review of Methods and Applications	https://arxiv.org/abs/1812.08434	Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Maosong Sun	Graph Neural Network (Graph Convolution)に関する研究の総まとめ資料。既存の手法の分類と解説、また適用領域(TextやImageなど)ごとの研究がまとめられている。
1047	Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter	https://arxiv.org/pdf/1809.00120.pdf	Lijun Wu 1, Xu Tan 2, Di He 3, Fei Tian 2, Tao Qin 2, Jianhuang Lai 1 and Tie-Yan Liu 2	ニューラル機械翻訳において生成するにつれて性能が低下する現象について、エラー伝播の影響と言語特徴の面から分析を行った。生成順を逆転させる実験からエラー伝播だけが性能低下を招いているのではないこと、Teacher forceによるエラー伝播の存在有無、様々な言語同士で翻訳することによる言語特徴の影響を確認した。結果としてエラー伝播は性能低下の一因として存在するが、言語特徴も影響を及ぼしていることがわかった。
1048	What do you learn from context? Probing for sentence structure in contextualized word representations	https://openreview.net/forum?id=SJzSgnRcKX	Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, Ellie Pavlick	ELMoやCoVeといった文脈情報を含む分散表現を評価した研究。評価には一定スパンの単語の集まりにラベルをつけるタスクを用いている(品詞、固有表現、Semantic Roleなど)。結果はELMoが強いが、何がその要因なのかについて、文法/局所特徴(CNN)など様々なベースラインと比較することで検証している。
1049	Analysis Methods in Neural Language Processing: A Survey	https://arxiv.org/abs/1812.08951	Yonatan Belinkov, James Glass	DNN系の自然言語処理のモデルを評価する方法についての体系的なまとめ。モデルの解析による評価(Attentionなど)、モデルのパフォーマンスによる評価(評価セットに対するスコア)、敵対的サンプルによる評価といった3つの観点で研究が整理されている。
1050	Learning Unsupervised Learning Rules	https://openreview.net/forum?id=HkNDsiC9KQ	Luke Metz, Niru Maheswaranathan, Brian Cheung, Jascha Sohl-Dickstein	教師なし学習の学習過程を、メタラーニングで学習しようという研究。完全教師なしというよりはFew-Shotに近く、数件のラベル付きデータに対する誤差が、複数のバッチにまたがり最小化されるよう学習する。学習に際し、誤差逆伝播を用いず重みの調整量を学習するNNを別途作成するという構成をとっている
1051	Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond	https://arxiv.org/abs/1812.10464	Mikel Artetxe, Holger Schwenk	多言語対応の文表現を学習する試み。93言語をサポートしており、異なる言語圏、文字種も含まれている。全言語共通のBytePairの辞書を作り、Bi-directionalの翻訳モデルに入れるという構成。これにより各言語のzero-shotによる分類で高い精度を記録(ただ日本語は低い)。
1052	Hyperbolic Deep Learning for Chinese Natural Language Understanding	https://arxiv.org/abs/1812.10408	Marko Valentin Micic, Hugo Chu	中国語で、文字の埋め込み表現を双曲空間上で作成した研究。対話意図の推定で、文字ベースよりも高い精度を記録している。基本はskip-gramだが、双曲空間のため内積はローレンツ内積、最適化はリーマン多様体上でのSGDを使用している。
1053	SlowFast Networks for Video Recognition	https://arxiv.org/abs/1812.03982	Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He	ビデオ中の動作の分類などについて、低頻度(Slow)と高頻度(Fast)のネットワークを組み合わせて認識を行った研究。Slow側は時系列のストライドが大きく、Fast側はより細かいストライドで畳み込みを行う。Slow側は(動作に影響しない)空間的特徴、Fast側はモーションの特徴を認識させることを狙っている
1054	Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity	https://openreview.net/forum?id=r1lrAiA5Ym	Thomas Miconi Aditya Rawal, Jeff Clune, Kenneth O. Stanley	脳の可塑性(ヘブ則: 信号が繰り返し発火する場合、少ない信号でも発火するようにシナプス間の結合が変化すること)をニューラルネットに組み込む研究。伝播を行う際、過去出力の強度を記憶しておくような機構(Hebb)をかませることでこれを実現している。
1055	Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling	https://arxiv.org/abs/1812.10860	Samuel R. Bowman, Ellie Pavlick, Edouard Grave, Benjamin Van Durme, Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, Berlin Chen	自然言語処理においてどんな事前学習が有効か調べた研究。ELMoの構造をベースに、文の意味理解に関する9つのタスクを集めたGLUEを始め様々なタスクで事前学習を行い性能を評価している。結果は言語モデルが優秀で、マルチタスクについては対象タスクとの相性があるよう
1056	Assessing BERT’s Syntactic Abilities	http://u.cs.biu.ac.il/~yogo/bert-syntax.pdf	Yoav Goldberg	BERT( 
1057	Neural Ordinary Differential Equations	https://arxiv.org/abs/1806.07366	Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud	系列データを扱う伝播プロセスは、隠れ層を「都度」再帰的に更新する形で行われる。この「都度」のステップを限界まで細かくすると微分と考えることができ、隠れ層の更新を連続的な形で定義することができる。連続的になることでBPTTのような段階的な勾配計算が不要になる。
1058	Learning Graph Embedding with Adversarial Training Methods	https://arxiv.org/pdf/1901.01250.pdf	Shirui Pan, Ruiqi Hu, Sai-fu Fung, Guodong Long, Jing Jiang, and Chengqi Zhang, Senior Member, IEEE	グラフ畳み込みを理解するために敵対的学習を行うフレームワーク adversarially regularized graph autoencoder (ARGA) を作った。Encorder に Graph Convolutional Neural Network を、Decorder でグラフの再構築を行っている。
1059	Pay Less Attention with Lightweight and Dynamic Convolutions	https://openreview.net/forum?id=SkVhlh09tX	Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, Michael Auli	Self-Attentionは強力だが、1:N(自分vsその他)の計算が必要でコストが高い。単にAttentionをタイムステップごとに予測するだけでも、同等の性能が得らるという研究。畳み込みはDepthwiseでカーネルの重みをAttentionライクに正規化するのが基本だが、この重みを動的に計算する
1060	Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions	https://arxiv.org/abs/1901.01753	Rui Wang, Joel Lehman, Jeff Clune, Kenneth O. Stanley	強化学習で、環境側を変化させることで学習を行う手法。エージェントと環境をペアとし、学習結果から難しすぎない/簡単すぎない環境を遺伝的アルゴリズムにより生成する。生成した複数の環境とエージェントをペアにし学習、を繰り返す。エージェントは前回を持ち越し/1stepの進化戦略で転移の2種類
1061	Advancing the State of the Art in Open Domain Dialog Systems through the Alexa Prize	https://arxiv.org/abs/1812.10757	Chandra Khatri, Behnam Hedayatnia, Anu Venkatesh, Jeff Nunn, Yi Pan, Qing Liu, Han Song, Anna Gottardi, Sanjeev Kwatra, Sanju Pancholi, Ming Cheng, Qinglang Chen, Lauren Stubel, Karthik Gopalakrishnan, Kate Bland, Raefer Gabriel, Arindam Mandal, Dilek Hakkani-Tur, Gene Hwang, Nate Michel, Eric King, Rohit Prasad	対話ボットの開発コンテストであるAmazon Alexa Prizeの中で生まれた開発ツールキット(CoBot)と、研究成果の紹介(CoBotはまだ一般には公開されてないよう)。対話システムにおける手法だけでなく、実践的なインフラ構成についてまで知ることができる良い資料。
1062	FIGR: Few-shot Image Generation with Reptile	https://arxiv.org/abs/1901.02199	Louis Clouatre, Marc Demers	GANでFew-Shotの生成を行うという研究。複数のタスクを学習することで良い初期値を得るMAMLの改良版であるReptile(
1063	An Introduction to Deep Reinforcement Learning	https://arxiv.org/abs/1811.12560	Vincent Francois-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau	深層強化学習について解説した本。Rainbow/TRPO・PPOあたりの深層強化学習の手法について良くまとまっている。
1064	Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context	https://arxiv.org/abs/1901.02860	Zihang Dai, Zhilin Yang, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov	Transformerでは入力を固定長にする必要があるため、言語モデルでは限定された長さの文脈しか扱えなかった。そこで前回の隠れ層を引き継ぐ手法の提案。前回/今回の位置関係を把握させるためにrelativeなposition encodingを使う＋位置情報による読み出しを学習可能にしている
1065	Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation	https://arxiv.org/abs/1901.02985	Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan Yuille, Li Fei-Fei	画像のセグメンテーションタスクでも、構造の自動探索を行なったという研究。既存の探索はセル構造のみ探索してそれを規定のネットワークに当てはめる形だったが、本研究ではネットワーク構造についても深さ/チャンネル数という軸で探索を行なっている。この結果いくつかのタスクでSOTAを更新している
1066	Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods	https://arxiv.org/abs/1802.10264	Deirdre Quillen, Eric Jang, Ofir Nachum, Chelsea Finn, Julian Ibarz, Sergey Levine	ロボットアームで物をつかむタスクについて、シンプルなものも含め様々なアルゴリズムでパフォーマンスを計測した研究。結果として、素のMonte Carlo法でもDDPGと同程度/タスクによっては超えるパフォーマンスが得られることを確認。
1067	Named Entity Recognition With Parallel Recurrent Neural Networks	http://aclweb.org/anthology/P18-2012	Andrej Zukov-Gregori  c, Yoram Bachrach, Sam Coope	固有表現認識において、複数のLSTMを並列で実行しその結果を結合して全結合層から予測する、というシンプルな構成でSOTAを達成したという研究。各LSTMは、学習内容が別個のものになるようコンテキストの重みが直交するような正則化を掛けている。
1068	Sample Efficient Adaptive Text-to-Speech	https://arxiv.org/abs/1809.10460	Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan Wang, Luis C. Cobo, Andrew Trask, Ben Laurie, Caglar Gulcehre, Aron van den Oord, Oriol Vinyals, Nando de Freitas	メタラーニングの手法を用い少量データでの話者適用を行なった研究。複数話者の潜在表現をあらかじめ学習しておき、その表現を元に新規話者の潜在表現を作る(SEA-EMB)、さらに音声生成モデルをファインチューンする(SEA-ALL)2つを検証し、SEA-ALLが良好＋話者表現を作るEncoderよりも優秀なことを確認
1069	Human few-shot learning of compositional instructions	https://arxiv.org/abs/1901.04587	Brenden M. Lake, Tal Linzen, Marco Baroni	人間が、未知の単語や質問に対してどう対処しているのかを調べた研究。擬似的な単語をいくつか作成し、その組み合わせがどういう結果になるか、数個の例を示すだけでほとんどの人が使いこなせることを確認。また、単語列が与えられたときにどう反応することが多いのかについても調べている。
1070	Learning agile and dynamic motor skills for legged robots	http://robotics.sciencemag.org/content/4/26/eaau5872	Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun and Marco Hutter	複雑なモーター制御が必要となる多脚ロボットを、深層強化学習で動かしたという研究(この制御は難しく、Boston Dynamicsでも強化学習は使っていなかった)。厳密な物理モデルと、モーターへ信号を送る際のラグを学習させたネットワークの2つを使い学習し、物理ロボへ搭載する
1071	Adversarial Examples that Fool both Computer Vision and Time-Limited Humans	https://arxiv.org/abs/1802.08195	Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein	様々な機械学習モデルだけでなく人間でも間違うようなAdversarialを作ろうという研究。複数の画像認識モデルのアンサンブルでAdversarialを作成＋人間が識別できるような、ある程度大きい変化を入れるようにしている(各モデルには人間の網膜を参考に中央より周辺をぼやかすようなレイヤを追加)。
1072	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	https://arxiv.org/abs/1901.05415	Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, Jason Weston	対話ボットについて、学習して配置された後に、ユーザーとの対話から学習する手法の提案。相手の発話の満足度を予測し、満足なら対話結果を学習データとして登録し、不満足なら「どう言えばよかった？」と聞いてフィードバックされた内容を学習データとして登録する。データセットも公開されている
1073	Photo-Sketching: Inferring Contour Drawings from Images	https://arxiv.org/abs/1901.00542	Mengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer, Deva Ramanan	写真から、輪郭描画を生成する研究。Conditional GANをベースとして、1:Nのデータセット(一つの写真につき、輪郭候補が複数ある)を使用し各輪郭候補に対するDの判断のMEAN、Gとの差異(L1)のMINの2つを利用し学習させている(MM-loss)。併せて、アノテーションが楽しくなるようなGUIが提供されている。
1074	RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free	https://arxiv.org/abs/1901.03353	Cheng-Yang Fu, Mykhailo Shvets, Alexander C. Berg	Segmentationタスクでは従来ピクセル毎にクラスを推論していたが、物体の形状（ここではマスクと呼称）を推論するネットワークを付加することで性能向上が出来ることを示した。
1075	Latent Multi-task Architecture Learning	https://arxiv.org/abs/1705.08142	Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, Anders Sgaard	自然言語処理における、マルチタスクのモデルの提案。マルチタスクではネットワーク構造のうちどれくらいを共有・タスク個別にするか決める必要があるが、レイヤをサブスペースに分割し、レイヤのどのパート、またどのレイヤを重視するか学習できるような構造を提案している。
1076	Recent Advances in Efficient Computation of Deep Convolutional Neural Networks	https://arxiv.org/abs/1802.00939	Jian Cheng, Peisong Wang, Gang Li, Qinghao Hu, Hanqing Lu	デバイス上で効率的にDNNを実行する手法についてのサーベイ。ネットワーク構造の工夫だけでなく、ハード上の演算工夫についてもまとめられている。構造側は主に軽量化(枝刈りや低ランク近似、量子化など)が中心で、ハード側は高速化/省電力化などの観点から手法(loopのアンロールなど)をまとめている
1077	Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments	https://openreview.net/forum?id=Hkx5cU26kN	?	文のベクトル表現について、文法的なタスクがどれだけ解けるか検証した論文。CoLAという文法的に正しいか否かをアノテーションしたデータセットに対し、さらに細かい大分類15/小分類63の文法的特徴(述語句を含む/命令句から成るなど)を付与し検証。BERTが優秀だが、形態変化の検知に弱いという結果。
1078	Self-Driving Cars: A Survey	https://arxiv.org/abs/1901.04407	Claudine Badue, Rnik Guidolini, Raphael Vivacqua Carneiro, Pedro Azevedo, Vinicius Brito Cardoso, Avelino Forechi, Luan Ferreira Reis Jesus, Rodrigo Ferreira Berriel, Thiago Meireles Paixo, Filipe Mutz, Thiago Oliveira-Santos, Alberto Ferreira De Souza	自動運転車の技術についてのサーベイ。DARPA challengeで発表されたものが中心で、自動運転車の基本的な構成(検知/制御)と各箇所に関わる研究が整理され紹介されている。
1079	Cross-lingual Language Model Pretraining	https://arxiv.org/abs/1901.07291	2019/1/22	多言語対応の文表現を得る際、どんなタスクが良いのか検証した研究。ベースは言語モデルで、通常通り次の単語を予測する(Causal LM)、単語をdropした箇所を予測する(Masked LM)＋翻訳データがある場合に、並べた文でMasked LMを行うTranslation LMの計3つを提案。CLM<MLMで、TLMで強化できるという結果
1080	Causal Reasoning from Meta-reinforcement Learning	https://arxiv.org/abs/1901.08162	Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward Hughes, Peter Battaglia, Matthew Botvinick, Zeb Kurth-Nelson	因果推論を強化学習で解く試み。ある交絡因子(関係を持つと予想されるA/B双方に影響を与える因子)を含む因果関係について、情報収集=>関係に関する回答(正当で来たら報酬)というタスクを通じ学習する。モデルフリーのエージェントによる経験からメタモデル(RNN)を学習するメタラーニングの構成をとる。
1081	Open-ended Learning in Symmetric Zero-sum Games	https://arxiv.org/abs/1901.08106	David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech M. Czarnecki, Julien Perolat, Max Jaderberg, Thore Graepel	強化学習において、戦略を得るのでなく戦略空間を明らかにするという研究。常勝の戦略が存在しないケース(Aには強いがBには負ける、というようなトレードオフがある場合)は、更新を続けても最適戦略は得られない。そこで、戦略間の関係性=戦略空間を解き明かし、その均衡点から戦略を得る手法を提案。
1082	On the Dimensionality of Word Embedding	https://arxiv.org/abs/1812.04224	Zi Yin, Yuanyuan Shen	単語分散表現の適切なサイズを求める研究。「適切」の定義として単語分散表現全体として回転のようなunitary operatorを適用しても内積は変わらない(各単語ベクトルの位置関係は変わらない)とし、適用前後の差異を測るPIP lossを提案。これが最小となるサイズをよしとしている
1083	Computational Optimal Transport	https://arxiv.org/abs/1803.00567	Gabriel Peyr, Marco Cuturi	最適輸送問題(Optimal Transport)に関する解説資料(200pくらいある)。最適な輸送を行うには距離やコストの計測が必要で、GANで使われるWasserstein距離も出自はこの分野となっている。画像や機械学習への応用も意識されている。
1084	On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models	https://arxiv.org/abs/1810.12715	Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, Pushmeet Kohli	Adversarial Exampleに対する耐性を上げるための手法。サンプルに対しノイズを加えて学習するが、判断が誤った場合は上限のノイズ、逆に正しい判断の場合は下限のノイズを加えたとして学習する(加えるノイズは、徐々に増やしていく)。
1085	Learning to Design Circuits	https://arxiv.org/abs/1812.02734	Hanrui Wang, Jiacheng Yang, Hae-Seung Lee, Song Han	アナログICの回路設計を、強化学習で行なったという研究。回路全体と各素子(トランジスタ)を状態とし、トランジスタの大きさ/静電容量などのパラメーター決定を行動とする。シミュレーターを用いて要求スペック満たしているか確認し、それに基づき報酬を与える。モデルはDDPGを使っている。
1086	Glyce: Glyph-vectors for Chinese Character Representations	https://arxiv.org/abs/1901.10125	Wei Wu, Yuxian Meng, Qinghong Han, Muyu Li, Xiaoya Li, Jie Mei, Ping Nie, Xiaofei Sun, Jiwei Li	漢字のような象形文字について、画像情報も使って文字分散表現を得る研究。漢字の形は時代を経るにつれ簡略化されてきたためいくつかの旧字体も使う、文字を4象限(漢字ドリルにあるような4マス)に区切った特徴を使用するなどの工夫を行なっている。様々な自然言語タスクで、1~2ptの改善ができている
1087	Go-Explore: a New Approach for Hard-Exploration Problems	https://arxiv.org/abs/1901.10995	Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune	内発的報酬の手法を改善し、Montezumaで圧倒的なスコアを記録した手法。内発的報酬は新規の状態に対し与えられるため、探索ルートが深い場合入り口付近の探索が済んでしまうと奥まで行かなくなる問題があった。そこで探索した所まで戻る手法を提案(状態到達の難易度(確率)=奥ほど低いをベースに選択)
1088	Learning and Evaluating General Linguistic Intelligence	https://arxiv.org/abs/1901.11373	Dani Yogatama, Cyprien de Masson d'Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, Phil Blunsom	BERT/ELMoを題材に、自然言語処理モデルの転移能力を検証した論文。モデルがどれだけ早く学習結果を応用できるかを計測するスコアを提案(Learning Curveの収束速度を数値化したようなもの)。教師ありタスクを事前学習に含むと、F1が上がる一方このスコアが下がることを確認
1089	The Evolved Transformer	https://arxiv.org/abs/1901.11117	David R. So, Chen Liang, Quoc V. Le	Transformerの構造を、構造探索を用いて最適化したという研究。構造探索には進化戦略を使用しており、学習結果が良いものを親としてさらに子を生成する。子についてはより良いものに学習リソースを割り当てるために、一定ステップごとにハードルを設けて厳選している。大幅な小サイズ化ができている
1090	Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet	https://openreview.net/forum?id=SkfMWhAqYQ	Wieland Brendel, Matthias Bethge	DNNを使ってBag of Featureを作成するという研究。局所特徴それぞれから各クラスの分類確率を出力し、その合算で最終分類を決定する。局所特徴とクラス分類確率の対応が明確なので、説明力が高くなる。得られたSensitivity Mapは、VGGやResNetなどと近かったとのこと。
1091	Evolving intrinsic motivations for altruistic behavior	https://arxiv.org/abs/1811.05931	Jane X. Wang, Edward Hughes, Chrisantha Fernando, Wojciech M. Czarnecki, Edgar A. Duenez-Guzman, Joel Z. Leibo	マルチエージェントの強化学習で、協調するための内発的報酬を与えるという研究。エージェントと内発的報酬のネットワークは分かれており、エージェントのチーム(5 policy)と1報酬ネットワークを組み合わせてプレイを行い、獲得できた報酬により重みをつけて学習を行う(進化戦略的な更新 
1092	Fixup Initialization: Residual Learning Without Normalization	https://arxiv.org/abs/1811.05931	Hongyi Zhang, Yann N. Dauphin, Tengyu Ma	バッチ正規化(BN)を使わずとも重みの初期化だけで同程度の性能が出せるとした研究(画像分類/翻訳の双方で検証)。ResNet Blockは2ルートが合流する形をとるため基本分散が2倍となり勾配爆発が起こる。BNはブロック単位でこれを阻止するが、重みのスケーリングでこれを阻止する
1093	How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions	https://arxiv.org/abs/1902.00508	Goran Glavas, Robert Litschko, Sebastian Ruder, Ivan Vulic	多言語の分散表現について、よく利用されるBLI評価(A言語の単語から同じ意味のB言語の単語を予測させるタスクの精度)だけでなく、翻訳や文書分類といった後続タスクでの性能についても検証した研究。直交性を持たない変換を使う場合はBLIと後続タスクの精度が比例しないケースがあるため要注意とのこと
1094	Representation Learning on Graphs with Jumping Knowledge Networks	https://arxiv.org/abs/1806.03536	Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, Stefanie Jegelka	Graph Convolutionではレイヤ数を増やすことで考慮できる周辺ノードの半径を広げられるが、グラフ構造は均一でないためある段階から爆発的に増減する場合もあり、ノードの特徴に大きな影響を与える。そのためレイヤーごとの出力をマージするレイヤを設け調整する手法を提案
1095	An Effective Approach to Unsupervised Machine Translation	https://arxiv.org/abs/1902.01313	Mikel Artetxe, Gorka Labaka, Eneko Agirre	教師なしの翻訳で、フレーズベースの翻訳モデルとSeq-to-Seqベースの翻訳モデルを組み合わせた研究。フレーズベースについてはcross-lingualな分散表現を使うとともに、固有表現などにうまく対応するためsubwordの情報も使用している。構築したフレーズベースをSeq-to-Seqの学習に使いブーストしている
1096	Insertion-based Decoding with Automatically Inferred Generation Order	https://arxiv.org/abs/1902.01370	Jiatao Gu, Qi Liu, Kyunghyun Cho	自然言語の生成を行う際、別に左から右に順番で生成しなくてもよいのでは？という着想に基づいた研究。ベースはTransformerだが、予測においては単語と挿入位置(左or右)を予測させ、組み立てる形で文を生成する。
1097	Robust Machine Comprehension Models via Adversarial Training	https://arxiv.org/abs/1804.06473	Yicheng Wang, Mohit Bansal	既存のQAモデルは、文書内の単語を置き換えたり不正解の文を挿入するといったAdversarialな操作に弱かった。そこで、既存のAdversarial手法に対応するためのData Augmentationを行った研究。不正解文をランダムに挿入/WordNetを使い対義語に置き換えといった操作を行っている
1098	Analyzing and Improving Representations with the Soft Nearest Neighbor Loss	https://arxiv.org/abs/1902.01889	Nicholas Frosst, Nicolas Papernot, Geoffrey Hinton	クラス間のもつれ具合を計測するSoft Nearest Neighbor Lossを検証した研究(発表自体は2007年。ただ本研究では温度Tを加えている)。もつれは分類性能が高い場合当然低くなるが、GANなど本物/偽物が区別つかなくなる方が良い場合高い方がよくなる。こうした理論的な推論を、実際に実験して確認している
1099	Manifold Mixup: Learning Better Representations by Interpolating Hidden States	https://arxiv.org/abs/1806.05236	Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio	Inputではなく隠れ層で混ぜ合わせるMIXUPを提案。隠れ層における表現で混ぜ合わせると、非線型変換で特徴量がより分離しやすい状態でMIXUPにするのでよい。２つのデータの中間点データが良い決定境界を作りやすくなるため、敵対的サンプルにも強いようだ。
1100	Distilling Policy Distillation	https://arxiv.org/abs/1902.02186	Wojciech Marian Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant M. Jayakumar, Grzegorz Swirszcz, Max Jaderberg	強化学習における、戦略の蒸留についてまとめた研究。既存の蒸留手法について、どの時にどういうパターンが良いのかがまとめられている。蒸留を行うに際して、教師側と生徒側の確率分布の差を正則化として使うと良いとしている。
1101	A Differentiable Gaussian-like Distribution on Hyperbolic Space for Gradient-Based Learning	https://arxiv.org/abs/1902.02992	Yoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, Masanori Koyama	双曲空間上で、ガウスライク＋微分可能な分布を定義したという研究。双曲空間としてはPoincare´が代表的だが、こちらではLorentzを使用している。平均0/分散Σでサンプリングした点を、Lorentzモデルに沿うよう移動(Parallel transport)/転写(Exponential map)することで空間上の点を得ている。
1102	Deep Learning for Anomaly Detection: A Survey	https://arxiv.org/abs/1901.03407	Raghavendra Chalapathy, Sanjay Chawla	異常検知に深層学習を使用した研究のサーベイ。既存のサーベイは特定領域にフォーカスしたものが多かったが(動画や医療画像など)、本サーベイでは包括的なまとめを行い、また研究だけでなく産業などでの適用事例についてもまとめている。
1103	Language Models are Unsupervised Multitask Learners	https://blog.openai.com/better-language-models/	Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever	OpenAIが、以前公開した言語モデル(GPT 
1104	Learning Latent Dynamics for Planning from Pixels	https://danijar.com/publications/2019-planet.pdf	Danijar Hafner. Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson	表現学習とモデルベースを組み合わせた手法の提案。RNNベースの表現学習が基本だが、画面の再構成だけでなく系列からの予測が直前からの予測からあまり離れないよう正則化を入れている。似た研究としてWorld Modelsがあるが、こちらは戦略のネットワークがなく完全にモデルベースの計画で解いている。
1105	Network Semantic Segmentation with Application to GitHub	https://arxiv.org/abs/1902.05220	Neda Hajiakhoond Bidoki, Gita Sukthankar	既存の2部グラフのネットワークを、意味単位に分類するタスクの提案。実例としてGitHubのデータを使用している。ユーザー=>リポジトリの関連(2部グラフ)から、ユーザー=>ユーザーの関連を推定する。手法としては代表的なクラスタリングの手法をいくつか使って試している。
1106	Superposition of many models into one	https://arxiv.org/abs/1902.05522	Brian Cheung, Alex Terekhov, Yubei Chen, Pulkit Agrawal, Bruno Olshausen	ニューラルネットの重みを、複数の重みの重ね合わせと考える手法の提案。背景として枝刈りの手法から大半の重みが暇をしていることがわかってきたことがある。重みの空間を効率的に使用するために、入力を分解するコンテキストをかけた上で重みを適用する手法を提案している(フーリエ変換に近い手法)。
1107	M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network	https://arxiv.org/abs/1811.04533	Qijie Zhao, Tao Sheng, Yongtao Wang, Zhi Tang, Ying Chen, Ling Cai, Haibin Ling	物体検知の手法で使用される、Feature Pyramidを改善した研究。既存のピラミッドはクラス分類特化のモデルの、単一レイヤからしか作られていない点を指摘。複数レイヤの特徴を混合し、そこからU型のConv/Deconvを積み上げ物体検出に特化したピラミッドを構築している。
1108	Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent	https://arxiv.org/abs/1902.06720	Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, Jeffrey Pennington	幅が無限とした場合のDNNを最急降下法で学習させる処理は、線形変換と同等であるとした論文(=学習結果は一次のテイラー展開で置き換えられる)。損失関数が2次の場合は出力がGaussianで維持されるため学習の過程はGaussian Processと見做すことができ、この点がBayesianNetと異なるとしている。
1109	Transfusion: Understanding Transfer Learning with Applications to Medical Imaging	https://arxiv.org/abs/1902.07208	Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, Samy Bengio	画像の転移学習について、何が転移されているのかを調べた研究(画像は医療画像が対象)。転移の効果として、収束速度の向上、ゼロからの学習では得られない特徴検出(本研究ではGabor filter=斜めの検知)の転移があるという。ただ収束速度は単に重みの平均/分散で初期化するだけでも得られるという
1110	Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation	https://arxiv.org/abs/1805.11004	Han Guo, Ramakanth Pasunuru, Mohit Bansal	要約の生成において、QA/Entailmentのタスクを複合させた研究。要約は文書において顕著な情報を含む必要があるため、QAで「質問されるようなこと」、Entailmentで「他の文で言及されること」を学習することを意図している。各タスクでコンテキストとAttentionをシェアしている
1111	IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS	https://openreview.net/forum?id=Bygh9j09KX	Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, Wieland Brendel	従来の定説と異なり、CNNは形状ではなく画像表面の表面質感(Texture）をもとに判断していると提唱。様々な表面質感変換を加えたStylized-ImageNetデータセットを用いて訓練することで、表面質感依存を軽減した頑健性が高いモデルができるようになるとのこと。
1112	Knowledge Distillation with Adversarial Samples Supporting Decision Boundary	https://arxiv.org/abs/1805.05532	Byeongho Heo1 Minsik Lee2 Sangdoo Yun3 Jin Young Choi1	敵対的サンプルを利用して、モデルの蒸留を行うという研究。敵対的サンプルは決定境界に近い場所にあるだろうという仮説から、それらを用いてより精度よくTeacherの決定境界をStudentに学習させる。あるデータに敵対的摂動を加えて様々なクラスに誤分類させるサンプルを作り、その予測ラベルを真似させることによって、より精度良く決定境界を近似できる。
1114	Random Search and Reproducibility for Neural Architecture Search	https://arxiv.org/abs/1902.07638	Liam Li, Ameet Talwalkar	ネットワーク構造の自動探索で、再現可能なベースラインを作るという研究。構造探索はハイパーパラメーター探索の一種なので、random search/early stoppingベースのシンプルかつ強力なASHAでベースラインを構築。weight-sharingでの探索でSOTAに近い結果が得られたとのこと。そのため、評価値として「(同等の結果を得るのに)何本のrandom searchが必要か」という指標が適切ではとしている。
1115	A Closer Look at Few-shot Classification	https://openreview.net/forum?id=HkxLXnAcFQ	Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, Jia-Bin Huang	Few-shot learningの評価についての研究。特徴抽出を深いネットワークで、分類を線形分離でなく距離(コサイン類似度)ベースにするだけでSOTAと同等の結果が得られることを確認。またドメインを変えてのテストをすべきとし、mini-ImageNetからCUBで評価を行なっている。
1116	World Discovery Models	https://arxiv.org/abs/1902.07685	Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo Avila Pires, Jean-Bastian Grill, Florent Altch, Rmi Munos	部分観測な環境(POMDP)にて、内発的報酬を導入した研究。内発的報酬については、行動に対する環境変化の予測分布と、実際の環境変化の分布を比較することで差が大きい(=予測と大きく異なる)なら探索を行うようにしている。変化予測は単体の行動からだけでなく、過去の行動系列に対しても行なっている
1117	Learning to Generalize from Sparse and Underspecified Rewards	https://arxiv.org/abs/1902.07198	Rishabh Agarwal, Chen Liang, Dale Schuurmans, Mohammad Norouzi	強化学習において、行動の正当性を高めるための手法の提案。「偶発的な成功」は予想しない行動の獲得につながるとして、それを抑制するために学習/検証のデータセットを分け、学習側で獲得した戦略が検証側でも通用する場合追加報酬を与えるような手法を提案している。
1118	Latent Translation: Crossing Modalities by Bridging Generative Models	https://arxiv.org/abs/1902.08261	Yingtao Tian, Jesse Engel	ドメインの異なる潜在表現(音声と画像など)を転移させる研究。双方の潜在表現を共通の潜在表現から生成するためのブリッジ用VAEを学習するという形。このVAEは、各ドメインの潜在表現の復元に加え、それらの分類、また意味が同一の場合(「4」という画像と音声など)の距離最小化から学習する。
1119	Online Meta-Learning	https://arxiv.org/abs/1902.08438	Chelsea Finn, Aravind Rajeswaran, Sham Kakade, Sergey Levine	連続的なタスクに対し後悔を最小化するOnline Learning(具体的には各時刻のlossを合計したものを最小化する。各時刻の重み調整が適切なら総和が小さくなる)と、メタラーニングを組み合わせた手法の提案。タスク開始時に重みを適したものに変換し(f_t)、タスクに応じた学習をする(U_t)ようにしている。
1120	From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following	https://arxiv.org/abs/1902.07742	Justin Fu, Anoop Korattikara, Sergey Levine, Sergio Guadarrama	逆強化学習を行う際に、推定対象である報酬関数に言語による条件付けを導入した研究(端的には、指示通り動けたら報酬が与えられる)。言語による指示は様々なタスクで使えるため、報酬の転移性が高くなるとのこと。手法はMax Entropyベースで、言語指示はLSTMで処理し画像側ベクトルと内積を取っている
1121	Multi-Task Learning with Contextualized Word Representations for Extented Named Entity Recognition	https://arxiv.org/abs/1902.10118	Thai-Hoang Pham, Khai Mai, Nguyen Minh Trung, Nguyen Tuan Duc, Danushka Bolegala, Ryohei Sasano, Satoshi Sekine	認識対象のカテゴリが数百などに及ぶFine-Grained NERにおいて、ELMoなどの文脈潜在表現やマルチタスクを導入し、どんな表現・タスク・ネットワーク構成が有効化を検証した研究。ELMoを利用し、タスクを階層的に積んでタスクごとの言語モデルタスクを同時に解かせるモデルが有効だったという結果。
1122	An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models	https://arxiv.org/abs/1902.10547	Alexandra Chronopoulou, Christos Baziotis, Alexandros Potamianos	言語モデルを転移させる、シンプルな手法(SiATL)の提案。最初に言語モデルまず学習。その後目的のタスクを学習させる際は、上に分類機などを積むとともに言語モデルの学習内容を忘れないよう言語モデルの目的関数も組み合わせる。細かい学習率の調整などが不要で手軽に使える
1123	SC-FEGAN: Face Editing Generative Adversarial Network with User’s Sketch and Color	https://arxiv.org/abs/1902.06838	Youngjoo Jo Jongyoul Park	写真に絵を描いたら、その通りに写真を修正してくれるSC-FEGANを提案。
1124	End-to-End Dense Video Captioning with Masked Transformer	https://arxiv.org/abs/1804.00819	Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher, Caiming Xiong	Transformerベースのモデルで、End2Endのビデオキャプションを実現したという研究。Encoder側は動画中からキャプション対象のイベント(時間範囲)を抽出し、Decoder側はイベントにマスクをかけた上で文の生成を行なっていく。
1125	code2seq: Generating Sequences from Structured Representations of Code	https://openreview.net/forum?id=H1gKYo09tX	Uri Alon, Shaked Brody, Omer Levy, Eran Yahav	コードからメソッド名を推測、また要約を行う研究。ソースコードは文字列として処理するのでなく、ASTという構造木にしてEncodeを行なっている(サブツリーのEncode結果を平均して潜在表現を作成する)。なおASTの作成はパーサーを使っているよう(アノテーションした節がない)
1126	The State of Sparsity in Deep Neural Networks	https://arxiv.org/abs/1902.09574	Trevor Gale, Erich Elsen, Sara Hooker	DNNの重みを疎にする手法について、どの手法が有効かを大規模なモデル(Transformer/ResNet)で検証した研究(疎=重みが0に近いパラメーターが多いと計算を簡略化できる)。結果としては、単純に重みのMagnitudeで枝刈りする手法が良好であり、枝刈りで得られる構造は素の状態からは得難いとのこと。
1127	How do Mixture Density RNNs Predict the Future?	https://arxiv.org/abs/1901.07859	Kai Olav Ellefsen, Charles Patrick Martin, Jim Torresen	強化学習で環境の学習に使用されたりするMixture Density RNN(MD-RNNs)についての研究。MD-RNNsではRNNの潜在表現から混合分布のパラメーター(=複数の平均μ/分散σ、その他のパラメータπ)を推定する形式をとっているが、これにより異なるイベント/ルールに基づく法則を個別に学習できているとしている。
1128	Model-Based Reinforcement Learning for Atari	https://arxiv.org/abs/1903.00374	Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Ryan Sepassi, George Tucker, Henryk Michalewski	モデルベースの手法で、Atariのゲームを攻略してみたという研究。ベースとしているのはモデルベースとモデルフリーを併用するDyna。環境のモデル化にはCNNのEncoder/Decoderが使用されており、Decode時にActionによる画面遷移表現(離散)で条件付けて生成を行う。多くのゲームでPPO/Rainbowを上回る効率
1129	α-Rank: Multi-Agent Evaluation by Evolution	https://arxiv.org/abs/1903.01373	Shayegan Omidshafiei, Christos Papadimitriou, Georgios Piliouras, Karl Tuyls, Mark Rowland, Jean-Baptiste Lespiau, Wojciech M. Czarnecki, Marc Lanctot, Julien Perolat, Remi Munos	マルチエージェントの戦略評価についての研究。評価はナッシュ均衡がセオリーだが、計算が難しく動的な環境になじまない。そこで、まず連続した環境の動きをConley’s Theorem(連続と接続点)で定義、連続箇所をマルコフ連鎖で近似(MCC)、均衡を複数エージェントの行動から近似(Micro-model)という流れ
1130	VideoFlow: A Flow-Based Generative Model for Video	https://arxiv.org/abs/1903.01434	Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent Dinh, Durk Kingma	厳密な潜在変数の推定が可能なGlow (
1131	Learning Latent Plans from Play	https://arxiv.org/abs/1903.01973	Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet	強化学習においてデモの中から汎用的な動作を認識し、使いまわせるようにする研究。デモの中から任意の長さのパートを切り出し、それがどんな(汎用)動作に該当するかを予測する。パートのどの位置でも動作の一部のため、位置により予測が変化しないよう潜在空間上の距離を小さくする形で学習する。
1132	Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions	https://arxiv.org/abs/1903.03088	Matthew MacKay, Paul Vicol, Jon Lorraine, David Duvenaud, Roger Grosse	ハイパーパラメーターを自動チューニングするという研究。実現にはもちろん学習セットに適合済みのネットワークが必要だが(その上でvalidationを使いチューニングするため)、その出力を近似する。手法としては、通常の重み/biasに加えハイパーパラメーターでスケールされる重み/biasを加え学習を行う。
1133	Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment	https://arxiv.org/abs/1903.01689	Yifan Wu, Ezra Winston, Divyansh Kaushik, Zachary Lipton	ドメイン適応では、潜在空間を通じた転移がよく用いられる。しかし潜在表現を完全に転写が可能な空間としてしまうと転移できる範囲に限界がある(ソースにはあるけどターゲットにはない、という状況が許容されずソースにあるものはターゲットに必ずなくてはならない)。そのため、この条件を緩和した研究
1134	A Mean Field Theory of Batch Normalization	https://openreview.net/forum?id=SyMDXnCcF7	Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, Samuel S. Schoenholz	バッチ正規化(BN)の効果について理論と実験双方から検証した研究。層を深くすることにより発生する勾配爆発は、BN単体では防げないことを示唆(Skip- connectionの併用が必要)。ただ、ネットワークの挙動を線形に近づけることで学習しやすくするなどのメリットがあるとのこと。
1135	Generative Graph Convolutional Network for Growing Graphs	https://arxiv.org/abs/1903.02640	Da Xu, Chuanwei Ruan, Kamiya Motwani, Evren Korpeoglu, Sushant Kumar, Kannan Achan	グラフにおいて「新しいノードをどう扱うか」という問題にチャレンジした研究。SNSやアイテム推薦における新規アイテム問題ではこの点が顕著だが、既存の研究では固定的なグラフを扱うのが大半だった。VAEをベースとして、順次ノードが追加されていく時系列の生成モデルという形でアプローチをしている
1136	Neural Language Modeling with Visual Features	https://arxiv.org/abs/1903.02930	Antonios Anastasopoulos, Shankar Kumar, Hank Liao	画像の情報もミックスして言語モデルを構築する研究。モデル自体はシンプルなRNNで、画像情報を入れる位置(複数層のLSTMの前/中/後)、また自然言語/画像ベクトルのマージ方法(線形結合/言語との相関に応じ重みをつけ画像ベクトルを結合)の組み合わせを検証している。Middle(中)が一番良好な精度。
1137	Online Model Distillation for Efficient Video Inference	https://arxiv.org/abs/1812.02699	Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, Kayvon Fatahalian	セグメンテーションの演算効率向上を狙った研究。
1138	Manipulation by Feel: Touch-Based Control with Deep Predictive Models	https://arxiv.org/abs/1903.04128	Stephen Tian, Frederik Ebert, Dinesh Jayaraman, Mayur Mudigonda, Chelsea Finn, Roberto Calandra, Sergey Levine	DNNを用いて、適切な触覚に基づく行動を行えるようにする研究(卵をつかむのに握りつぶしちゃった！ということにならないようにするなど)。モデルベースの手法を使用しており、現在の観測/モデルの予測をベースに、候補となる行動系列から最も目的の触覚に近づくと見込まれる系列の最初の行動を取る。
1139	Attention is not Explanation	https://arxiv.org/abs/1902.10186	Sarthak Jain, Byron C. Wallace	自然言語処理においてAttentionはよくモデルの判断根拠として用いられるが、本当に説明になっているのかを検証した研究。結果として、GradientベースのスコアとAttentionは乖離があり、またAttentionの分布が異なるよう変更しても予測結果を維持することができることを確認
1140	Skew-Fit: State-Covering Self-Supervised Reinforcement Learning	https://arxiv.org/abs/1903.03698	Vitchyr H. Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, Sergey Levine	報酬関数を自ら推定できるようにする研究。端的には目的達成に重要な状態に報酬を振りたいが、そのためには全状態を検証する必要がありこれは困難。そこで、Replay Bufferのうち到達確率が低いものを重点的に見るようにすることで効率を上げている。
1141	To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks	https://arxiv.org/abs/1903.05987	Matthew Peters, Sebastian Ruder, Noah A. Smith	ELMo/BERTといった事前学習済みモデルを有効に使うための方法を、様々な実験から検証した資料。重みを固定し特徴抽出機として使う/Fine Tuningする、いずれもほぼ同様の結果だが文関係の推論においてはELMoは固定、BERTはFine Tuneが良いとの結果。これはLSTMの特性(関係認識に弱い)に由来する可能性有
1143	Stiffness: A New Perspective on Generalization in Neural Networks	https://arxiv.org/abs/1901.09491	Stanislav Fort, Pawe Krzysztof Nowak, Srini Narayanan	あるデータのlossによる更新が、他のデータのlossにどのような影響を与えるのかを調べることで汎化性能が調査できるとした研究。同クラス内のデータであればクラス内の、異なるクラス間のデータであればクラス間の相関を知ることができる。学習率が高い方が離れたデータ間の相関を誘導できるとのこと
1144	Algorithms for Verifying Deep Neural Networks	https://arxiv.org/abs/1903.06758	Changliu Liu, Tomer Arnon, Christopher Lazarus, Clark Barrett, Mykel J. Kochenderfer	DNNが意図した振る舞いをするかどうか検証するための手法のまとめ。入力=>出力への到達過程の検証(Reachability)、最適化において(線形の)制約をかける(Optimization)、意図しない出力を生む入力の探索(Search)、という3つの種別に分けて解説されている。具体的な疑似コードもあり実装も公開されている
1145	Semantic Image Synthesis with Spatially-Adaptive Normalization	https://arxiv.org/abs/1903.07291	Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu	セグメンテーションによる条件付けをした上での画像生成を行う研究。既存のネットワークでは正規化のレイヤを通すと条件付けの情報を(正規化により)消してしまう傾向があった。このため、事前にマスク単位で係数/バイアス(=特徴)を計算しておき、正規化後に乗せるという形をとっている。
1146	Exact Gaussian Processes on a Million Data Points	https://arxiv.org/abs/1903.08114	Ke Alexander Wang, Geoff Pleiss, Jacob R. Gardner, Stephen Tyree, Kilian Q. Weinberger, Andrew Gordon Wilson	Gaussian Processesについて、演算処理をマルチGPUに最適化(=並列+行列演算)した研究。これにより、通常数千点へのフィッティングが限界のところ百万まで拡大できたという。GPyTorchという形で実装も公開されている。
1147	Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables	https://arxiv.org/abs/1903.08114	Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, Sergey Levine	強化学習におけるメタラーニングにおいて、戦略ではなく環境にフォーカスを当てた研究。既存の研究ではメタな戦略(新しい環境にすぐ適合する連略)の作成を試みることが多いが、本研究ではメタな環境認識(具体的には潜在表現)を学習させ、それを基に行動をとるというOff-Policyなアプローチをとっている
1148	Implicit Generation and Generalization Methods for Energy-Based Models	https://openai.com/blog/energy-based-models/	Yilun Du, Igor Mordatch	生成モデルの学習は潜在表現を得るのに有効だが、学習時文字通り生成モデル(画像の生成など)が必要で計算コストが高い。そこで潜在表現に勾配を適用していくことでサンプリングを行うLangevin dynamicsを用いこれを解消している(勾配適用により最終的には実データ分布からのサンプリングと等価になる)
1149	MONet: Unsupervised Scene Decomposition and Representation	https://arxiv.org/abs/1901.11390	Christopher P. Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, Alexander Lerchner	複数の物体が混在する画像(入力)について、個別に認識して表現を獲得する手法の提案(教師なしでのセグメンテーション/セグメント内表現学習といった印象)。対象領域を選択(マスクを生成)し、対象箇所についてVAEで表現学習、という処理を再帰的に繰り返す構成となっている。
1150	Multi-Object Representation Learning with Iterative Variational Inference	https://arxiv.org/abs/1903.00450	Klaus Greff, Raphal Lopez Kaufmann, Rishab Kabra, Nick Watters, Chris Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, Alexander Lerchner	複数物体が含まれる画像(入力)について、物体ごと段階的に生成(改善)を行うVAEの提案。物体ごとに潜在表現を推定し一気に生成、ではなく物体ごとパラメーターとマスクを生成=>未生成の部分を次の周回に渡す、という形で段階的に生成する(物体数(slot数)は事前に決める)。
1151	Cloze-driven Pretraining of Self-attention Networks	https://arxiv.org/abs/1903.07785	Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli	Bi-directionalとTransformerを組み合わせた、事前学習モデルの提案。順方向と逆方向、2方向のTransformerを持ち、最終的には順/逆の情報をSelf-Attention+全結合で処理して予測を行う。固有表現認識/構文解析にて、BERTより精度が微増。
1152	Multi-Task Deep Neural Networks for Natural Language Understanding	https://arxiv.org/abs/1901.11504	Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao	BERTをマルチタスクでチューニングした形の研究。タスクは、文分類・文間類似度・文関係分類・QAの4種類。SNLI/SciTailといった、文関係の推論タスクの精度が素のBERTに比べて大幅に更新。
1153	Adaptive Gradient Methods with Dynamic Bound of Learning Rate	https://arxiv.org/abs/1902.09843	Liangchen Luo, Yuanhao Xiong, Yan Liu, Xu Sun	Adamから徐々にSGDへと移行していく最適化手法の提案。既存の研究では大きすぎる学習率の抑制に焦点を置いたものが多かったが、小さすぎる学習率も影響がある点を指摘し、上限/下限を設定することでこれをクリップし所定範囲内で徐々にSGDへ移行するようにしている。
1154	Low Resource Text Classification with ULMFit and Backtranslation	https://arxiv.org/abs/1903.09244	Sam Shleifer	Sentimentの分類(IMDB)で、事前学習による転移(ULMFit)とData Augmentationの効果を検証した研究。転移が有効なのはもちろんとして、AugmentationではBacktranslationがかなり有効との結果(ランダムなノイズ挿入は逆効果のよう)。転移＋実データ数50(+Augmentation 500)で80%超の精度を達成。
1155	Left-to-Right Dependency Parsing with Pointer Networks	https://arxiv.org/abs/1903.08445	Daniel Fernndez-Gonzlez, Carlos Gmez-Rodrguez	係り受け解析で、左から右に一発読むだけでツリーを作成するという手法の提案。Pointer Networkを利用し、位置iの単語の親はどれか?を直接推定していく(0の場合ROOTとする)。English PTB datasetで既存手法より精度が高いばかりでなく、2倍ほど速度が向上。
1156	Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition	https://arxiv.org/abs/1903.10346	Yao Qin, Nicholas Carlini, Ian Goodfellow, Garrison Cottrell, Colin Raffel	音声におけるAdversarial Exampleの研究。音声の場合画像に比べてノイズが顕著になるため、聞いてもわからない＋空間に伝播しても誤認識させられることを目指している。このために、空間反響のシミュレーターを経由した(Adversarial入り)音声が、実音声の周波数マスク帯以内に入るよう学習している。
1157	Improving Relation Extraction by Pre-trained Language Representations	https://openreview.net/forum?id=BJgrxbqp67	Christoph Alt, Marc Hbner, Leonhard Hennig	Transformer (
1158	Weight Standardization	https://arxiv.org/abs/1903.10520	Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille	出力チャンネルごとに重みを正規化するWeight Standardizationの提案。Batch Normalizationの効果は当初提唱されていた共変量シフトの解消ではなく、勾配をなだらかにする方にあることが最近示されているが、そうならば後者をより強める手法を、ということで発明された。小バッチサイズでも有効な手法
1159	StrokeNet: A Neural Painting Environment	https://openreview.net/forum?id=HJxwDiActX	Ningyuan Zheng, Yifan Jiang, Dingjiang Huang	ブラシストロークによる文字の生成を学習する手法の研究。入力画像を一旦ブラシの種別/ストローク(位置の連続)の表現にし生成、という処理を再帰的に繰り返すことで元の画像をストロークにより再現することを学習する。
1160	On Measuring Social Biases in Sentence Encoders	https://arxiv.org/abs/1903.10561	Chandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, Rachel Rudinger	文表現における偏見を検証する手法の提案(例えば「xxは賢い」という文について、xxに人種を入れた場合差が出るかなど)。既存の手法は単語ベースであったが(Word Embedding Association Test=WEAT)、これを文(Sentence=SEAT)に拡張している。文は、xx is yyのような単純なテンプレートを用いている。
1161	Generalized Off-Policy Actor-Critic	https://arxiv.org/abs/1903.11329	Shangtong Zhang, Wendelin Boehmer, Shimon Whiteson	Off-Policy Policy Gradientでは実際の戦略(target policy)とは別に探索用のbehavior policyを用いて更新を行うが、実際の戦略が目指したいところ(期待値の最大化)と探索が目指したいところ(価値の最大化)はバッティングする可能性がある。そこで、両者のバランスを調整するパラメーターの導入を提案
1162	ELDEN: Improved Entity Linking Using Densified Knowledge Graphs	https://aclweb.org/anthology/papers/N/N18/N18-1167/	Priya Radhakrishnan, Partha Talukdar, Vasudeva Varma	Entity Linkingのタスクでは、当然Entity間のRelation(接続)が多いEntityほど高精度な推定が可能になる。逆に言えば接続が少ないEntityについては推定精度が下がることになるが、その問題を擬似的なEntityを経由した接続を行うことで解消したという研究。擬似的なEntityは、unigram/bigramの頻度を元にテキスト中から選出される。
1163	Context-Aware Learning for Neural Machine Translation	https://arxiv.org/abs/1903.04715	Sbastien Jean, Kyunghyun Cho	機械翻訳において、コンテキストはあくまで単語というトークンの予測に使用されるが、これを文/データの予測にも有用なら採用するように矯正する手法の提案。単語の予測だと短期のコンテキストが採用されがちだが、文・データ全体に対する再現性を加味することで長期を意識させるようにする
1164	Recent Advances in Autoencoder-Based Representation Learning	https://arxiv.org/abs/1812.05069	Michael Tschannen, Olivier Bachem, Mario Lucic	VAEを中心とした、表現学習の手法のまとめ。表現学習の目的はデータ間に存在するメタ的な構造(meta-prior)を抽出することがあるとし、これをDisentanglement(独立した因子に分解)、階層(葉=>枝=木のような属性構造)、Semi-Supervised、Clusteringの4つに分けている。
1165	Guided Meta-Policy Search	https://arxiv.org/abs/1904.00956	Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, Chelsea Finn	強化学習におけるメタラーニングについて、「メタ」側の学習時間を模倣学習で短縮する研究。様々なタスクに適合可能な初期値を得るには、複数タスクによる十分なメタ学習が必要であり、これはOn-policyの場合特に時間がかかる。そこで模倣学習+Off-policyにより高速な学習を行う手法を提案
1166	HoloGAN: Unsupervised learning of 3D representations from natural images	https://arxiv.org/abs/1904.01326	Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, Yong-Liang Yang	画像生成を行うGANで、3D構造を意識した生成を行わせる研究。これにより学習データにない角度での生成なども可能になる。元の画像から3D Convolutionにより立体表現を得て、それに回転などの変更を加えた上で2Dへのレンダリングを行う。どの角度から見ても同じ物体とされるよう、lossを設定している。
1167	Scalable Muscle-actuated Human Simulation and Control	http://mrl.snu.ac.kr/research/ProjectScalable/Page.htm	Seunghwan Lee, Kyoungmin Lee, Moonseok Park, Jehee Lee	筋骨格モデルに基づく運動を、強化学習で学ぶという研究。骨格を動かすモデルと筋肉を活性させるモデルの2つに分かれており、骨格=>筋肉と階層型の構成にしてコントロールを行う。学習には模倣学習を使用しており、姿勢(間接角度)と末端位置の2つを報酬としている。
1168	A Structural Probe for Finding Syntax in Word Representations	https://nlp.stanford.edu/pubs/hewitt2019structural.pdf	John Hewitt, Christopher D. Manning	BERTやELMoといった文脈埋め込みベクトルについて、潜在表現を線形変換することで係り受け解析の距離が得られる空間に転写できるとした研究。端的には、BERTやELMoの表現内にパースツリーの情報が含まれていることを示した。距離については、ツリー内の接続を辿った距離を使用している。
1169	HYPE: Human eYe Perceptual Evaluation of Generative Models	https://arxiv.org/abs/1904.01121	Sharon Zhou, Mitchell Gordon, Ranjay Krishna, Austin Narcomey, Durim Morina, Michael S. Bernstein	GANなどの生成モデルに対する人手評価について、評価方法を統一しようという研究。画像を見る時間に制限を設けたHYPE-Timeと設けないHYPE-Infinityを提案している。Timeは心理学における適応的階段法をベースとした厳密な評価方法であり、Infinityはその簡易版としている。
1170	Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation	https://arxiv.org/abs/1904.02331	Jiawei Wu, Xin Wang, William Yang Wang	教師なし翻訳における逆翻訳の手法を改善した研究。逆翻訳(Back-translation)は逆文の品質があまり良くないため、候補をいくつかとる(Extract)、かつ元文の意味に近づける(Edit)操作を行ない選択を行う。前者は意味(潜在空間上)の距離比較で、後者は元/候補の間でMaxpool(=意味強調)をとることで行う
1171	Resource Efficient 3D Convolutional Neural Networks	https://arxiv.org/abs/1904.02422	Okan Kpkl, Neslihan Kose, Ahmet Gunduz, Gerhard Rigoll	2Dにおける省サイズモデル(SqueezeNetやMobileNetV1/V2)を、3Dのタスクに適用してみたという研究。具体的には、モーション認識に適用している。基本的に性能は引き継がれるという結果で、depthwiseが有効で3Dの場合深いほうが良いとのこと。
1172	Unifying Human and Statistical Evaluation for Natural Language Generation	https://arxiv.org/abs/1904.02792	Tatsunori B. Hashimoto, Hugh Zhang, Percy Liang	自然言語における文生成で、人の評価では品質が測れるが多様性が、機械的な指標では多様性は測れるが品質を測ることが難しい問題を解決する試み。人の評価(生成されたものか人間が作ったものかをクラウドソーシングで判定してもらい作成)と機械的な識別結果を結合した特徴でスコアを算出している。
1173	Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning	https://arxiv.org/abs/1812.01628	Prithviraj Ammanabrolu, Mark O. Riedl	テキストアドベンチャーゲームを、強化学習＋知識グラフで攻略したという研究。ゲームは選択肢で分岐して進むため、得られた状態で内部のグラフを更新していく。グラフ表現(Graph Convolution + Attention)+テキスト表現(一定範囲のBi-LSTM)で行動価値を出力する(行動数はグラフで絞り込む)。
1174	Exploring Randomly Wired Neural Networks for Image Recognition	https://arxiv.org/abs/1904.01569	Saining Xie, Alexander Kirillov, Ross Girshick, Kaiming He	ネットワーク構造の自動構築において、より自由な構造を探索するためランダムにグラフを作成してそれを実行可能なNNに変換する方式を提案(通常は探索空間を人が決めて絞る)。各アルゴリズムについて5回実行の平均/分散をとっており、ImageNetベンチマークと同等の結果。
1175	Learning to Describe Phrases with Local and Global Contexts	https://arxiv.org/abs/1811.00266	Shonosuke Ishiwatari, Hiroaki Hayashi, Naoki Yoshinaga, Graham Neubig, Shoetsu Sato, Masashi Toyoda, Masaru Kitsuregawa	知らない単語について、定義を書き出せるようにする研究。人間が知らない単語について、周辺の文脈から意味を察するようにLocal+Global+Characterの情報を使い定義を生成する。「知らない単語の定義」を学習するために、(辞書ではなく)Wikipedia/Wikidataから学習データを作成している。
1176	Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras	https://arxiv.org/abs/1904.04998	Ariel Gordon, Hanhan Li, Rico Jonschkowski, Anelia Angelova	単眼の画像フレームから深度/自己・物体動作だけでなく、カメラの内部パラメーターまで推定する手法。平行移動の場合カメラ外部で調整できてしまうため学習ができないが、回転についてはそうはいかないため(外部/内部双方が正しくないと復元できない)、回転を用い学習している
1177	NOVEL2GRAPH: Visual Summaries of Narrative Text Enhanced by Machine Learning	http://ceur-ws.org/Vol-2342/paper4.pdf	Vani K, Alessandro Antonucci	自然言語処理を用いてハリーポッターの人物相関図を作成したという研究。人名/別名(あだ名など)を固有表現認識で抽出し、関係に関するフレーズを共起から取得。そのフレーズから関係分類を行うことで人物間のエッジを作成している。
1178	Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension	https://arxiv.org/abs/1809.06963	Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, Jianfeng Gao	QAなどの読解タスクで、複数タスクを並列で学習させる手法の提案。どのタスクをどれだけ学習するかの配分率を決めるのに、言語モデルを使用している。質問の言語モデル、回答は短すぎるので長さの分布を作り、ターゲットタスクとは傾向が異なるものに重みを振るようにしている
1179	Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution	https://export.arxiv.org/abs/1904.05049	Yunpeng Chen, Haoqi Fang, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach, Shuicheng Yan, Jiashi Feng	CNNで出力される特徴マップを、低周波(解像度が荒い)特徴と高周波(解像度が高い)特徴にわける手法の提案。これにより特徴マップの冗長性を無くし、精度面＋演算速度面でメリットが出るとしている。CNNのフィルタを同周波(高=>高・低=>低)・異周波交換(高=>低・低=>高)の4つに分割し実装している
1180	Bridging Theory and Algorithm for Domain Adaptation	https://arxiv.org/abs/1904.05801	Yuchen Zhang, Tianle Liu, Mingsheng Long, Michael I. Jordan	教師なしのドメイン適用ではソース側で学習した後、ラベルなしでターゲット側への転移を行う。このときソース/ターゲット側で共通する判断空間が転移可能な上限となるが、空間が広すぎるため上界設定が難しかった。そこで、制約を加えることで学習/上界到達を行いやすくした
1181	End-to-End Robotic Reinforcement Learning without Reward Engineering	https://arxiv.org/abs/1904.07854	Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, Sergey Levine	一般的な模倣学習ではエキスパートのデモが必要になるが、デモでなく成功時の画像だけで行動を学習する手法の提案。成功画像から成否判定を行う分類機をCNNで作成し、それを報酬関数として学習させる。
1182	NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection	https://arxiv.org/abs/1904.07392	Golnaz Ghiasi, Tsung-Yi Lin, Ruoming Pang, Quoc V. Le	物体検出を行うネットワークの特徴ピラミッドを、自動探索で作成したという研究。異なるスケールの特徴マップを混合させるのが有効という知見から、候補から2つ選択=>出力サイズ決定=>混合(sum/global poolingでchannelの重みを計算し足し合わせる)=>候補に加える、を再帰的に繰り返し構造を作る。
1183	Document Expansion by Query Prediction	https://arxiv.org/abs/1904.08375	Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho	文書から文書を検索する際に使われそうなクエリを予測して、文書＋予測クエリでインデックスを作成しておくことで検索性能を上げるという研究。非常にシンプルな手法ながら、MS MARCOの関連箇所検索(Passage Retrieval)でSOTAの性能。
1184	Temporal Cycle-Consistency Learning	https://arxiv.org/abs/1904.07846	Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman	時間変化の表現を、教師なしで学習する試み。同じ動作を行っている2つの動画を用意し、画像フレームをEncodeして表現を作成=>相手先の画像フレームから表現が最も近いものを選択=>相手先からも選んだ場合一致するか、で学習する(Cycle Consistency)。
1185	Neural Painters: A learned differentiable constraint for generating brushstroke paintings	https://arxiv.org/abs/1904.08410	Reiichiro Nakano	ブラシストロークを学習させるネットワークの提案。VAEで画像の表現を学習、全結合でブラシアクションをVAE表現との対応を学習、GANでブラシアクションによる画像生成を学習、と3段階で学習を行い学習効率をあげている。生成画像のクラス分類タスクを入れることで、任意クラスの画像生成も行なっている
1186	SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems	https://w4ngatang.github.io/static/papers/superglue.pdf	Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman	言語理解タスクのベンチマークとして使用されているGLUEを、新しいタスクで作り直したSuperGLUEが公開。分類/文関係(NLI)のタスクが減り、文書理解系のタスク(共参照/意味識別)が増えている。
1187	SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition	https://arxiv.org/abs/1904.08779	Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le	音声認識におけるData Augmentationの研究。spectrogramを入力とし、時間軸方向へのwarping、周波数軸/時間軸のrandom dropという3つの手法を検証している。いずれも効果があるが、warpingは大変な割に効果が微小なため、計算量の制約があるなら最初に落とす手法としている。
1188	Fooling automated surveillance cameras: adversarial patches to attack person detection	https://arxiv.org/abs/1904.08653	Simen Thys, Wiebe Van Ranst, Toon Goedem	物体検出に対するAdversarial Exampleで、人検出の回避に特化した研究。イメージ的には、パッチの印刷されたボードを持つだけで人検出システムをかいくぐって侵入、が可能になる。YOLOv2をベースに、物体/クラスの判定スコアを下げると共に印刷可能/自然(滑らか)なパッチになるようlossを設計している
1189	Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting	https://arxiv.org/abs/1904.00310	Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, Caiming Xiong	転移学習を行う際に起こる破壊的忘却(学習済みタスクのことを忘れてしまう現象)を防ぐためには、タスク固有のレイヤをどこにどれだけ設けるかなどを設計する必要がある。この設計をネットワークの自動探索を使って行う試み。再利用・転移用重みの追加・固有レイヤの追加の3行動を基本とし最適化を行う
1190	Language Models with Transformers	https://arxiv.org/abs/1904.00310	Chenguang Wang, Mu Li, Alexander J. Smola	BERTやGPTといったTransformer (
1191	Fine-tune BERT for Extractive Summarization	https://arxiv.org/abs/1903.10318	Yang Liu	BERTを使った抽出型(Extractive)要約の研究。BERTはトークン単位で特徴を出力するが、抽出型では文単位の予測となる。このため文境界を識別するためのSegment Embeddingを入力し、境界開始のトークン表現を文表現として使用＋文同士の関係を認識する層を追加し予測している
1192	Pretraining-Based Natural Language Generation for Text Summarization	https://arxiv.org/abs/1902.09243	Haoyu Zhang, Jianjun Xu, Ji Wang	BERTを使った抽象型(Abstractive)要約の研究。BERTはマスクされた箇所を予測する形で学習するため、そのままでは文生成に使えない。このため、一旦Transformerベースのモデルで文(ドラフト)を作成=>MaskをかけてBERTで補完=>補完文と入力を元に再作成、と2段階で生成している
1193	Exploring the Limitations of Behavior Cloning for Autonomous Driving	https://arxiv.org/abs/1904.08980	Felipe Codevilla, Eder Santana, Antonio M. Lpez, Adrien Gaidon	模倣学習で自動運転を行う際の、性能と限界についての研究。ベンチーマークとして行動意図(直進/右に行くなど)で条件付けし学習する CILを使用し十分な精度を確認。一方、学習データを集めるほどレアイベント(事故など)の対応が難しくなる、明確な因果モデルの欠如、学習分散の大きさなどをあげている
1194	Generating Long Sequences with Sparse Transformers	https://arxiv.org/abs/1904.08980	Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever	TransformerのAttention計算を効率化し、画像や音声といった入力数が多いデータにも適応可能にした研究。Self-Attentionは入力数Nに対しNxN行列の計算が必要で、Nが多いデータでは計算困難だった。そこで、実際Attentionがはられる箇所を元に行方向/列方向の計算箇所を限定し、計算を効率化している。
1196	Image Synthesis with a Convolutional Capsule Generative Adversarial Network	https://openreview.net/forum?id=rJen0zC1lE	Cher Bass, Tianhong Dai, Benjamin Billot, Kai Arulkumaran, Antonia Creswell, Claudia Clopath, Vincenzo De Paola, Anil Anthony Bharath	Capsule機構を使用した画像生成についての研究。pix2pixをベースにしており、U-Netライクなアーキテクチャーの畳み込み部分にCapsuleを使って生成を行っている。生成画像を使うことでセグメンテーションタスクの精度が向上＋Capsuleが意味あるグループの特徴をとらえていることを確認。
1197	Learning to Paint with Model-based Deep Reinforcement Learning	https://arxiv.org/abs/1903.04411	Zhewei Huang, Wen Heng, Shuchang Zhou	モデルベースでブラシストロークを学習する研究。通常は行動をペインティングソフト(MyPaintなど)に入れることが多いが、描画環境(=モデル)をNeural Netで構築している。遷移は描画プログラムを教師として学習、報酬は描画前後の画像に対するWGANのスコア(Discrimator判定)差を使用している。
1198	Local Relation Networks for Image Recognition	https://arxiv.org/abs/1904.11491	Han Hu, Zheng Zhang, Zhenda Xie, Stephen Lin	CapsuleNetのようなボトムアップ型の特徴集約を、CNNのように局所領域単位で行うことで、双方の良いところどり(特徴認識/計算効率)を目指した研究。SOTAの精度には及ばないが、フィルタによる固定的な集約でなく、入力画素間の相関に応じた可変的な集約演算をImageNetのサイズで行うことに成功。
1199	Meta-Sim: Learning to Generate Synthetic Datasets	https://arxiv.org/abs/1904.11621	Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba, Sanja Fidler	シーン認識などのタスクで、学習データを水増しする試み。画像の合成ではGANなどが使われることが多いが、本研究では普通にグラフィックエンジンを使用している。シーンをグラフで定義、グラフの属性を変更、グラフからレンダリング、というプロセスを取る。
1200	High-Fidelity Image Generation With Fewer Labels	https://arxiv.org/abs/1903.02271	Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, Sylvain Gelly	条件(クラスラベル)を指定した画像生成では、もちろんクラスラベルのついたデータセットが必要だった。これを教師なし/半教師ありの手法で削減したという研究。画像の回転予測と少量ラベルからの予測2つのタスクについて、事前学習する場合と補助タスクとして導入する場合を検証している。
1201	Understanding Dataset Design Choices for Multi-hop Reasoning	https://arxiv.org/abs/1904.12106	Jifan Chen, Greg Durrett	回答を行うために複数文にまたがる推論が必要なタスク(トムは東京にいる、東京は関東=>トムは関東にいるか？等)について、単純に単文だけ、選択式の場合単文すらなくても回答できるケースがあるという調査結果。WikiHop/HotpotQAで検証しており、既存モデルが複数文推論を学習していない可能性を示唆。
1202	Low-Memory Neural Network Training: A Technical Report	https://arxiv.org/abs/1904.10631	Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczynski, Jian Zhang, Christopher R	DNNを省メモリで学習する手法と、その効果をまとめた研究。Sparsity(DNNは実際貢献する重みが少ないため、重みをもつものをまとめ効率化)、Low precision(低精度演算)、Microbatching(ミニバッチを分割し勾配計算)、Gradient checkpointing(forward時特定箇所の勾配だけキープ)の4つを検証している
1203	Deep Learning for Audio Signal Processing	https://arxiv.org/abs/1905.00078	Hendrik Purwins, Bo Li, Tuomas Virtanen, Jan Schlter, Shuo-yiin Chang, Tara Sainath	音声に対するDNNの適用についてまとめられた資料。音声と画像の性質的な違い(時系列/周波数という相関のない2軸で表現される点、時系列のため順次処理が必要など)を示しその違いを各手法がどう扱っているのかという観点からまとめられている。概要的な資料だが問題設定と手法が上手くまとめられている
1204	Using Embeddings to Correct for Unobserved Confounding	https://arxiv.org/abs/1902.04114	Victor Veitch, Yixin Wang, David M. Blei	DNNの表現学習能力を、因果推論に活用する仕組みの提案。因果推論では交絡因子(証明したい関係xとy双方に影響のある因子z)の影響をどう除外するかが大きな課題だが、因果推論の問題を予測問題に変換することで交絡因子の影響推定(予測)に表現学習モデル(論文中ではGCN/BERT)を使えるようにしている。
1205	Unsupervised Data Augmentation	https://arxiv.org/abs/1904.12848	Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le	Data Augmentationはラベル付きデータをかさ増しするために使われるのが通例だが、ラベルなしのデータに対し適用する手法の提案。重みを固定したモデルを用い、元データに対する予測分布と、Augmentされたデータに対する予測分布とで差異が出ないように学習を行う。画像/テキスト共に効果を確認。
1206	Searching for MobileNetV3	https://arxiv.org/abs/1905.02244	Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam	モバイルデバイスに最適化したモデル(=精度と実行速度のバランスが取れたモデル)を、自動探索で構築する研究。ネットワークのブロックはMobileNetV2+SENet(channelのAttention的な手法
1207	Unsupervised Learning of Depth and Ego-Motion from Video	https://arxiv.org/abs/1704.07813	Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe	Structure from Motion の深層学習化。いわゆる SfM Learner 系のベースとなったモデル。動画から単眼深度推定を学習する。
1208	Unsupervised Learning of Geometry from Videos with Edge-aware Depth-Normal Consistency	https://arxiv.org/abs/1711.03665	Zhenheng Yang, Peng Wang, Wei Xu, Liang Zhao, Ramakant Nevatia	Depth と同時に物体表面の法線を推定する研究
1209	Learning Depth from Monocular Videos using Direct Methods	https://arxiv.org/abs/1712.00175	Chaoyang Wang, Jose Miguel Buenaposada, Rui Zhu, Simon Lucey	SfM Learner の PoseNet を DVO というアルゴリズムに置き換えたもの
1210	Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks	https://arxiv.org/abs/1810.09536	Yikang Shen, Shawn Tan, Alessandro Sordoni, Aaron Courville	文における単語間の関係(ツリー構造)を、LSTMの隠れ層で表現できるようにした研究。上位ノードが入れ替わる場合は上位配下のノード情報が全て消去される必要がある。これを表現するため、配下ノードが増える(=隠れ層の占有領域が増える)につれforgetを多くしinputを減らすmasterノードを導入している。
1211	Meta-learning of Sequential Strategies	https://arxiv.org/abs/1905.03030	Pedro A. Ortega, Jane X. Wang, Mark Rowland, Tim Genewein, Zeb Kurth-Nelson, Razvan Pascanu, Nicolas Heess, Joel Veness, Alex Pritzel, Pablo Sprechmann, Siddhant M. Jayakumar, Tom McGrath, Kevin Miller, Mohammad Azar, Ian Osband, Neil Rabinowitz, Andrs Gyrgy, Silvia Chiappa, Simon Osindero, Yee Whye Teh, Hado van Hasselt, Nando de Freitas, Matthew Botvinick, Shane Legg	強化学習のサンプル効率を向上させるため、環境の挙動に関する知識を戦略の推定に活かす手法の提案(論文では計算フレームワークの紹介のみ)。通常は戦略単体を学習、表現学習は戦略と別に環境表現を学習するが(並列)、本研究では環境推定から行動分布(戦略)推定と連続的な学習を行なっている(直列)
1212	Adversarial Examples Are Not Bugs, They Are Features	https://arxiv.org/abs/1905.02175	Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry	Adversarial Exampleへの耐性を上げるために、モデル側ではなくデータセット側を工夫するという論文。そもそもAdversarial Exampleは「汎用的ではない特徴」でモデルがこれに依存しているだけではとし、入力から「汎用的ではない特徴」を抜いたデータセットを作成しそれで学習という方法をとっている
1213	Learning Loss for Active Learning	https://arxiv.org/abs/1905.03677	Donggeun Yoo, In So Kweon	Lossを予測するサブタスクを追加することで、Unlabeledなデータに対しLossを予測し高い(=学習効果の高い)ものを優先してラベルづけする手法。画像分類/物体検知/姿勢推定といった様々なタスクで検証し効果を確認したが、タスクが複雑になる程Lossの予測が難しくなるという結果。
1214	Deep learning generalizes because the parameter-function map is biased towards simple functions	https://arxiv.org/abs/1805.08522	Guillermo Valle-Prez, Chico Q. Camargo, Ard A. Louis	DNNがなぜ汎化するのかについて、パラメーター空間の割にシンプルな関数になるようバイアスがかかっているから、とした研究。パラメーター空間の広がりに比べ関数のバリエーション変化が少ないことを、様々な初期化方法で確認。また、出現率の高い関数の複雑性(Lempel-Ziv)が低いことを確認。
1215	Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation	https://arxiv.org/abs/1804.03619	Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, Michael Rubinstein	音声分離タスクで、画像情報も用いる研究。画像は顔画像(顔検出は別途)をDilated Convolutionでdown/up sampling、音声はSTFTの画像を使用し、2つの特徴を結合してSpectrogramを出力する形を取っている。話者分離のために、実数/虚数双方を使用するマスクを使用している。
1216	HyperGAN: A Generative Model for Diverse, Performant Neural Networks	https://arxiv.org/abs/1901.11058	Neale Ratzlaff, Li Fuxin	ネットワークの重みを、学習ではなく生成で作成する研究。ノイズを全結合層(Mixer)に通して各層のノイズを作り、層ごとのGで生成を行う。生成した重みでlossを計測、最適化するようGを学習する。ただ生成する重みが固定的にならないようDで生成元ノイズがGaussianに近いかチェックする。
1217	DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback	https://arxiv.org/abs/1810.11748	Riku Arakawa, Sosuke Kobayashi, Yuya Unno, Yuta Tsuboi, Shin-ichi Maeda	代表的なモデルフリーの強化学習手法であるDQNと、人のフィードバックを学習に用いるTAMERを組み合わせた手法(DQN-TAMER)の提案。DQNは環境から推定した価値関数(Q)、TAMERは人のフィードバックを推定する関数(H)を学習に用いるが、双方を学習に用い学習が進むにつれHへの依存量をへらしていく。
1218	BERT Rediscovers the Classical NLP Pipeline	https://arxiv.org/abs/1905.05950	Ian Tenney, Dipanjan Das, Ellie Pavlick	BERTのどのレイヤがどのタスクで効いているのかを調査した研究。品詞推定(POS)や係り受け(Deps)などは低層で、役割推定(SPR)や関係推定(RE)は上層という割合直感と符合する結果となっている。ただNER/SPR/REは特定層が顕著に反応しているという感じではないよう。
1219	Objects as Points	https://arxiv.org/abs/1904.07850	Xingyi Zhou, Dequan Wang, Philipp Krhenbhl	物体検出で領域(矩形)を予測するのは無駄が多いため、中心点のみ予測を行い領域の大きさや角度などはその属性として推定しようという研究。入力画像をストライド幅でダウンサンプルし、ガウシアンカーネルを用いてヒートマップを作製。それを推定するという形をとっている。
1220	Conversion Prediction Using Multi-task Conditional Attention Networks to Support the Creation of Effective Ad Creative	https://arxiv.org/abs/1905.07289	Shunsuke Kitada, Hitoshi Iyatomi, Yoshifumi Seki	広告のコンバージョン性能(商品購入やダウンロードに繋がるか)を評価する手法の提案。広告のタイトル/テキストを個別にRNNでEncodeし、目的のCVRだけでなくCTRも予測させることで汎化性能を高めている。また広告の属性(ジャンル等)による重み付け(Attention)により予測精度を高めている。
1221	Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets	https://arxiv.org/abs/1904.02668	Nelson F. Liu, Roy Schwartz, Noah A. Smith	モデルを診断するために開発されたデータセットが、何を診断しているのかを調べた研究。診断データの少量を学習させ、診断データに対する精度が出る=学習データの弱点、出ない=モデルの弱点をついていることになる。診断を学習させることで精度が下がる場合、そもそもラベルの分散が違うケースと分類。
1222	Autonomous Penetration Testing using Reinforcement Learning	https://arxiv.org/abs/1905.05965	Jonathon Schwartz, Hanna Kurniawati	ペネトレーションテストは職人の経験と勘に頼ることが多いため、それを強化学習で自動化しようという試み。既存の研究はモデルベースが主流だが、診断環境の変遷は早いためモデルフリーの手法を使用している。環境の作成からQ学習の適用まで丁寧に書かれている(81pある)
1223	PaperRobot: Incremental Draft Generation of Scientific Ideas	https://arxiv.org/abs/1905.07870	Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan	論文の概要/結論/課題を自動生成するという夢の研究。既存の論文から知識グラフを作成し、Link PredictionによりEntity間の接続を増やす。生成を行う際は既存研究のタイトルを与え、そこから知識グラフを用い関連Entityを抽出。既存研究とEntityとの関連度に応じAttentionをかけながら生成を行う。
1224	HellaSwag: Can a Machine Really Finish Your Sentence?	https://arxiv.org/abs/1905.07830	Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi	BERTで攻略困難な多肢選択問題を作成したという研究(同様のデータセットとしてSWAGがあるが、SWAGはBERTに攻略されてしまったためその後継となる)。選択肢の文品質が重要そうとの検証結果から、より高精度な言語モデル(GPTなど)を使い回答と識別が困難な文生成を行なっていくことで作成している
1225	Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience	https://arxiv.org/abs/1810.05687	Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, Dieter Fox	強化学習で、戦略の学習と共にシミュレーターの挙動を徐々に現実の挙動に近づけていくという研究。シミュレーターで一旦戦略を学習したのち、同じ戦略に基づき現実/シミュレーター双方でプレイ(Rollout)、得られた双方の軌跡の距離を近づける形でシミュレーターを更新する。
1226	Few-Shot Unsupervised Image-to-Image Translation	https://arxiv.org/abs/1905.01723	Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, Jan Kautz	Few shortで画像のドメイン変換を行う研究。ソース画像1枚とターゲット画像N枚を入力とし、ターゲット画像についてはそれぞれEncodeして平均を取る。この平均ベクトルを全結合のネットワークに入力し、各層の平均/分散で(正規化済みの)Decode側の層をシフトさせる形でターゲット情報を与えている。
1227	Decentralized Learning of Generative Adversarial Networks from Multi-Client Non-iid Data	https://arxiv.org/abs/1905.09684	Ryo Yonetani, Tomohiro Takahashi, Atsushi Hashimoto, Yoshitaka Ushiku	データが分散した環境でGANの学習を行う研究。ユーザーのモバイル端末内にデータがあり、それで学習したいが個々の秘匿性は担保したいというケースが想定されている。個々の端末でDを、中央側で各Dを用いGを学習するというのが基本構成で、各Dを合わせる際は最も判断がゆるいもの(Forgiver)に合わせる
1228	MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies	https://arxiv.org/abs/1905.09808	Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, Sergey Levine	複数のベース戦略を組み合わせ、様々な戦略を実現する手法の提案。通常、戦略の組み合わせは合算(Additive)で行われることが多いが、掛け合わせ(Multiplicative)を使うことで複数戦略を統合して行動分布を作るような形をとっている。これにより、複雑な連続コントロールタスクができることを確認
1229	The Convolutional Tsetlin Machine	https://arxiv.org/abs/1905.09688	Ole-Christoffer Granmo, Sondre Glimsdal, Lei Jiao, Morten Goodwin, Christian W. Omlin, Geir Thore Berge	解釈が容易かつ簡単に演算ができるCNNの提案。Tsetlin Machineというアルゴリズムをベースとしている。入力に対して0/1の値で構成されたフィルタを論理演算で適用し、各フィルタの演算結果を集計し出力を行う。各フィルタ(演算結果)の重みはバンディットアルゴリズムで調整する(ラベルとの適合が報酬)
1230	Sample Efficient Text Summarization Using a Single Pre-Trained Transformer	https://arxiv.org/abs/1905.08836	Urvashi Khandelwal, Kevin Clark, Dan Jurafsky, Lukasz Kaiser	事前学習済み言語モデルを用い、少量で学習可能な要約モデルを作成した研究。既存のEncoder/Decoderモデルを踏襲するタイプと(重みは共通で事前学習済みの言語モデルからとる)、文章/要約を結合し言語モデルタスクにして要約を生成する2パターンを検証。後者にて3000程度で十分なROUGEが出ることを確認
1231	Towards Neural Decompilation	https://arxiv.org/abs/1905.08325	Omer Katz, Yuval Olshaker, Yoav Goldberg, Eran Yahav	機械翻訳の仕組みを用いて、プログラムのデコンパイルを行ったという研究。具体的には、LLVMの中間コードやx86のアセンブリからCのコードを生成する。入力から直にコードを生成するのでなく、一旦canonical形式を経由し、そこからテンプレートを生成して中の値を埋めるという形式をとっている。
1232	Revisiting Graph Neural Networks: All We Have is Low-Pass Filters	https://arxiv.org/abs/1905.09550	Hoang NT, Takanori Maehara	Graph Convolutionは、グラフ信号に対するローパスフィルタの役割しか果たしていないのではという研究。代表的なデータセット(Cora等)で、高周波成分が有効な特徴を含まない場合(=単なるノイズの場合)にNNとGCNで精度の違いが大きくなることを確認。隣接行列の適用がフィルタに相当するという。
1233	Training language GANs from Scratch	https://arxiv.org/abs/1905.09922	Cyprien de Masson d'Autume, Mihaela Rosca, Jack Rae, Shakir Mohamed	GANで言語モデルを作成しようという試み。G側は(単語の)サンプリングが必要なため、強化学習の枠組みで学習を行っている。D側は報酬を与えることになるが、生成系列全体ではなく生成単語単位に報酬を与える形をとっている。単語単位の密な報酬と大きいバッチサイズ(512)が学習に寄与するとの結果。
1234	Counterfactual Visual Explanations	https://arxiv.org/abs/1904.07451	Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, Stefan Lee	「サンプルの画像のある部分を変化させると別のクラスに分類される」という可視化を行うことによって、画像の分類根拠を説明するという試み。野鳥を分類する教育資料への利用例が書いてあり、この技術で２つの鳥の違いを明示することで教育の効率化を図っている。
1235	SQIL: Imitation Learning via Regularized Behavioral Cloning	https://arxiv.org/abs/1905.11108	Siddharth Reddy, Anca D. Dragan, Sergey Levine	模倣学習で、シンプルな手法で既存の優秀な手法(GANをベースにしたGAIL)を上回ったという研究。エキスパートの動作を模倣するよう学習するが、エキスパートの行動から外れた場合にペナルティを与える(報酬を0にする)ことで正則化を行うという手法。
1236	Levenshtein Transformer	https://arxiv.org/abs/1905.11006	Jiatao Gu, Changhan Wang, Jake Zhao	言語モデルでは通常一方通行に次の単語を予測していくが、編集を可能にすることでより柔軟な生成を行うという研究。具体的には、各単語に対し「削除」「挿入(単語の前に何個挿入を行うか)」「挿入単語」の3つを予測し、予測結果をもとに編集を行うことで系列を生成していく。生成速度面で大きな寄与。
1237	MixMatch: A Holistic Approach to Semi-Supervised Learning	https://arxiv.org/abs/1905.02249	David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel	半教師あり学習で使われる３つの戦略（consistensy regularization: 本論ではデータ拡張された予測値の平均を用いる, generic regularization : 本論ではMIXUPを使う, entropy minimization:本論では温度項を用いてデータを決定境界から遠ざける）を合わせてsota達成。
1238	BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS	https://arxiv.org/abs/1903.12261	Dan Hendrycks(University of California, Berkeley), Thomas Dietterich(Oregon State University)	ICRL Best Paperの１つ。ネットワークの画像汚染と摂動に対する。AlexNetをベースラインとした評価指標を提案。汚染の方は、汚染された画像に対してどれだけ強いか、綺麗なデータと比較した場合の精度の落ち込み度、をAlexネットと比較する。 著者らがいうには、ヒストグラム平坦化、multi scaleで画像を取り込む手法(MSDNetsw等）や複数の特徴を取り込む手法(DenseNet, ResNext等)が頑健性に対して良いらしい。
1239	Discrete Flows: Invertible Generative Models of Discrete Data	https://arxiv.org/abs/1905.10347	Dustin Tran, Keyon Vafa, Kumar Krishna Agrawal, Laurent Dinh, Ben Poole	可逆変換可能な関数で、確率分布を連続的に変換するNormalizing Flowを離散データに対して適用したという研究。Normalizing Flowの構成要素である変換関数、変換による密度変化を調整する項の2つについて、離散の場合どうするかをそれぞれ定義して導出している。文字レベルの言語モデルで有効性を確認
1240	Cold Case: The Lost MNIST Digits	https://arxiv.org/abs/1905.10498	Chhavi Yadav, Lon Bottou	MNISTの原型を新しく作り直したという研究。具体的には、詳細が不明だった前処理のプロセスを構築し直し、当時計算リソースの問題から使われなかった50000のテストセットを発掘して加えている。
1241	An Explicitly Relational Neural Network Architecture	https://arxiv.org/abs/1905.10307	Murray Shanahan, Kyriacos Nikiforou, Antonia Creswell, Christos Kaplanis, David Barrett, Marta Garnelo	DNNに、認識対象間の関係を論理法則(and/or/notなど)で表現することを学習させる手法の提案。対象オブジェクトの選択(attention)、関係評価の特徴選択(binding)、関係の推定(relation)の3段階からなる。画像内のオブジェクトパターンを認識するタスクで効果を確認。
1242	Causal Confusion in Imitation Learning	https://arxiv.org/abs/1905.11979	Pim de Haan, Dinesh Jayaraman, Sergey Levine	強化学習で、見せかけの因果関係で行動をするケースを防止する手法の提案(「ブレーキを踏む」行動が「信号」ではなく「速度メーター」に依存するなど)。環境の情報が多いほど惑わされやすい。関係を示す0/1ベクトル(関係定義グラフから取得)で余分な情報をマスクし、模倣学習を用い学習させている。
1243	EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks	https://arxiv.org/abs/1905.11946	Mingxing Tan, Quoc V. Le	精度/実行速度のトレードオフを上手くチューニングする手法と、チューニング元のネットワーク(EfficientNet)の提案(これはAutoMLで構築)。ネットワークの幅・深さ・入力画像の解像度(H x M)が調整対象で、これらの相関関係から係数を算出することで単一パラメーターによる同時調整を可能にしている
1244	Inorganic Materials Synthesis Planning with Literature-Trained Neural Networks	https://arxiv.org/abs/1901.00032	Edward Kim/Dept. of Materials Science and Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA	無機合成の情報はデータベース化が進んでいない。論文テキストをCVAEで学習させることで合成手順の自動生成を実現した。外部知識なしに材料の特性に応じた合成手法を生成できた。
1245	Inorganic Materials Synthesis Planning with Literature-Trained Neural Networks	https://arxiv.org/abs/1901.00032	Edward Kim/Dept. of Materials Science and Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA	無機合成の情報はデータベース化が進んでいない。論文テキストをCVAEで学習させることで合成手順の自動生成を実現した。外部知識なしに材料の特性に応じた合成手法を生成できた。
1246	Are Disentangled Representations Helpful for Abstract Visual Reasoning?	https://arxiv.org/abs/1905.12506	Sjoerd van Steenkiste, Francesco Locatello, Jrgen Schmidhuber, Olivier Bachem	既存の表現学習手法について、画像内オブジェクトを識別する性能を検証した研究。レーヴン漸進的マトリックスという画像内からパターンを認識して答えるテストを参考にデータセットを作成し、β-VAEなど代表的な手法についてパラメーターを変えて幅広く実験を行なっている。
1247	Defending Against Neural Fake News	https://arxiv.org/abs/1905.12616	Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi	言語モデルなどを利用したフェイクニュース生成に対応していくために、基礎的な脅威モデルを作成したという研究。具体的にはニュース生成器/識別器を作成し現時点での生成/識別精度を検証している。生成器は発行元/日付/タイトル/本文等について、指定されたもの以外を順次穴埋めしていく形で生成する
1248	Using Text Embeddings for Causal Inference	https://arxiv.org/abs/1905.12741	Victor Veitch, Dhanya Sridhar, David M. Blei	自然言語処理で因果推論を行う試み。論文のAccept、掲示板における投稿評価を題材に、BERTを用いたembeddingから論文の特定単語やTheoremが採択に影響を与える度合い、投稿者の性別が評価に与える影響を分析している。
1249	Genie: A Generator of Natural Language Semantic Parsers for Virtual Assistant Commands	https://arxiv.org/abs/1904.09020	Giovanni Campagna, Silei Xu, Mehrad Moradshahi, Richard Socher, Monica S. Lam	ユーザーの発話/入力テキストを、コマンド実行可能なフォーマットに変換するフレームワークの開発。変換先のコードはThingTalkをベースにしている。変換はNNで行うが当然学習データの問題があるため、テンプレートをベースにしたデータの自動生成やクラウドソーシングとの連携機能を搭載している。
1250	Generating Diverse High-Fidelity Images with VQ-VAE-2	https://arxiv.org/abs/1906.00446	Ali Razavi, Aaron van den Oord, Oriol Vinyals	画像は圧縮しても劣化がほぼないことをヒントに、離散表現化(≒圧縮)を経由するVQ-VAE(
1251	Massively Multilingual Neural Machine Translation	https://arxiv.org/abs/1903.00089	Roee Aharoni, Melvin Johnson, Orhan Firat	複数言語の翻訳を並列で学習させることが、どれだけスケールするかを調べた研究。タスクはto英語/from英語が基本で、英語=>複数言語、複数言語=>英語、そしてfrom/to両方学習の計3つ。to英語/from英語単体より、両方学習の方が良いとの結果。この傾向は学習データの大きさによらず観測されるという。
1252	Learning Sparse Networks Using Targeted Dropout	https://arxiv.org/abs/1905.13678	2019/5/31	Dropoutを、あまり役立っていない接続に狙いをつけて行う手法の提案。イメージ的には枝刈りを学習中に行なっている形に近い。DNNでは役立っている接続が全体として少ないため、役立っているものに絞り込んで学習していこうという発想。
1253	Episodic Memory in Lifelong Language Learning	https://arxiv.org/abs/1906.01076	Cyprien de Masson d'Autume, Sebastian Ruder, Lingpeng Kong, Dani Yogatama	継続的な学習を行う自然言語処理のモデルの提案。強化学習で使用されるExperience Replayに似た機構を持ち、ここに学習サンプルをランダムに格納する。学習中定期的にReplayから学習するほか、推論時もテストサンプルに似たデータをサンプリングし追加学習する(local adaptation)。
1254	Unsupervised Object Segmentation by Redrawing	https://arxiv.org/abs/1905.13539	Mickal Chen, Thierry Artires, Ludovic Denoyer	セグメンテーションを、画像生成の文脈で解くという研究。セグメンテーションの学習はピクセルレベルのマスクアノテーションが必要で、データを作るコストが高い。しかし、画像生成なら教師なしで行える。具体的にはGANの枠組みで一旦マスクを生成し、各マスク内の画像を生成＋合わせる形で生成を行う
1255	MelNet: A Generative Model for Audio in the Frequency Domain	https://arxiv.org/abs/1906.01083	Sean Vasquez, Mike Lewis	音声の生成を、一般的な振幅X時系列ではなく周波数X時系列(スペクトログラム)で行った研究。スペクトログラムは縦横軸の意味がそれぞれ異なるため、雑にCNNで畳み込むのは適さない。そこで、時間方向/周波数方向で別個のEncode(RNN)を行い生成を行っている。
1256	Non-Monotonic Sequential Text Generation	https://arxiv.org/abs/1902.02192	Sean Welleck, Kiant Brantley, Hal Daum III, Kyunghyun Cho	順不同で文生成を行う研究。Quick Sortを模した生成を手本とし、模倣学習の枠組みで学習を行う。ある単語を選択=>その単語を境界に前半/後半を分ける、という操作を繰り返し、最終的にEndが予測されるまで行うという形。通常の生成(単方向)に若干劣るものの、そこそこの精度での生成ができている。
1257	On Value Functions and the Agent-Environment Boundary	https://arxiv.org/abs/1905.13341	Nan Jiang	強化学習において環境とエージェント(価値関数etc)の境界をどこに置くのかという研究。画面を入力とする場合、一般的には画面に対し前処理を行う。この場合「状態」とは元の画面なのか、前処理済みの画面なのか？という問い。状態に対する前提がアルゴリズムの理論的な解析に影響を与えているという。
1258	Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding	https://arxiv.org/abs/1904.09482	Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao	モデルの蒸留を行い精度を上げる手法を、マルチタスクにも適用した研究。蒸留では教師となるネットワークを先に学習し、教師の予測で生徒を学習させる。本研究では各タスクで教師(アンサンブルのモデル)を作成し、学習データに加え教師モデルの予測結果(分類なら分類確率=soft target)も使い学習を行う
1259	Visualizing and Measuring the Geometry of BERT	https://arxiv.org/abs/1906.02715	Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Vigas, Martin Wattenberg	BERT内で単語/単語間の関係がどう表現されているかを調べた研究。単語間の関係はAttention、単語の意味については複数文を入れた場合の位置変動を可視化し確認＋語義曖昧性タスクの精度で検証している。BERTは係り受けや単語意味を認識しており、この「認識」は各線型空間に射影できるのではとしている
1260	Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World	https://arxiv.org/abs/1703.06907	2017/03/20	シミュレーション環境をランダム化する事で実環境への転移が可能である事を示した論文。
1261	Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules	https://arxiv.org/abs/1905.05393	Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, Xi Chen	自動で最適なData Augmentationを探索するAutoAugment(
1262	Disentangling Disentanglement in Variational Autoencoders	http://proceedings.mlr.press/v97/mathieu19a.html	Emile Mathieu, Tom Rainforth, N Siddharth, Yee Whye Teh	VAEのような入力を構造分解(Disentangle)する手法について、評価手法＋改善方法を提案した研究。分解した分布間の適切なoverlapと、全体を合わせたときのデータ分布との適合性の2点を評価点として提案。β-VAEは前者はコントロールできるが後者はできないことから、事前/事後の距離を正則化項として追加
1263	A Survey of Reinforcement Learning Informed by Natural Language	https://arxiv.org/abs/1906.03926	Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, Tim Rocktschel	BERTやELMoが登場した今、自然言語と強化学習の組み合わせを考える時期に来ているのでは？という研究。言語で指示するか補助するか、という2つの観点で既存研究を整理している(言語理解が必須か否かで指示/補助が別れる)。自然言語知識を経由したタスク間の転移を最終的に目指しているよう。
1264	Multi-hop Reading Comprehension through Question Decomposition and Rescoring	https://arxiv.org/abs/1906.02916	Sewon Min, Victor Zhong, Luke Zettlemoyer, Hannaneh Hajishirzi	複数根拠文にまたがる推論が必要な質問(日光は日本にある/日光には猿がいる=日本に猿がいる、等)について、一度に回答せず一つ一つの質問に分解して答えるという手法。個別質問への分解は少量のアノテーションで学習し(400件)、入力文を元にどの個別質問の回答が正答かを予測する。
1266	Adaptive Neural Trees	http://proceedings.mlr.press/v97/tanno19a.html	Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi, Aditya Nori	DNNとDecision Treeを複合させる研究。木の経路・分岐それぞれで表現学習を行う＋木構造が固定的でないものは初とのこと。growthではleafにデータ分割/データ変換/既存維持のいずれかを追加・追加箇所以外の重みを固定し学習、を行なっていきlossが小さくなる操作を採用。refinementで全体の学習を行う
1267	Watch, Try, Learn: Meta-Learning from Demonstrations and Reward	http://proceedings.mlr.press/v97/tanno19a.html	プロジェクトサイト	模倣学習で、用意されたデモだけでなく自己プレイの結果も活用する研究。これにより少量のデモで効率的に学習できる。複数タスクのデモから基本戦略を学習(メタラーニング)、基本戦略＋デモの情報から実際にプレイ、デモとの差異からリトライ戦略を学習する。基本とリトライで戦略を分けている。
1269	Selfie: Self-supervised Pretraining for Image Embedding	https://arxiv.org/abs/1906.02940	Trieu H. Trinh, Minh-Thang Luong, Quoc V. Le	BERT(
1270	Computer Vision with a Single (Robust) Classifier	http://gradientscience.org/robust_apps/	Logan Engstrom, Andrew Ilyas, Aleksander Mdry, Shibani Santurkar, Brandon Tran, Dimitris Tsipras	Adversarial耐性のある分類機が、画像の生成(変換)やInpaint、超解像に有効であると示した研究。Adversarial耐性がある=画像(クラス)の本質的な特徴を捉えているので、画像のクラス変換を行う際は顕著な特徴を入れることが、Inpaint/超解像ではより「らしい」補完を行うことができるという。
1271	Weight Agnostic Neural Networks	https://arxiv.org/abs/1906.04358	Adam Gaier, David Ha	ニューラルネットにおける構造の重要性を調べた研究。学習を一切せず、構造探索のみでタスクが解けるか検証している。進化戦略で優秀な構造を残していく手法を取っており、評価時の重みは一様分布から取得した共通のものを使う。これにより、学習なしでいくつかの強化学習タスクを解くことに成功。
1272	Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis	https://arxiv.org/abs/1906.03402	Eric Battenberg, Soroosh Mariooryad, Daisy Stanton, RJ Skerry-Ryan, Matt Shannon, David Kao, Tom Bagby	音声合成における、潜在表現の役割を分析するフレームワークの提案。テキスト/話者以外に与える潜在表現は、テキストと音声のマッチ(合成精度)を優先するか、テキスト独立の音声表現(スタイル)を優先するかのトレードオフがある。教師とするEncoderでこれがどう異なるかをペナルティ項から分析している
1273	Classification Accuracy Score for Conditional Generative Models	https://arxiv.org/abs/1905.10887	Suman Ravuri, Oriol Vinyals	GANは本物っぽい画像を作れるようになったけど、その生成画像だけで分類器作っても上手くいかなかった、という研究。分類器の精度とGAN評価で頻繁に使われるFIDスコアとは相関がなく、FIDスコアが低い自己回帰やVQ-VAEで生成したデータの方がよい精度の分類器を作れたとのこと。
1274	Monotonic Infinite Lookback Attention for Simultaneous Machine Translation	https://arxiv.org/abs/1906.05218	Naveen Arivazhagan, Colin Cherry, Wolfgang Macherey, Chung-Cheng Chiu, Semih Yavuz, Ruoming Pang, Wei Li, Colin Raffel	リアルタイムの翻訳を行うためにAttentionを工夫した研究。基本は直近のStateを重視するMonotonicなAttentionだが、直近を重視するほど計算の待ち時間が長くなるため、直近ほどコストが高くなるようペナルティ項をもうけている。一方過去は計算済みのため、範囲を限定せずAttentionを許可している
1275	Visual Relationships as Functions: Enabling Few-Shot Scene Graph Prediction	https://arxiv.org/abs/1906.04876	Apoorva Dornadula, Austin Narcomey, Ranjay Krishna, Michael Bernstein, Li Fei-Fei	グラフを用いてシーンを理解させる手法の研究。ノードを認識オブジェクト、エッジを述語でつなぐ方式をとっている(英文法のSVOが基本で、S/Oがノード、VがS=>O/O=>Sへ変換するエッジとなっている)。文法構造を基本とすることで、Few-shotでの関係予測が行えるよう試みている
1276	Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned	https://arxiv.org/abs/1905.09418	Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov	TransformerのAttention Headについて、役に立っているHeadとそうでないHead、役に立っているものはどんな役割を果たしているのかについて調べた研究。結果として多くのHeadは枝刈り可能で、残ったHeadは位置関係/係り受け関係などの情報を持っているという。
1277	Tackling Climate Change with Machine Learning	https://arxiv.org/abs/1906.05433	David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik Mukkavilli, Konrad P. Kording, Carla Gomes, Andrew Y. Ng, Demis Hassabis, John C. Platt, Felix Creutzig, Jennifer Chayes, Yoshua Bengio	気候変動に関する活動と、機械学習の親和性について調査された研究。画像や自然言語処理など、機械学習の各技術が気候変動のどの活動にどれぐらい役立ちそうかまとめられている。役立つ、だけでなく適用にリスク/副作用が考えられるという点についても述べられている。
1278	Analyzing the Limitations of Cross-lingual Word Embedding Mappings	https://arxiv.org/abs/1906.05407	Aitor Ormazabal, Mikel Artetxe, Gorka Labaka, Aitor Soroa, Eneko Agirre	多言語の分散表現について、一緒に学習するのが良いか(joint)、別々に学習して対応を取るのがいいか(mapping)調べた研究。結果として、一緒に学習する方が同型の(isomorphic)、かつノイズ(Hubness)が低い構造を学習できるという結果。
1279	Analyzing the Role of Model Uncertainty for Electronic Health Records	https://arxiv.org/abs/1906.03842	Michael W. Dusenberry, Dustin Tran, Edward Choi, Jonas Kemp, Jeremy Nixon, Ghassen Jerfel, Katherine Heller, Andrew M. Dai	医療において予測の不確実性はとても大きな問題なので、既存の不確実性をとらえる手法が実際の臨床データでどれぐらい有効かを調べた研究。実験から、AUCや尤度といった既存のメトリクスでは固有の不確実性(特定の患者集団に対してだけ不確実性が高いなど)を評価できないことを指摘している。
1280	Deep learning for molecular design - a review of the state of the art	https://arxiv.org/abs/1903.04388	Daniel C. Elton, Zois Boukouvalas, Mark D. Fuge, Peter W. Chung	分子設計に対しDNNを適用する手法のサーベイ。まず分子の表現方法から始まり、それを利用した生成手法、生成結果の評価/報酬、という3部構成でまとめられている。既存のアーキテクチャーがまとめられたTable2は要チェック。
1281	Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening	https://arxiv.org/abs/1903.08297	Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, Stanisaw Jastrzbski, Thibault Fvry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, Kyunghyun Cho, Krzysztof J. Geras	CNNを用いて乳がんの診断を行うモデルの提案。マンモグラフィで撮影される2方向(脇から内側にかけての斜め方向=MLO、上下方向=CC)の画像を入力として良性腫瘍の有無・悪性腫瘍の有無計4つを予測。パッチレベルのネットワークを別途作成し、良性/悪性の予測で作成したヒートマップを追加入力にしている
1282	Inverse Cooking: Recipe Generation from Food Images	https://arxiv.org/abs/1812.06164	Amaia Salvador, Michal Drozdzal, Xavier Giro-i-Nieto, Adriana Romero	Image Captionのように、料理の画像からレシピを生成するという研究。直接の生成は難しいので、画像をEncodeしたのち一旦材料の予測(Decode)を行い、予測した材料のEncode+画像のEncodeで手順の生成を行なっている。
1283	Query Expansion Techniques for Information Retrieval: a Survey	https://arxiv.org/abs/1708.00247	Hiteshwar Kumar Azad, Akshay Deepak	検索で、クエリ側を拡張するQuery Expansionのサーベイ。入力されるキーワードは1~2語が大半で、また入力者は対象に関する知識がないこともある。そこでクエリ側を拡張しようという分野。変換の類型(Methodology)とそれを行うための手法(Approach)、データソース、また適用例についてまとめられている
1284	Self-Attentional Models for Lattice Inputs	https://arxiv.org/abs/1906.01617	Matthias Sperber, Graham Neubig, Ngoc-Quan Pham, Alex Waibel	Lattice(DAG)を、Self-Attentionのモデルでうまく処理する手法の提案。Graph入力といえばGCNだが、長期コンテキストを読むのが苦手なため結局LSTMと複合させることが多い。そこでTransformerベースのEncoderを使用し、Latticeの接続/接続確率をベースにAttentionに対しMaskをかけて処理している。
1285	Stacked Capsule Autoencoders	https://arxiv.org/abs/1906.06818	Adam R. Kosiorek, Sara Sabour, Yee Whye Teh, Geoffrey E. Hinton	Capsule Netを使用したAuto Encoderの提案。Part/Poseを推定したのち、それらを特定のパターン(Template)に統合して元画像を復元するAutoencoderと、(推定された)Partの表現を学習するAutoencoderの2つから構成される。
1286	Mid-Level Visual Representations  Improve Generalization and Sample Efficiency  for Learning Visuomotor Policies	https://arxiv.org/abs/1812.11971	Alexander Sax, Bradley Emi, Amir R. Zamir, Leonidas Guibas, Silvio Savarese, Jitendra Malik	強化学習で、画像直接ではなく中間/加工表現を状態に使う手法の提案。全20の様々な表現(Edgeや認識クラスなど)を使用するが、そのままでは処理コストが高いため最も有効な特徴からの距離が最小になるようサブセットを選択する。これにより学習/テスト、タスク間の転移を行いやすくできるという
1287	Learning Compositional Neural Programs with Recursive Tree Search and Planning	https://arxiv.org/abs/1905.12941	Thomas Pierrot, Guillaume Ligner, Scott Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, Nando de Freitas	DNNの実行で、再利用可能なモジュールを組み合わせる試み。全体としては強化学習(AlphaZero)の枠組みで、環境以外にプログラムのembeddingを受け取りLSTMで処理、Action/Valueを出力するという流れになっている。LSTMはプログラムの実行環境として動作し、STOPというactionが出るまで実行を続ける。
1288	When to Trust Your Model: Model-Based Policy Optimization	https://arxiv.org/abs/1906.08253	Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine	強化学習における、適切なモデルベースの使用方法の研究。フルにモデルベースで学習すると、モデルの再現性が低い箇所がハックされる現象が起こる(実環境では得られない報酬を取るようになるなど)。モデルを短いスパン(=予測性能が高い範囲)で使用することでこの現象を抑えつつ汎化性能をあげられる
1289	XLNet: Generalized Autoregressive Pretraining for Language Understanding	https://arxiv.org/abs/1906.08237	Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le	BERT (
1290	Attention Branch Network: Learning of Attention Mechanism for Visual Explanation	https://arxiv.org/abs/1812.10025	Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi	通常学習後に観測するActivation Mapを、Attentionとしてネットワーク内に組み込んだ研究。Activation Mapの計算には特徴マップ以外にクラス分類への貢献を測る重みが必要だが(通常は全結合層の重みを使う)、これを取得するためAttention側からもクラス分類確率を出力し、マルチタスクで学習している。
1291	Towards Understanding Linear Word Analogies	https://arxiv.org/abs/1810.04882	Kawin Ethayarajh, David Duvenaud, Graeme Hirst	単語分散表現において、Euclid距離で意味の近さ、加減算で意味の差し引きができる理由について調べた研究。共起シフトPMI(csPMI=PMI(x,y) + log p(x,y))という値が単語ペア間(王様/男性, 女王/女性)でそれぞれ等しければ、それらはベクトル空間上で同一平面に存在することを証明している。
1292	Group Sampling for Scale Invariant Face Detection	http://openaccess.thecvf.com/content_CVPR_2019/html/Ming_Group_Sampling_for_Scale_Invariant_Face_Detection_CVPR_2019_paper.html	Xiang Ming, Fangyun Wei, Ting Zhang, Dong Chen, Fang Wen	顔画像の検知において、複数層のマルチスケール特徴が必ずしも必要ないということを示した研究。顔のような小さいオブジェクトは検出領域/ストライドの幅に合うより外れる確率が高くなる=相対的に小Positive Sampleで学習している。よって学習データのバランスさえ調整すれば単一層で十分な特徴を得られるという。
1293	Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection	https://arxiv.org/abs/1903.10661	Zhiwei Liu, Xiangyu Zhu, Guosheng Hu, Haiyun Guo, Ming Tang, Zhen Lei, Neil M. Robertson, Jinqiao Wang	顔のランドマーク検知における、ノイズの影響を調査した研究。ランドマークのうち輪郭などは定義が不明確なためノイズが多く、それによりモデルの精度が毀損される。そこで「真のランドマーク」の分布を推定=>学習=>学習機をアノテータとし分布学習、を繰り返す学習法を提案
1294	CollaGAN : Collaborative GAN for Missing Image Data Imputation	https://arxiv.org/abs/1903.10661	複数ドメイン間の画像変換を行う手法であるStarGANの学習は、CycleGANと同様一対一のドメイン間の画像変換を行う枠組みだったが、この制約を緩和して既にドメインが対応している複数画像を入力として許すことで、欠損画像の補完というタスクに特化している。	画像のデータ収集における画像データの欠損の問題を、複数ドメインの画像変換の枠組み(StarGAN)を拡張して解決する手法の提案。対応するドメインの複数画像入力から欠損しているドメインの画像を生成して補完する。実験では、医療画像と顔画像データセットでCycleGAN・StarGANと比較。CVPR2019(oral)
1295	Progressive Teacher-student Learning for Early Action Prediction	http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Progressive_Teacher-Student_Learning_for_Early_Action_Prediction_CVPR_2019_paper.html	Xionghui Wang, Jian-Fang Hu, Jian-Huang Lai, Jianguo Zhang, Wei-Shi Zheng	部分的な動画から、何をしようとしているか検出する研究。フルの動画を入力としBi-LSTMを利用する(=行動終了の状態も把握している)Teacherに対し、部分動画を入力としLSTMを利用するStudentの潜在表現/予測が近くしなるよう学習を行う。
1296	Searching for A Robust Neural Architecture in Four GPU Hours	https://xuanyidong.com/publication/cvpr-2019-gradient-based-diff-sampler/	Xuanyi Dong, Yi Yang	ネットワークの構造探索を効率的に行う手法の提案。範囲としてはセル構造の探索で、stride=1の標準セルとstride=2のダウンサイズするセルの2つを探索し、所与のネットワークに当てはめる。構造をDAGで定義し、パスのサンプリング自体を微分可能にすることでE2Eで最適化を行う
1297	What Does BERT Look At? An Analysis of BERT's Attention	https://arxiv.org/abs/1906.04341	Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning	BERT(
1298	Neural Decipherment via Minimum-Cost Flow: from Ugaritic to Linear B	https://arxiv.org/abs/1906.06718	Jiaming Luo, Yuan Cao, Regina Barzilay	失われた言語の解読(翻訳)を行う研究。失われている故に大規模なコーパスは使えないので、同族言語とのアライメントを手がかりに翻訳を行なっている。具体的には、既知の同族言語における文字の並び/単語の出現確率で制約をかけて生成を行なっている。
1299	Predictive Uncertainty Estimation via Prior Networks	https://arxiv.org/abs/1802.10501	Andrey Malinin, Mark Gales	予測モデルにおける不確かさを、1.パラメーターの不確実性、2.データの不確実性(そもそも予測困難)、3.分布差異による不確実性(学習/テストの分布差異)の3つに分類し3に焦点を当てて解決を試みている研究。既存の不確実性推定では2, 3を混合していたため、これを分布として切り離して推定している。
1300	ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness	https://arxiv.org/abs/1811.12231	Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, Wieland Brendel	CNNが物体の構造(大域的な情報)よりも表面的なテクスチャー(局所的な情報)を手がかりにしているという研究。ImageNetをStyle Transferして学習させる実験を行い、モデルの問題というよりタスク(ImageNet)に起因する問題と指摘している。
1301	Unsupervised Learning of Object Keypoints for Perception and Control	https://arxiv.org/abs/1906.11883	Tejas Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew Zisserman, Volodymyr Mnih	強化学習にとって有効な表現学習を行う試み。オブジェクトのキーポイントに注目し、キーポイント上の特徴変化のみを手掛かりに時刻tから時刻t'の画像を生成するよう学習する(時刻tのキーポイント上の特徴を時刻t'のキーポイント上の特徴で置き換えて生成を行う)。これによりサンプル/探索効率を改善。
1302	Benchmarking Model-Based Reinforcement Learning	https://arxiv.org/abs/1907.02057	Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba	近年研究が多い、モデルベースの強化学習を比較検証した研究。総計10のアルゴリズムを、18の環境で実験しそのパフォーマンスを比較している。モデルベースの課題として、サンプル数に対するパフォーマンスの伸びが悪くなる点、モデルによる計画期間の調整が難しい点などをあげている。
1303	PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows	https://arxiv.org/abs/1906.12320	Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, Bharath Hariharan	点群を生成する手法についての研究。形状の推定=>形状から点群の推定という2段階の推定を行なっており、全体をContinuous Normalizing Flow(CNF)でモデル化している。点群推定にとっての事前分布となる形状分布の表現力を上げるために、Encoder(Q)を使い形状分布のCNFを学習している。
1304	Dynamics-Aware Unsupervised Discovery of Skills	https://arxiv.org/abs/1906.12320	Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman	完全なモデルを作成することは困難、というモデルベースの手法が抱える問題に取り組んだ研究。「スキル」という条件付けを使い、特定スキルを発動しているという条件下でモデル/戦略の学習を行う(戦略の学習には内発的報酬を使用)。テスト時はスキルのシーケンスを計画しスキルに応じた戦略を実行する
1305	Sim2real transfer learning for 3D pose estimation: motion to the rescue	https://arxiv.org/abs/1907.02499	Carl Doersch, Andrew Zisserman	3Dモーションの認識で、シミュレーターデータの拡張を行うことで精度を上げたという研究。人間はキーポイントの情報のみで3次元の構造を高度に認識できるという心理学研究の結果から、キーポイント/Optical Flowの情報を加える＋実画像を背景に利用することで、合成画像のみでSOTAを達成。
1306	Toward Fairness in AI for People with Disabilities: A Research Roadmap	https://arxiv.org/abs/1907.02227	Anhong Guo, Ece Kamar, Jennifer Wortman Vaughan, Hanna Wallach, Meredith Ringel Morris	障害を持つ人でもフェアに恩恵を受けられるAIの開発にむけて、必要な検討事項がまとめられた研究。例として、顔認識では特徴的な顔の変形を伴う疾患を持つ人だと認識率が下がってしまう、音声認識では認知・知的障害がある人の場合話す速度が遅く認識できない、などといったことが挙げられている。
1307	Conditional Density Estimation with Neural Networks: Best Practices and Benchmarks	https://arxiv.org/abs/1903.00954	Jonas Rothfuss, Fabio Ferreira, Simon Walther, Maxim Ulrich	ファイナンスの時系列データをニューラルネットワークで予測する際のベストプラクティスについて調査した研究(データの前処理がメイン)。ユーロ・ストックス50指数を題材に十分な予測が行えたとしている。
1308	CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features	https://arxiv.org/abs/1905.04899	Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, Youngjoon Yoo	画像の一部をDropするCutoutと、画像を合成するMixupを複合させて、Dropさせた箇所に別サンプルの画像をはめ込むCutMixという手法を提案。Cutoutは重要な特徴を落としてしまう可能性がある、Mixupは合成画像が不自然になるという双方の課題を克服し、双方を上回る精度を達成
1309	Evolving the Hearthstone Meta	https://arxiv.org/abs/1907.01623	Fernando de Mesentier Silva, Rodrigo Canaan, Scott Lee, Matthew C. Fontaine, Julian Togelius, Amy K. Hoover	進化戦略を用いて、新カードの登場/既存カードの変更が行われた場合のゲームバランスを調べるという研究。様々なデッキでも対等に戦える(勝率が五分五分になる)というポリシーを基本に、変更により勝率の変動が発生するかを調べる。また特定カードをドローした場合に勝率が大きく変わるかを調べている
1310	Large Scale Adversarial Representation Learning	https://arxiv.org/abs/1907.02544	Jeff Donahue, Karen Simonyan	BigGANを利用して表現学習を行う研究。実サンプルx/Encoderで生成した潜在表現zと、Generatorから生成したサンプルG(z)/潜在表現分布からサンプルしたzの距離を近づける形で学習を行う。この形式の学習が、潜在表現的にも画像生成的にも有効であることを確認。
1311	Predicting the Generalization Gap in Deep Networks with Margin Distributions	https://arxiv.org/abs/1810.00113	Yiding Jiang, Dilip Krishnan, Hossein Mobahi, Samy Bengio	DNNの汎化誤差を推定する手法の研究。SVMにおけるマージンをヒントに、決定境界(正しいクラスと次点のクラスの予測確率が五分五分になる点)から、予測結果が変わらないギリギリの点までの距離を、テイラー展開で近似する(一次近似)。各レイヤでこの計算を行い、マージンとテスト精度の相関を示した。
1312	BAM! Born-Again Multi-Task Networks for Natural Language Understanding	https://arxiv.org/abs/1907.04829	Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, Quoc V. Le	マルチタスク学習における蒸留の提案。各教科専任の家庭教師をつけるイメージで、シングルタスクで学習したモデル(教師)の予測結果と近しくなるよう学習を行う(教師の模倣だけに終わらないように、通常のlossと教師との差異でバランスを取る)。BERT(
1313	Superhuman AI for multiplayer poker	https://science.sciencemag.org/content/early/2019/07/10/science.aay2400	Noam Brown, Tuomas Sandholm	6プレイヤーのポーカーでプロに勝ったという研究。既存の強化学習は1:1・完全情報のゲームが多いが、複数プレイヤーかつ不完全情報という難しい状態を扱っている。self-play(6人なので、更新を1体ずつ行う)で学習を行うが、ナッシュ均衡の達成=ゲームの勝利ではないことから均衡達成を目的としていない
1314	Sparse Networks from Scratch: Faster Training without Losing Performance	https://arxiv.org/abs/1907.04840	Tim Dettmers, Luke Zettlemoyer	DNNの学習を効率化する手法の提案。有望なWeightを残し、役立たないWeightは回収し(Prune)他に割り振る(Regrow)というのが基本的な考え。割り振りはMomentum(勾配の履歴を指数平滑で重みづけし合計したもの)のMagnitudeを基準に行い、Pruneされた重みはレイヤー単位/レイヤー内の順に割り振られる。
1315	Large Memory Layers with Product Keys	https://arxiv.org/abs/1907.05242	Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, Herv Jgou	DNNに組み込める巨大なメモリ機構の提案。Key/Value型だが普通に実装すると計算コストがかかるため、Keyの作り方を工夫している。Key空間を2つのsub-key空間に分割し、クエリも2つに分割、各分割クエリに対応するsub-key空間で最も近いsub-keyを取得し、その外積でキーを作成する。
1316	A Deep Generative Model for Graph Layout	https://arxiv.org/abs/1904.12225	Oh-Hyun Kwon, Kwan-Liu Ma	グラフ構造の可視化では、可視化のレイアウトによって見え方が異なりその調整が難しいと言う問題がある。そこで、選択したサンプルを元に様々なレイアウトを生成するEncoder/Decoderモデルを作成したという研究。生成レイアウトを2Dで表示するため、2次元の一様分布を経由してVAEのように生成を行う。
1317	R-Transformer: Recurrent Neural Network Enhanced Transformer	https://arxiv.org/abs/1907.05572	Zhiwei Wang, Yao Ma, Zitao Liu, Jiliang Tang	Globalな情報はTransformerのSelf-Attentionで、Localな情報をRNNで取得するという手法の提案。Transformerは大域的な情報に強いものの局所情報はposition embeddingという限られた情報に依存しているため、これをRNNで代替/補強するというアイデア。言語モデルで優秀な制度を記録。
1318	PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization	https://arxiv.org/abs/1905.05172	Shunsuke Saito(1,2) Zeng Huang(1,2) Ryota Natsume(3)	１枚または複数の画像から３D像を生成する研究。全ての３Dドットを同時に生成するのではなく、射影の関数を学習する戦略をとっているため、必要なメモリが少ないことがポイント。人の存在確率を予測するPIFuと、それと画像から３Dの各点を予測するTe-PIFuに分かれる。
1319	GraphNVP: An Invertible Flow Model for Generating Molecular Graphs	https://arxiv.org/abs/1905.11600	Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, Motoki Abe	可逆変換可能なFlowモデルで、分子生成を行う手法。VAEは分布を経由するため厳密な生成ができない、GANは生成対象のコントロールが難しいという点をFlowモデルの厳密な潜在表現が得られる＋そこからの生成も可能という特性で解消している。ノード/エッジの潜在表現を別々に計算し生成を行なっている。
1320	Learning Perceptually-Aligned Representations via Adversarial Robustness	https://arxiv.org/abs/1906.00945	Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Aleksander Madry	DNNで学習される特徴表現をロバストにしようという研究。既存のモデルでは人間には異なる画像に見えても特徴表現的には同じということがあるが、Adversarialの学習と似た枠組みで、ノイズが乗っても予測が可能なよう学習することでこの現象を緩和することができる。
1321	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	https://arxiv.org/abs/1711.08267	Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, Minyi Guo	Graphの接続生成をGANで行う手法の研究。あるノードに接続しうるノードの生成をGeneratorで行い、Discriminatorは実際の接続分布からサンプリングされたものか判定を行う。Gを単なるsoftmaxにすると全ノードの確率計算が発生してしまうため、BFSで部分木を生成し絞り込む
1322	Towards Human-Friendly Referring Expression Generation	https://arxiv.org/abs/1811.12104	Mikihiro Tanaka, Takayuki Itamochi, Kenichi Narioka, Ikuro Sato, Yoshitaka Ushiku, Tatsuya Harada	見分けのつきにくい物体に対し、人間が識別しやすい説明つける研究。テキストを生成するSpeakerは評価が高いテキストを生成するよう学習される一方、事前学習済みのReinforcerの評価を最大化するようにも学習される。評価は、テキストの正確性だけでなく識別の速さ(アノテーターが判断)も加味される。
1323	Incremental Skip-gram Model with Negative Sampling	https://www.aclweb.org/anthology/papers/D/D17/D17-1037/	Nobuhiro Kaji, Hayato Kobayashi	学習済みの単語分散表現を、SkipGram+Negative Samplingで追加学習する手法の解説記事。Negative Samplingに使用するノイズ分布を勾配法で更新していくことで、真のノイズ分布の計算(全単語のデータが必要)を回避しつつ単語数が多い場合真の分布に近づくことを担保している。
1324	Probing Neural Network Comprehension of Natural Language Arguments	https://arxiv.org/abs/1907.07355	Timothy Niven, Hung-Yu Kao	BERTが論理展開の根拠を選択するタスクで人間を超える精度を出せたのは、データに問題があったからとする研究。「AはBをおこし(Reason)、BはCにつながるから(Warrant)、Aは良い/悪いetc(Claim)」という論理展開で正しいWarrantを選択するが、Warrantにnotがあるかどうかで半分以上正答できたという。
1325	A Learned Representation for Scalable Vector Graphics	https://arxiv.org/abs/1904.02632	Raphael Gontijo Lopes, David Ha, Douglas Eck, Jonathon Shlens	軌跡で描画するベクター画像の生成を行った研究。画像のスタイルをVAEで抽出し、スタイルに基づきMDN(Mixture Density Network)で描画を行っている。潜在表現の分析も行っており、似たフォントスタイルが潜在表現空間上でも近いことを確認している。
1327	Green AI	https://arxiv.org/abs/1907.10597	Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni	機械学習の研究で、精度だけでなく効率性もきちんと重視しようという意見論文。単純に演算コストの高いモデルの研究(RedAI)を非難しているわけではなく、効率性を考えた研究(GreenAI)より重視されすぎていることを指摘している。Greenの評価指標として、浮動小数点の演算回数(FPO)の使用を提案している
1328	Bayesian Uncertainty Estimation for Batch Normalized Deep Networks	https://arxiv.org/abs/1802.06455	Mattias Teye, Hossein Azizpour, Kevin Smith	予測の不確実性を推定する手法の提案。既存研究ではDropoutを使った不確実推定があったが、Dropoutよりもよく使われているBatch Normalizationを使い推定する。正規化に用いる平均/分散を学習データからサンプリングすることでモデル出力の分布を得る。
1329	RoBERTa: A Robustly Optimized BERT Pretraining Approach	https://arxiv.org/abs/1907.11692	Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov	BERTの学習方法を細かく調査した研究。基本的に長い時間、大きなコーパス、大きなバッチで訓練するほどよくなる。segmentが次の文のものか当てさせるタスク(Next sentence prediction)をやめる、動的なマスクを行うのも有効。チューニングで、様々な亜種の結果を上回る
1330	Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems	https://arxiv.org/abs/1905.08743	Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, Pascale Fung	マルチドメインに対応するための対話システムの研究。発話のEncoder、Domain/Slotに該当するValueをコピーメカニズムで抽出するState Generatorが基本だが、複数あるDomain/Slotに対しそもそも予測するかフィルタするSlot Gateを組み込んでいる。この仕組みがFew-shotの学習でも有効なことを確認。
1331	Attention Guided Graph Convolutional Networks for Relation Extraction	https://arxiv.org/abs/1906.07510	Zhijiang Guo, Yan Zhang, Wei Lu	構文解析結果を利用して関係認識のタスクを解く研究。係り受け関係を隣接行列にしてGCNを行うのが基本だが、係り受けの隣接行列をSelf-Attentionのレイヤに通して全結合のグラフを出力している。これにより接続がないところも含め、どの関係を重視するか学習させる。
